{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前置動作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install evaluate rouge-score bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install peft==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall peft triton -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "數據前處理。分割訓練和驗證集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# 加載 train.json\n",
    "with open('train.json', 'r') as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "    \n",
    "with open('test.json', 'r') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# 轉換為 Hugging Face Dataset 格式\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "\n",
    "# 將數據集分割為訓練集和驗證集（80% 訓練，20% 驗證）\n",
    "split_dataset = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_split = split_dataset['train']  # 訓練集\n",
    "valid_split = split_dataset['test']   # 驗證集\n",
    "\n",
    "print(f\"訓練集大小: {len(train_split)}\")\n",
    "print(f\"驗證集大小: {len(valid_split)}\")\n",
    "print(f\"測試集大小: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import PegasusTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# 加載 tokenizer\n",
    "model_name = 'google/pegasus-large'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 加載數據\n",
    "with open('train.json', 'r') as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "    \n",
    "with open('test.json', 'r') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# 函數：計算 token 長度\n",
    "def get_token_lengths(data, field):\n",
    "    lengths = [len(tokenizer.encode(item[field], add_special_tokens=True)) for item in data]\n",
    "    return lengths\n",
    "\n",
    "# 計算 train.json 中 introduction 和 abstract 的長度\n",
    "train_intro_lengths = get_token_lengths(train_data, 'introduction')\n",
    "train_abs_lengths = get_token_lengths(train_data, 'abstract')\n",
    "\n",
    "# 計算 test.json 中 introduction 的長度\n",
    "test_intro_lengths = get_token_lengths(test_data, 'introduction')\n",
    "\n",
    "# 統計信息\n",
    "def print_stats(lengths, name):\n",
    "    print(f\"{name} 長度統計：\")\n",
    "    print(f\"平均長度: {np.mean(lengths):.1f}\")\n",
    "    print(f\"中位數: {np.median(lengths):.1f}\")\n",
    "    print(f\"最大長度: {max(lengths)}\")\n",
    "    print(f\"最小長度: {min(lengths)}\")\n",
    "    print(f\"90% 分位數: {np.percentile(lengths, 90):.1f}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# 輸出結果\n",
    "print_stats(train_intro_lengths, \"Train Introduction\")\n",
    "print_stats(train_abs_lengths, \"Train Abstract\")\n",
    "print_stats(test_intro_lengths, \"Test Introduction\")\n",
    "\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "model_name = \"google/pegasus-x-large\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 分析 token 長度分佈\n",
    "def analyze_token_lengths(data, tokenizer, field=\"introduction\"):\n",
    "    lengths = []\n",
    "    for item in data:\n",
    "        text = item[field]\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        lengths.append(len(tokens))\n",
    "    return lengths\n",
    "\n",
    "# 假設 train_data 和 test_data 已定義\n",
    "# 分析 train 和 test 數據\n",
    "train_intro_lengths = analyze_token_lengths(train_data, tokenizer, \"introduction\")\n",
    "train_abs_lengths = analyze_token_lengths(train_data, tokenizer, \"abstract\")\n",
    "test_intro_lengths = analyze_token_lengths(test_data, tokenizer, \"introduction\")\n",
    "\n",
    "# 打印統計信息\n",
    "import numpy as np\n",
    "\n",
    "print(\"Train Introduction Token Lengths:\")\n",
    "print(f\"Mean: {np.mean(train_intro_lengths):.2f}, Max: {max(train_intro_lengths)}, Min: {min(train_intro_lengths)}\")\n",
    "print(f\"Percentage > 1024: {sum(l > 1024 for l in train_intro_lengths) / len(train_intro_lengths) * 100:.2f}%\")\n",
    "\n",
    "print(\"Train Abstract Token Lengths:\")\n",
    "print(f\"Mean: {np.mean(train_abs_lengths):.2f}, Max: {max(train_abs_lengths)}, Min: {min(train_abs_lengths)}\")\n",
    "print(f\"Percentage > 660: {sum(l > 660 for l in train_abs_lengths) / len(train_abs_lengths) * 100:.2f}%\")\n",
    "\n",
    "print(\"Test Introduction Token Lengths:\")\n",
    "print(f\"Mean: {np.mean(test_intro_lengths):.2f}, Max: {max(test_intro_lengths)}, Min: {min(test_intro_lengths)}\")\n",
    "print(f\"Percentage > 1024: {sum(l > 1024 for l in test_intro_lengths) / len(test_intro_lengths) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方向1、Traditional  Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegasus、T5 (FLAN-T5)、或 BART 都是最常見的「摘要三巨頭」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打算使用 PEGASUS-Large、PEGASUS-ArXiv、LED 、LongT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegasus-x-large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy summa nlpaug nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "設定超參數、評估指標、訓練階段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "# 加載 spacy 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "allowed_unicode = \"∑∂∇∞θπ𝒟𝒫𝒩αβγδελμσφωℝ𝔽𝓛\"\n",
    "def is_allowed_char(c):\n",
    "    return (\n",
    "        ord(c) < 128 or\n",
    "        c in allowed_unicode or\n",
    "        \"MATHEMATICAL\" in unicodedata.name(c, \"\")\n",
    "    )\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)  # 保留 \\emph{} 內文\n",
    "    text = ''.join(c if is_allowed_char(c) else ' ' for c in text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Load the Pegasus-X model and tokenizer\n",
    "model_name = \"google/pegasus-x-large\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 2. Load the datasets with cleaning\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    # 清理 introduction 和 abstract\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "train_data = load_json(\"train.json\")  \n",
    "test_data = load_json(\"test.json\")    \n",
    "\n",
    "# 3. 從兩端截斷 + 語義重要性選擇\n",
    "def truncate_from_ends_with_importance(introduction, tokenizer, max_length=1024):\n",
    "    # 使用 spacy 分割句子\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    # 如果句子數少於等於 1，直接分詞並截斷\n",
    "    if len(sentences) <= 1:\n",
    "        tokens = tokenizer(\n",
    "            introduction,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()    \n",
    "\n",
    "    # 計算每個句子的 TF-IDF 分數\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        sentence_scores = tfidf_matrix.sum(axis=1).A1\n",
    "    except ValueError:\n",
    "        sentence_scores = np.arange(len(sentences), 0, -1)\n",
    "\n",
    "    # 確定從兩端保留的比例（例如前 40% 和後 40%）\n",
    "    num_sentences = len(sentences)\n",
    "    num_end_sentences = max(1, int(num_sentences * 0.45))  # 至少保留 1 句\n",
    "    start_sentences = sentences[:num_end_sentences]  # 開頭部分\n",
    "    end_sentences = sentences[-num_end_sentences:]   # 結尾部分\n",
    "    middle_sentences = sentences[num_end_sentences:-num_end_sentences]  # 中間部分\n",
    "    middle_scores = sentence_scores[num_end_sentences:-num_end_sentences]\n",
    "\n",
    "    # 計算開頭和結尾部分的 token 數\n",
    "    start_tokens = tokenizer.encode(\" \".join(start_sentences), add_special_tokens=False)\n",
    "    end_tokens = tokenizer.encode(\" \".join(end_sentences), add_special_tokens=False)\n",
    "    current_token_count = len(start_tokens) + len(end_tokens)\n",
    "\n",
    "    # 如果開頭和結尾已經超過 max_length，直接截斷\n",
    "    if current_token_count >= max_length - 2:\n",
    "        combined_text = \" \".join(start_sentences + end_sentences)\n",
    "        tokens = tokenizer(\n",
    "            combined_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "    # 從中間部分選擇關鍵句\n",
    "    selected_middle_sentences = []\n",
    "    if middle_sentences:\n",
    "        sorted_middle_indices = np.argsort(middle_scores)[::-1]\n",
    "        for idx in sorted_middle_indices:\n",
    "            sentence = middle_sentences[idx]\n",
    "            tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            if current_token_count + len(tokens) <= max_length - 2:\n",
    "                selected_middle_sentences.append(sentence)\n",
    "                current_token_count += len(tokens)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    truncated_introduction = \" \".join(start_sentences + selected_middle_sentences + end_sentences)\n",
    "    tokens = tokenizer(\n",
    "        truncated_introduction,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "# 訓練數據集\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024, max_target_length=660):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        abstract = item[\"abstract\"]\n",
    "\n",
    "        # 使用從兩端截斷 + 語義重要性選擇\n",
    "        input_ids, attention_mask = truncate_from_ends_with_importance(introduction, self.tokenizer, self.max_input_length)\n",
    "\n",
    "        # 分詞 abstract (target)\n",
    "        targets = self.tokenizer(\n",
    "            abstract,\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": target_ids,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "\n",
    "# 測試數據集\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "\n",
    "        # 使用從兩端截斷 + 語義重要性選擇\n",
    "        input_ids, attention_mask = truncate_from_ends_with_importance(introduction, self.tokenizer, self.max_input_length)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"]\n",
    "        }\n",
    "\n",
    "# 創建數據集和 DataLoader\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer, max_input_length=1024, max_target_length=660)\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer, max_input_length=1024)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 4. Fine-tune the model with checkpoint saving\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "num_epochs = 300\n",
    "\n",
    "# 創建檢查點儲存目錄\n",
    "checkpoint_dir = \"pegasus-x-large_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "best_loss = float('inf')  # 用於儲存最佳損失\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 儲存檢查點\n",
    "    if epoch % 10 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch + 1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    # 儲存最佳模型（根據損失）\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, best_model_path)\n",
    "        print(f\"Best model saved at {best_model_path} with loss {best_loss:.4f}\")\n",
    "\n",
    "# 5. 後處理函數：清理生成的摘要\n",
    "def clean_abstract(abstract):\n",
    "    # 移除非英文字符\n",
    "    abstract = clean_text(abstract)\n",
    "    \n",
    "    # 移除開頭重複的 \"In\"\n",
    "    words = abstract.split()\n",
    "    while len(words) > 1 and words[0] == \"In\" and words[1] == \"In\":\n",
    "        words.pop(0)\n",
    "    abstract = \" \".join(words)\n",
    "\n",
    "    # 移除重複的句子\n",
    "    sentences = abstract.split(\". \")\n",
    "    seen_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence and sentence not in seen_sentences:\n",
    "            seen_sentences.append(sentence)\n",
    "    abstract = \". \".join(seen_sentences)\n",
    "    if abstract and not abstract.endswith(\".\"):\n",
    "        abstract += \".\"\n",
    "\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推理階段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "# 加載 spacy 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 創建檢查點儲存目錄\n",
    "checkpoint_dir = \"pegasus-x-large_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"google/pegasus-x-large\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "allowed_unicode = \"∑∂∇∞θπ𝒟𝒫𝒩αβγδελμσφωℝ𝔽𝓛\"\n",
    "def is_allowed_char(c):\n",
    "    return (\n",
    "        ord(c) < 128 or\n",
    "        c in allowed_unicode or\n",
    "        \"MATHEMATICAL\" in unicodedata.name(c, \"\")\n",
    "    )\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)  # 保留 \\emph{} 內文\n",
    "    text = ''.join(c if is_allowed_char(c) else ' ' for c in text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    # 清理 introduction 和 abstract\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "def clean_abstract(abstract):\n",
    "    # 移除非英文字符\n",
    "    abstract = clean_text(abstract)\n",
    "    \n",
    "    # 移除開頭重複的 \"In\"\n",
    "    words = abstract.split()\n",
    "    while len(words) > 1 and words[0] == \"In\" and words[1] == \"In\":\n",
    "        words.pop(0)\n",
    "    abstract = \" \".join(words)\n",
    "\n",
    "    # 移除重複的句子\n",
    "    sentences = abstract.split(\". \")\n",
    "    seen_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence and sentence not in seen_sentences:\n",
    "            seen_sentences.append(sentence)\n",
    "    abstract = \". \".join(seen_sentences)\n",
    "    if abstract and not abstract.endswith(\".\"):\n",
    "        abstract += \".\"\n",
    "\n",
    "    return abstract\n",
    "\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024, max_target_length=660):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        abstract = item[\"abstract\"]\n",
    "\n",
    "        # 使用從兩端截斷 + 語義重要性選擇\n",
    "        input_ids, attention_mask = truncate_from_ends_with_importance(introduction, self.tokenizer, self.max_input_length)\n",
    "\n",
    "        # 分詞 abstract (target)\n",
    "        targets = self.tokenizer(\n",
    "            abstract,\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": target_ids,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "\n",
    "# 測試數據集\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "\n",
    "        # 使用從兩端截斷 + 語義重要性選擇\n",
    "        input_ids, attention_mask = truncate_from_ends_with_importance(introduction, self.tokenizer, self.max_input_length)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"]\n",
    "        }\n",
    "\n",
    "def truncate_from_ends_with_importance(introduction, tokenizer, max_length=1024):\n",
    "    # 使用 spacy 分割句子\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    # 如果句子數少於等於 1，直接分詞並截斷\n",
    "    if len(sentences) <= 1:\n",
    "        tokens = tokenizer(\n",
    "            introduction,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()    \n",
    "\n",
    "    # 計算每個句子的 TF-IDF 分數\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        sentence_scores = tfidf_matrix.sum(axis=1).A1\n",
    "    except ValueError:\n",
    "        sentence_scores = np.arange(len(sentences), 0, -1)\n",
    "\n",
    "    # 確定從兩端保留的比例（例如前 40% 和後 40%）\n",
    "    num_sentences = len(sentences)\n",
    "    num_end_sentences = max(1, int(num_sentences * 0.45))  # 至少保留 1 句\n",
    "    start_sentences = sentences[:num_end_sentences]  # 開頭部分\n",
    "    end_sentences = sentences[-num_end_sentences:]   # 結尾部分\n",
    "    middle_sentences = sentences[num_end_sentences:-num_end_sentences]  # 中間部分\n",
    "    middle_scores = sentence_scores[num_end_sentences:-num_end_sentences]\n",
    "\n",
    "    # 計算開頭和結尾部分的 token 數\n",
    "    start_tokens = tokenizer.encode(\" \".join(start_sentences), add_special_tokens=False)\n",
    "    end_tokens = tokenizer.encode(\" \".join(end_sentences), add_special_tokens=False)\n",
    "    current_token_count = len(start_tokens) + len(end_tokens)\n",
    "\n",
    "    # 如果開頭和結尾已經超過 max_length，直接截斷\n",
    "    if current_token_count >= max_length - 2:\n",
    "        combined_text = \" \".join(start_sentences + end_sentences)\n",
    "        tokens = tokenizer(\n",
    "            combined_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "    # 從中間部分選擇關鍵句\n",
    "    selected_middle_sentences = []\n",
    "    if middle_sentences:\n",
    "        sorted_middle_indices = np.argsort(middle_scores)[::-1]\n",
    "        for idx in sorted_middle_indices:\n",
    "            sentence = middle_sentences[idx]\n",
    "            tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            if current_token_count + len(tokens) <= max_length - 2:\n",
    "                selected_middle_sentences.append(sentence)\n",
    "                current_token_count += len(tokens)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    truncated_introduction = \" \".join(start_sentences + selected_middle_sentences + end_sentences)\n",
    "    tokens = tokenizer(\n",
    "        truncated_introduction,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "train_data = load_json(\"train.json\")  # 408 samples\n",
    "test_data = load_json(\"test.json\")    # 103 samples\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer, max_input_length=1024, max_target_length=660)\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer, max_input_length=1024)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 6. 推理時加載訓練好的模型\n",
    "# 加載最佳模型（可以根據需要改為特定的 epoch 檢查點，例如 \"checkpoint_epoch_50.pth\"）\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path} (epoch {checkpoint['epoch']}, loss {checkpoint['loss']:.4f})\")\n",
    "else:\n",
    "    print(f\"Checkpoint {checkpoint_path} not found, using the last trained model.\")\n",
    "\n",
    "# 設置模型為評估模式\n",
    "model.eval()\n",
    "predictions = []\n",
    "predicted_abstracts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        paper_ids = batch[\"paper_id\"]\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=1024,\n",
    "            min_length=50, \n",
    "            num_beams=8, \n",
    "            length_penalty=1.8,\n",
    "            early_stopping=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "        for i, generated in enumerate(generated_ids):\n",
    "            abstract = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "            abstract = clean_abstract(abstract)\n",
    "            if isinstance(paper_ids[i], torch.Tensor):\n",
    "                paper_id = str(paper_ids[i].item())\n",
    "            elif isinstance(paper_ids[i], np.ndarray):\n",
    "                paper_id = str(paper_ids[i].item())\n",
    "            else:\n",
    "                paper_id = str(paper_ids[i])\n",
    "            abstract = str(abstract) if not isinstance(abstract, str) else abstract\n",
    "            predictions.append({\n",
    "                \"paper_id\": paper_id,\n",
    "                \"abstract\": abstract\n",
    "            })\n",
    "            predicted_abstracts.append(abstract)\n",
    "\n",
    "# 7. 評估模型\n",
    "reference_abstracts = [item[\"abstract\"] for item in train_data[:103]]\n",
    "\n",
    "metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "rouge_scores = metric_rouge.compute(\n",
    "    predictions=predicted_abstracts,\n",
    "    references=reference_abstracts,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "bert_scores = metric_bertscore.compute(\n",
    "    predictions=predicted_abstracts,\n",
    "    references=reference_abstracts,\n",
    "    lang=\"en\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "\n",
    "print(\"\\nBERTScore:\")\n",
    "print(f\"Precision: {np.mean(bert_scores['precision']):.4f}\")\n",
    "print(f\"Recall: {np.mean(bert_scores['recall']):.4f}\")\n",
    "print(f\"F1: {np.mean(bert_scores['f1']):.4f}\")\n",
    "\n",
    "# 8. Save predictions\n",
    "with open(\"submission.json\", \"w\") as f:\n",
    "    for pred in predictions:\n",
    "        f.write(json.dumps(pred) + \"\\n\")\n",
    "\n",
    "print(\"Predictions saved to submission.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegasus-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微調訓練階段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge import Rouge\n",
    "import evaluate\n",
    "\n",
    "# 加載 spacy 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 清理 LaTeX 和亂碼的函數\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F’‘–—∑∂∇∞θπ𝒟𝒫𝒩αβγδελμσφωℝ𝔽𝓛()]', ' ', text)  # 保留括號\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 設置設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. 加載 Pegasus-ArXiv 模型和 tokenizer\n",
    "model_name = \"google/pegasus-arxiv\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 2. 加載數據集並清理文本\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "train_data = load_json(\"train.json\")  # 408 個樣本\n",
    "test_data = load_json(\"test.json\")    # 103 個樣本\n",
    "\n",
    "# 分割驗證集\n",
    "val_size = int(0.2 * len(train_data))\n",
    "val_data = train_data[-val_size:]\n",
    "train_data = train_data[:-val_size]\n",
    "\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(val_data)}\")\n",
    "\n",
    "# 3. 使用 sentence-transformers 改進截斷策略，引入領域知識\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def select_important_sentences(introduction, tokenizer, max_length=1024):\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    if len(sentences) <= 1:\n",
    "        tokens = tokenizer(\n",
    "            introduction,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "    # 計算句子嵌入\n",
    "    embeddings = embedder.encode(sentences)\n",
    "    doc_embedding = np.mean(embeddings, axis=0)\n",
    "    similarities = cosine_similarity(embeddings, doc_embedding.reshape(1, -1)).flatten()\n",
    "\n",
    "    # 引入領域知識：定義學術文章中常見的關鍵詞\n",
    "    academic_keywords = [\n",
    "        \"propose\", \"method\", \"approach\", \"result\", \"finding\", \"conclusion\",\n",
    "        \"demonstrate\", \"show\", \"achieve\", \"contribution\", \"investigate\", \"study\",\n",
    "        \"analysis\", \"evaluate\", \"performance\", \"improve\", \"novel\", \"framework\"\n",
    "    ]\n",
    "\n",
    "    # 給包含關鍵詞的句子加權\n",
    "    scores = similarities.copy()\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if any(keyword in sentence.lower() for keyword in academic_keywords):\n",
    "            scores[idx] *= 1.5  # 提高包含關鍵詞的句子的分數\n",
    "\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "\n",
    "    selected_sentences = []\n",
    "    current_length = 0\n",
    "    for idx in sorted_indices:\n",
    "        sentence = sentences[idx]\n",
    "        tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        if current_length + len(tokens) <= max_length - 2:\n",
    "            selected_sentences.append(sentence)\n",
    "            current_length += len(tokens)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    truncated_text = \" \".join(selected_sentences)\n",
    "    tokens = tokenizer(\n",
    "        truncated_text,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "# 4. 定義數據集\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024, max_target_length=660):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        abstract = item[\"abstract\"]\n",
    "\n",
    "        input_ids, attention_mask = select_important_sentences(introduction, self.tokenizer, self.max_input_length)\n",
    "\n",
    "        targets = self.tokenizer(\n",
    "            abstract,\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": target_ids,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        input_ids, attention_mask = select_important_sentences(introduction, self.tokenizer, self.max_input_length)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"]\n",
    "        }\n",
    "\n",
    "# 創建數據集和 DataLoader\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer, max_input_length=1024, max_target_length=660)\n",
    "val_dataset = TrainPaperDataset(val_data, tokenizer, max_input_length=1024, max_target_length=660)\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer, max_input_length=1024)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 5. 定義 ROUGE 和 BERTScore 損失計算函數\n",
    "rouge = Rouge()\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_rouge_loss(model, input_ids, attention_mask, reference_abstracts):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=660,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_abstracts = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "        scores = rouge.get_scores(generated_abstracts, reference_abstracts, avg=True)\n",
    "        rouge_l = scores['rouge-l']['f']\n",
    "    model.train()\n",
    "    return rouge_l\n",
    "\n",
    "def compute_bertscore_loss(model, input_ids, attention_mask, reference_abstracts):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=660,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_abstracts = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "        scores = bertscore_metric.compute(\n",
    "            predictions=generated_abstracts,\n",
    "            references=reference_abstracts,\n",
    "            lang=\"en\"\n",
    "        )\n",
    "        bertscore_f1 = np.mean(scores['f1'])\n",
    "    model.train()\n",
    "    return bertscore_f1\n",
    "\n",
    "# 新增 ROUGE-1 計算函數\n",
    "def compute_rouge_scores(model, input_ids, attention_mask, reference_abstracts):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=660, num_beams=5)\n",
    "        generated_abstracts = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "        scores = rouge.get_scores(generated_abstracts, reference_abstracts, avg=True)\n",
    "        rouge1 = scores['rouge-1']['f']\n",
    "        rougeL = scores['rouge-l']['f']\n",
    "    model.train()\n",
    "    return rouge1, rougeL\n",
    "\n",
    "# 6. 訓練模型：優化早停和學習率調度，並添加評估\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.05)  # 增加初始學習率\n",
    "\n",
    "# 定義溫暖啟動調度器\n",
    "def warm_up_lambda(epoch):\n",
    "    warm_up_epochs = 10  # 增加溫暖啟動階段\n",
    "    if epoch < warm_up_epochs:\n",
    "        return (epoch + 1) / warm_up_epochs  # 線性增加學習率\n",
    "    return 1.0\n",
    "\n",
    "def train_segmented_input(model, input_ids, attention_mask, labels, max_segment_length=1024):\n",
    "    total_length = input_ids.shape[1]\n",
    "    if total_length <= max_segment_length:\n",
    "        return model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    \n",
    "    losses = []\n",
    "    for start in range(0, total_length, max_segment_length):\n",
    "        end = min(start + max_segment_length, total_length)\n",
    "        segment_input_ids = input_ids[:, start:end]\n",
    "        segment_attention_mask = attention_mask[:, start:end]\n",
    "        segment_labels = labels[:, start:end] if labels.shape[1] == total_length else labels\n",
    "        output = model(input_ids=segment_input_ids, attention_mask=segment_attention_mask, labels=segment_labels)\n",
    "        losses.append(output.loss)\n",
    "    return torch.mean(torch.stack(losses))\n",
    "\n",
    "warm_up_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warm_up_lambda)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=290, eta_min=1e-6)\n",
    "\n",
    "num_epochs = 50  # 增加總訓練 epoch 數\n",
    "accumulation_steps = 5\n",
    "\n",
    "checkpoint_dir = \"pegasus-arxiv_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Baseline 分數（用於比較）\n",
    "baseline_scores = {\n",
    "    \"rouge1\": 0.47,\n",
    "    \"rouge2\": 0.12,\n",
    "    \"rougeL\": 0.22,\n",
    "    \"bertscore_f1\": 0.85\n",
    "}\n",
    "\n",
    "eval_frequency = 5  # 每 5 個 epoch 進行一次評估\n",
    "\n",
    "# 儲存最佳 ROUGE-1 分數（用於早停）\n",
    "best_rouge1 = 0.0\n",
    "patience = 100\n",
    "patience_counter = 0\n",
    "\n",
    "# 儲存最佳評估分數（用於最終報告）\n",
    "best_rouge2 = 0.0\n",
    "best_rougeL = 0.0\n",
    "best_bertscore_f1 = 0.0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    total_ce_loss = 0  # 記錄交叉熵損失\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        reference_abstracts = batch[\"abstract\"]\n",
    "\n",
    "        outputs = train_segmented_input(model, input_ids, attention_mask, labels)\n",
    "        ce_loss = outputs.loss  # 提取 loss 屬性\n",
    "        total_ce_loss += ce_loss.item()  # 從張量中獲取標量值\n",
    "\n",
    "        # 每 5 個 batch 計算 ROUGE 損失（增加頻率）\n",
    "        if i % 5 == 0:\n",
    "            rouge1, rougeL = compute_rouge_scores(model, input_ids, attention_mask, reference_abstracts)\n",
    "            reward_rouge1 = torch.tensor(rouge1, device=device)\n",
    "            reward_rougeL = torch.tensor(rougeL, device=device)\n",
    "        else:\n",
    "            reward_rouge1 = 0.0\n",
    "            reward_rougeL = 0.0\n",
    "\n",
    "        # 每 10 個 batch 計算 BERTScore 損失（增加頻率）\n",
    "        if i % 10 == 0:\n",
    "            bertscore_f1 = compute_bertscore_loss(model, input_ids, attention_mask, reference_abstracts)\n",
    "            reward_bertscore = torch.tensor(bertscore_f1, device=device)\n",
    "        else:\n",
    "            reward_bertscore = 0.0\n",
    "        \n",
    "        loss = ce_loss - 0.3 * reward_rouge1 - 0.1 * reward_rougeL - 0.05 * reward_bertscore\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # 梯度裁剪，防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    if (i + 1) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_ce_loss = total_ce_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_loss:.4f}, CE Loss: {avg_ce_loss:.4f}\")\n",
    "\n",
    "    # 驗證階段：計算損失並選擇性進行評估\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_ce_loss = 0\n",
    "    predicted_abstracts = []\n",
    "    reference_abstracts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            reference_abstract = batch[\"abstract\"]\n",
    "\n",
    "            # 計算驗證損失\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_ce_loss += outputs.loss.item()\n",
    "\n",
    "            # 僅在需要評估時生成摘要\n",
    "            if (epoch + 1) % eval_frequency == 0:\n",
    "                generated_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=660,\n",
    "                    min_length=50,\n",
    "                    num_beams=15,\n",
    "                    length_penalty=1.5,\n",
    "                    repetition_penalty=1.2,\n",
    "                    early_stopping=True,\n",
    "                    no_repeat_ngram_size=3\n",
    "                )\n",
    "                generated_abstract = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                predicted_abstracts.append(generated_abstract)\n",
    "                reference_abstracts.append(reference_abstract[0])\n",
    "\n",
    "    val_ce_loss /= len(val_loader)\n",
    "    val_loss = val_ce_loss\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # 每隔 eval_frequency 個 epoch 進行評估\n",
    "    current_rouge1 = 0.0  # 預設值，確保早停邏輯正常運行\n",
    "    if (epoch + 1) % eval_frequency == 0:\n",
    "        # 計算 ROUGE 和 BERTScore\n",
    "        metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "        metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "        rouge_scores = metric_rouge.compute(\n",
    "            predictions=predicted_abstracts,\n",
    "            references=reference_abstracts,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "\n",
    "        bert_scores = metric_bertscore.compute(\n",
    "            predictions=predicted_abstracts,\n",
    "            references=reference_abstracts,\n",
    "            lang=\"en\"\n",
    "        )\n",
    "\n",
    "        # 提取評估分數\n",
    "        current_rouge1 = rouge_scores['rouge1']\n",
    "        current_rouge2 = rouge_scores['rouge2']\n",
    "        current_rougeL = rouge_scores['rougeL']\n",
    "        current_bertscore_f1 = np.mean(bert_scores['f1'])\n",
    "\n",
    "        # 打印評估結果並與 baseline 比較\n",
    "        print(\"\\n=== Validation Evaluation Results ===\")\n",
    "        print(\"ROUGE Scores:\")\n",
    "        print(f\"ROUGE-1: {current_rouge1:.4f} (Baseline: {baseline_scores['rouge1']:.4f}, Diff: {current_rouge1 - baseline_scores['rouge1']:.4f})\")\n",
    "        print(f\"ROUGE-2: {current_rouge2:.4f} (Baseline: {baseline_scores['rouge2']:.4f}, Diff: {current_rouge2 - baseline_scores['rouge2']:.4f})\")\n",
    "        print(f\"ROUGE-L: {current_rougeL:.4f} (Baseline: {baseline_scores['rougeL']:.4f}, Diff: {current_rougeL - baseline_scores['rougeL']:.4f})\")\n",
    "        print(\"\\nBERTScore:\")\n",
    "        print(f\"Precision: {np.mean(bert_scores['precision']):.4f}\")\n",
    "        print(f\"Recall: {np.mean(bert_scores['recall']):.4f}\")\n",
    "        print(f\"F1: {current_bertscore_f1:.4f} (Baseline: {baseline_scores['bertscore_f1']:.4f}, Diff: {current_bertscore_f1 - baseline_scores['bertscore_f1']:.4f})\")\n",
    "\n",
    "    # 溫暖啟動調度\n",
    "    warm_up_scheduler.step()\n",
    "    # 動態調整學習率\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 保存最佳模型（基於 ROUGE-1）\n",
    "    if current_rouge1 > best_rouge1:\n",
    "        best_rouge1 = current_rouge1\n",
    "        patience_counter = 0\n",
    "        best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'val_loss': val_loss,\n",
    "        }, best_model_path)\n",
    "        print(f\"Best model saved at {best_model_path} with ROUGE-1 {best_rouge1:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement in ROUGE-1\")\n",
    "            break\n",
    "\n",
    "    # 保存最佳評估分數（基於 ROUGE-L 和 BERTScore F1）\n",
    "    if (epoch + 1) % eval_frequency == 0:\n",
    "        if current_rougeL > best_rougeL or current_bertscore_f1 > best_bertscore_f1:\n",
    "            best_rouge2 = max(best_rouge2, current_rouge2)\n",
    "            best_rougeL = max(best_rougeL, current_rougeL)\n",
    "            best_bertscore_f1 = max(best_bertscore_f1, current_bertscore_f1)\n",
    "            print(f\"New best evaluation scores: ROUGE-L = {best_rougeL:.4f}, BERTScore F1 = {best_bertscore_f1:.4f}\")\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch + 1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 打印最終最佳評估分數\n",
    "print(\"\\n=== Final Best Evaluation Scores ===\")\n",
    "print(f\"Best ROUGE-1: {best_rouge1:.4f} (Baseline: {baseline_scores['rouge1']:.4f})\")\n",
    "print(f\"Best ROUGE-2: {best_rouge2:.4f} (Baseline: {baseline_scores['rouge2']:.4f})\")\n",
    "print(f\"Best ROUGE-L: {best_rougeL:.4f} (Baseline: {baseline_scores['rougeL']:.4f})\")\n",
    "print(f\"Best BERTScore F1: {best_bertscore_f1:.4f} (Baseline: {baseline_scores['bertscore_f1']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推理階段(後處理可加BART輔助)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, BartForConditionalGeneration, BartTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 加載 spacy 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 清理 LaTeX 和亂碼的函數\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F’‘–—∑∂∇∞θπ𝒟𝒫𝒩αβγδελμσφωℝ𝔽𝓛()]', ' ', text)  # 保留括號\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 設置設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. 加載 Pegasus-ArXiv 模型和 tokenizer\n",
    "model_name = \"google/pegasus-arxiv\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 2. 加載數據集並清理文本\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "test_data = load_json(\"test.json\")  # 103 個樣本\n",
    "\n",
    "# 3. 使用 sentence-transformers 改進截斷策略，引入領域知識\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def select_important_sentences(introduction, tokenizer, max_length=1024):\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    sentences = [\n",
    "        s for s in sentences\n",
    "        if not re.match(r'^\\[\\d+\\]$', s.strip()) and         # 過濾 [12]\n",
    "        not re.search(r'^\\s*(features|methods|results)?\\s*\\d{4}\\s*$', s.strip(), re.IGNORECASE) and  # 過濾 \"features 2021\"\n",
    "        len(s.strip().split()) > 3  # 長度太短通常沒資訊\n",
    "    ]\n",
    "        \n",
    "    if len(sentences) <= 1:\n",
    "        tokens = tokenizer(introduction, truncation=True, max_length=max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "    embeddings = embedder.encode(sentences)\n",
    "    doc_embedding = np.mean(embeddings, axis=0)\n",
    "    similarities = cosine_similarity(embeddings, doc_embedding.reshape(1, -1)).flatten()\n",
    "\n",
    "    academic_keywords = [\"propose\", \"method\", \"approach\", \"result\", \"finding\", \"conclusion\",\n",
    "                        \"demonstrate\", \"show\", \"achieve\", \"contribution\", \"investigate\", \"study\",\n",
    "                        \"analysis\", \"evaluate\", \"performance\", \"improve\", \"novel\", \"framework\"]\n",
    "    scores = similarities.copy()\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if any(keyword in sentence.lower() for keyword in academic_keywords):\n",
    "            scores[idx] *= 1.5\n",
    "\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    selected_sentences = []\n",
    "    current_length = 0\n",
    "    for idx in sorted_indices:\n",
    "        sentence = sentences[idx]\n",
    "        tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        if current_length + len(tokens) <= max_length - 2:\n",
    "            selected_sentences.append(sentence)\n",
    "            current_length += len(tokens)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    truncated_text = \" \".join(selected_sentences)\n",
    "    tokens = tokenizer(truncated_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "# 4. 定義測試數據集（用於推理）\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        input_ids, attention_mask = select_important_sentences(introduction, self.tokenizer, self.max_input_length)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"introduction\": introduction  # 保存原始引言以供後處理使用\n",
    "        }\n",
    "\n",
    "# 創建測試 DataLoader\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer, max_input_length=1024)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 5. 後處理函數：基礎清理和關鍵詞保留\n",
    "def clean_abstract(abstract):\n",
    "    abstract = clean_text(abstract)\n",
    "    words = abstract.split()\n",
    "    while len(words) > 1 and words[0] == \"In\" and words[1] == \"In\":\n",
    "        words.pop(0)\n",
    "    abstract = \" \".join(words)\n",
    "    sentences = abstract.split(\". \")\n",
    "    seen_sentences = set()\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence and sentence not in seen_sentences:\n",
    "            seen_sentences.add(sentence)\n",
    "            cleaned_sentences.append(sentence)\n",
    "    abstract = \". \".join(cleaned_sentences)\n",
    "    if abstract and not abstract.endswith(\".\"):\n",
    "        abstract += \".\"\n",
    "    return abstract\n",
    "\n",
    "def extract_keywords(text, top_n=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.toarray()[0]\n",
    "    keyword_indices = scores.argsort()[-top_n:][::-1]\n",
    "    keywords = [feature_names[idx] for idx in keyword_indices]\n",
    "    return keywords\n",
    "\n",
    "def ensure_keywords_in_abstract(abstract, original_text):\n",
    "    keywords = extract_keywords(original_text, top_n=10)\n",
    "    abstract_words = abstract.split()\n",
    "    if len(abstract_words) > 660:\n",
    "        abstract = \" \".join(abstract_words[:660])\n",
    "    abstract_lower = abstract.lower()\n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() not in abstract_lower:\n",
    "            abstract += f\" {keyword}\"\n",
    "    return abstract\n",
    "\n",
    "# 6. 使用 BART 進行後處理\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").to(device)\n",
    "\n",
    "def enhance_abstract_with_bart(abstract):\n",
    "    inputs = bart_tokenizer(abstract, return_tensors=\"pt\", max_length=660, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    outputs = bart_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=660,\n",
    "        min_length=100,\n",
    "        num_beams=12,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=1.2,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    enhanced = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_abstract(enhanced)\n",
    "\n",
    "# 7. 推理階段：分段生成（針對 test_data）\n",
    "checkpoint_dir = \"pegasus-arxiv_checkpoints\"\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path} (epoch {checkpoint['epoch']}, loss {checkpoint['loss']:.4f})\")\n",
    "else:\n",
    "    print(f\"Checkpoint {checkpoint_path} not found, using the last trained model.\")\n",
    "\n",
    "# 設置模型為評估模式\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "def generate_segmented_abstract(input_ids, attention_mask, segment_length=1024, overlap=128):\n",
    "    total_length = input_ids.shape[1]\n",
    "    if total_length <= segment_length:\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=660,\n",
    "            min_length=50,\n",
    "            num_beams=6,  \n",
    "            repetition_penalty=1.0,  \n",
    "            length_penalty=1.5,  \n",
    "            early_stopping=False\n",
    "        )\n",
    "        return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    segments = []\n",
    "    start = 0\n",
    "    while start < total_length:\n",
    "        end = min(start + segment_length, total_length)\n",
    "        segment_input_ids = input_ids[:, start:end]\n",
    "        segment_attention_mask = attention_mask[:, start:end]\n",
    "        \n",
    "        if segment_input_ids.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=segment_input_ids,\n",
    "            attention_mask=segment_attention_mask,\n",
    "            max_length=660,\n",
    "            min_length=50,\n",
    "            num_beams=6,\n",
    "            repetition_penalty=1.0,\n",
    "            length_penalty=1.5,\n",
    "            early_stopping=False\n",
    "        )\n",
    "        segments.append(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "\n",
    "        # 移動下一段，帶有重疊區\n",
    "        start = end - overlap\n",
    "\n",
    "    return \" \".join(segments)\n",
    "\n",
    "\n",
    "test_paper_ids = [item[\"paper_id\"] for item in test_data] \n",
    "prediction_dict = {}  \n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(test_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        paper_ids = batch[\"paper_id\"]\n",
    "        introductions = batch[\"introduction\"]\n",
    "\n",
    "        for i in range(len(paper_ids)):\n",
    "            # 分段生成\n",
    "            abstract = generate_segmented_abstract(input_ids[i:i+1], attention_mask[i:i+1], segment_length=1024)\n",
    "            # 基礎清理\n",
    "            abstract = clean_abstract(abstract)\n",
    "            # 使用 BART 後處理\n",
    "            abstract = enhance_abstract_with_bart(abstract)\n",
    "            # 確保關鍵詞保留\n",
    "            abstract = ensure_keywords_in_abstract(abstract, introductions[i])\n",
    "\n",
    "            # 獲取當前 paper_id\n",
    "            paper_id = paper_ids[i]\n",
    "            if isinstance(paper_id, torch.Tensor):\n",
    "                paper_id = str(paper_id.item())\n",
    "            elif isinstance(paper_id, np.ndarray):\n",
    "                paper_id = str(paper_id.item())\n",
    "            else:\n",
    "                paper_id = str(paper_id)\n",
    "\n",
    "            # 存儲預測結果\n",
    "            abstract = str(abstract) if not isinstance(abstract, str) else abstract\n",
    "            prediction_dict[paper_id] = abstract\n",
    "\n",
    "# 按照 test.json 的 paper_id 順序生成 predictions\n",
    "predictions = []\n",
    "for paper_id in test_paper_ids:\n",
    "    paper_id_str = str(paper_id)\n",
    "    if paper_id_str in prediction_dict:\n",
    "        predictions.append({\n",
    "            \"paper_id\": paper_id_str,\n",
    "            \"abstract\": prediction_dict[paper_id_str]\n",
    "        })\n",
    "    else:\n",
    "        predictions.append({\n",
    "            \"paper_id\": paper_id_str,\n",
    "            \"abstract\": \"\"\n",
    "        })\n",
    "\n",
    "# 9. 保存預測結果\n",
    "with open(\"submission_arxiv.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for pred in predictions:\n",
    "        f.write(json.dumps(pred, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Predictions saved to submission_arxiv.json, total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, BartForConditionalGeneration, BartTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 加載 spacy 模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 清理 LaTeX 和亂碼的函數\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F’‘–—∑∂∇∞θπ𝒟𝒫𝒩αβγδελμσφωℝ𝔽𝓛()]', ' ', text)  # 保留括號\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 設置設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. 加載 Pegasus-ArXiv 模型和 tokenizer\n",
    "model_name = \"google/pegasus-arxiv\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 2. 加載數據集並清理文本\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "test_data = load_json(\"test.json\")  # 103 個樣本\n",
    "\n",
    "# 3. 使用 sentence-transformers 改進截斷策略，引入領域知識\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def select_important_sentences(introduction, tokenizer, max_length=1024):\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    if len(sentences) <= 1:\n",
    "        tokens = tokenizer(introduction, truncation=True, max_length=max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "    embeddings = embedder.encode(sentences)\n",
    "    doc_embedding = np.mean(embeddings, axis=0)\n",
    "    similarities = cosine_similarity(embeddings, doc_embedding.reshape(1, -1)).flatten()\n",
    "\n",
    "    academic_keywords = [\"propose\", \"method\", \"approach\", \"result\", \"finding\", \"conclusion\",\n",
    "                        \"demonstrate\", \"show\", \"achieve\", \"contribution\", \"investigate\", \"study\",\n",
    "                        \"analysis\", \"evaluate\", \"performance\", \"improve\", \"novel\", \"framework\"]\n",
    "    scores = similarities.copy()\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if any(keyword in sentence.lower() for keyword in academic_keywords):\n",
    "            scores[idx] *= 1.5\n",
    "\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    selected_sentences = []\n",
    "    current_length = 0\n",
    "    for idx in sorted_indices:\n",
    "        sentence = sentences[idx]\n",
    "        tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        if current_length + len(tokens) <= max_length - 2:\n",
    "            selected_sentences.append(sentence)\n",
    "            current_length += len(tokens)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    truncated_text = \" \".join(selected_sentences)\n",
    "    tokens = tokenizer(truncated_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "# 4. 定義測試數據集（用於推理）\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        input_ids, attention_mask = select_important_sentences(introduction, self.tokenizer, self.max_input_length)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"introduction\": introduction  # 保存原始引言以供後處理使用\n",
    "        }\n",
    "\n",
    "# 創建測試 DataLoader\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer, max_input_length=1024)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 5. 後處理函數：基礎清理和關鍵詞保留\n",
    "def clean_abstract(abstract):\n",
    "    abstract = clean_text(abstract)\n",
    "    words = abstract.split()\n",
    "    while len(words) > 1 and words[0] == \"In\" and words[1] == \"In\":\n",
    "        words.pop(0)\n",
    "    abstract = \" \".join(words)\n",
    "    sentences = abstract.split(\". \")\n",
    "    seen_sentences = set()\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence and sentence not in seen_sentences:\n",
    "            seen_sentences.add(sentence)\n",
    "            cleaned_sentences.append(sentence)\n",
    "    abstract = \". \".join(cleaned_sentences)\n",
    "    if abstract and not abstract.endswith(\".\"):\n",
    "        abstract += \".\"\n",
    "    return abstract\n",
    "\n",
    "def extract_keywords(text, top_n=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.toarray()[0]\n",
    "    keyword_indices = scores.argsort()[-top_n:][::-1]\n",
    "    keywords = [feature_names[idx] for idx in keyword_indices]\n",
    "    return keywords\n",
    "\n",
    "def ensure_keywords_in_abstract(abstract, original_text):\n",
    "    keywords = extract_keywords(original_text, top_n=10)\n",
    "    abstract_words = abstract.split()\n",
    "    if len(abstract_words) > 660:\n",
    "        abstract = \" \".join(abstract_words[:660])\n",
    "    abstract_lower = abstract.lower()\n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() not in abstract_lower:\n",
    "            abstract += f\" {keyword}\"\n",
    "    return abstract\n",
    "\n",
    "# 6. 使用 BART 進行後處理\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").to(device)\n",
    "\n",
    "def enhance_abstract_with_bart(abstract):\n",
    "    inputs = bart_tokenizer(abstract, return_tensors=\"pt\", max_length=660, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    outputs = bart_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=660,\n",
    "        min_length=100,\n",
    "        num_beams=12,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=1.2,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    enhanced = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_abstract(enhanced)\n",
    "\n",
    "# 7. 推理階段：分段生成（針對 test_data）\n",
    "checkpoint_dir = \"pegasus-arxiv_checkpoints\"\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path} (epoch {checkpoint['epoch']}, loss {checkpoint['loss']:.4f})\")\n",
    "else:\n",
    "    print(f\"Checkpoint {checkpoint_path} not found, using the last trained model.\")\n",
    "\n",
    "# 設置模型為評估模式\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "def generate_segmented_abstract(input_ids, attention_mask, segment_length=1024, overlap=128):\n",
    "    total_length = input_ids.shape[1]\n",
    "    if total_length <= segment_length:\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=660,\n",
    "            min_length=50,\n",
    "            num_beams=6,  \n",
    "            repetition_penalty=1.0,  \n",
    "            length_penalty=1.5,  \n",
    "            early_stopping=False\n",
    "        )\n",
    "        return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    segments = []\n",
    "    start = 0\n",
    "    while start < total_length:\n",
    "        end = min(start + segment_length, total_length)\n",
    "        segment_input_ids = input_ids[:, start:end]\n",
    "        segment_attention_mask = attention_mask[:, start:end]\n",
    "        \n",
    "        if segment_input_ids.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=segment_input_ids,\n",
    "            attention_mask=segment_attention_mask,\n",
    "            max_length=660,\n",
    "            min_length=50,\n",
    "            num_beams=6,\n",
    "            repetition_penalty=1.0,\n",
    "            length_penalty=1.5,\n",
    "            early_stopping=False\n",
    "        )\n",
    "        segments.append(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "\n",
    "        # 移動下一段，帶有重疊區\n",
    "        start = end - overlap\n",
    "\n",
    "    return \" \".join(segments)\n",
    "\n",
    "\n",
    "test_paper_ids = [item[\"paper_id\"] for item in test_data] \n",
    "prediction_dict = {}  \n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(test_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        paper_ids = batch[\"paper_id\"]\n",
    "        introductions = batch[\"introduction\"]\n",
    "\n",
    "        for i in range(len(paper_ids)):\n",
    "            # 分段生成\n",
    "            abstract = generate_segmented_abstract(input_ids[i:i+1], attention_mask[i:i+1], segment_length=1024)\n",
    "            # 基礎清理\n",
    "            abstract = clean_abstract(abstract)\n",
    "            # 使用 BART 後處理\n",
    "            abstract = enhance_abstract_with_bart(abstract)\n",
    "            # 確保關鍵詞保留\n",
    "            abstract = ensure_keywords_in_abstract(abstract, introductions[i])\n",
    "\n",
    "            # 獲取當前 paper_id\n",
    "            paper_id = paper_ids[i]\n",
    "            if isinstance(paper_id, torch.Tensor):\n",
    "                paper_id = str(paper_id.item())\n",
    "            elif isinstance(paper_id, np.ndarray):\n",
    "                paper_id = str(paper_id.item())\n",
    "            else:\n",
    "                paper_id = str(paper_id)\n",
    "\n",
    "            # 存儲預測結果\n",
    "            abstract = str(abstract) if not isinstance(abstract, str) else abstract\n",
    "            prediction_dict[paper_id] = abstract\n",
    "\n",
    "# 按照 test.json 的 paper_id 順序生成 predictions\n",
    "predictions = []\n",
    "for paper_id in test_paper_ids:\n",
    "    paper_id_str = str(paper_id)\n",
    "    if paper_id_str in prediction_dict:\n",
    "        predictions.append({\n",
    "            \"paper_id\": paper_id_str,\n",
    "            \"abstract\": prediction_dict[paper_id_str]\n",
    "        })\n",
    "    else:\n",
    "        predictions.append({\n",
    "            \"paper_id\": paper_id_str,\n",
    "            \"abstract\": \"\"\n",
    "        })\n",
    "\n",
    "# 9. 保存預測結果\n",
    "with open(\"submission_arxiv.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for pred in predictions:\n",
    "        f.write(json.dumps(pred, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Predictions saved to submission_arxiv.json, total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微調訓練階段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import spacy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import LEDForConditionalGeneration, LEDTokenizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Clean LaTeX and noise\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\\\[\\w]+\\{.*?\\}', '', text)\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(r'\\[\\d+,\\s*\\d+\\]', '', text)\n",
    "    text = re.sub(r'Fig\\.\\s*\\d+.*?(?=\\.\\s|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+.*?(?=\\.\\s|$)', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?()-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load LED base model and tokenizer\n",
    "model_name = \"allenai/led-base-16384\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(model_name)\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(device)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load dataset\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "train_data = load_json(\"train.json\")\n",
    "test_data = load_json(\"test.json\")\n",
    "val_size = int(0.2 * len(train_data))\n",
    "val_data = train_data[-val_size:]\n",
    "train_data = train_data[:-val_size]\n",
    "\n",
    "# Truncation function\n",
    "def truncate_from_ends_with_importance(introduction, abstract=None, tokenizer=tokenizer, max_model_length=16384, attention_window=1024):\n",
    "    tokens = tokenizer.encode(introduction, add_special_tokens=True)\n",
    "    token_length = len(tokens)\n",
    "    min_length = 128\n",
    "    max_input_length = min(max_model_length, max(min_length, token_length))\n",
    "    max_input_length = ((max_input_length + attention_window - 1) // attention_window) * attention_window\n",
    "\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    if len(sentences) <= 1:\n",
    "        tokens = tokenizer(introduction, truncation=True, max_length=max_input_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze(), max_input_length\n",
    "\n",
    "    num_sentences = len(sentences)\n",
    "    proportion = 0.3 if 1024 <= token_length <= 4096 else (0.4 if token_length < 1024 else 0.2)\n",
    "    num_end_sentences = max(1, int(num_sentences * proportion))\n",
    "    start_sentences = sentences[:num_end_sentences]\n",
    "    end_sentences = sentences[-num_end_sentences:]\n",
    "    middle_sentences = sentences[num_end_sentences:-num_end_sentences]\n",
    "\n",
    "    start_tokens = tokenizer.encode(\" \".join(start_sentences), add_special_tokens=False)\n",
    "    end_tokens = tokenizer.encode(\" \".join(end_sentences), add_special_tokens=False)\n",
    "    current_token_count = len(start_tokens) + len(end_tokens)\n",
    "\n",
    "    selected_middle_sentences = []\n",
    "    if middle_sentences and abstract:\n",
    "        abstract_words = set(abstract.lower().split())\n",
    "        sentence_scores = []\n",
    "        for sent in middle_sentences:\n",
    "            sent_words = set(sent.lower().split())\n",
    "            overlap = len(sent_words & abstract_words) / len(sent_words) if sent_words else 0\n",
    "            sentence_scores.append(overlap)\n",
    "        sorted_middle_indices = np.argsort(sentence_scores)[::-1]\n",
    "        for idx in sorted_middle_indices:\n",
    "            sentence = middle_sentences[idx]\n",
    "            tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            if current_token_count + len(tokens) <= max_input_length - 2:\n",
    "                selected_middle_sentences.append(sentence)\n",
    "                current_token_count += len(tokens)\n",
    "            else:\n",
    "                break\n",
    "    elif middle_sentences:\n",
    "        vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "        tfidf_matrix = vectorizer.fit_transform(middle_sentences)\n",
    "        middle_scores = tfidf_matrix.sum(axis=1).A1\n",
    "        sorted_middle_indices = np.argsort(middle_scores)[::-1]\n",
    "        for idx in sorted_middle_indices:\n",
    "            sentence = middle_sentences[idx]\n",
    "            tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            if current_token_count + len(tokens) <= max_input_length - 2:\n",
    "                selected_middle_sentences.append(sentence)\n",
    "                current_token_count += len(tokens)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    truncated_introduction = \" \".join(start_sentences + selected_middle_sentences + end_sentences)\n",
    "    tokens = tokenizer(truncated_introduction, max_length=max_input_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze(), max_input_length\n",
    "\n",
    "# Dataset definitions\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_target_length=800):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids, attention_mask, max_input_length = truncate_from_ends_with_importance(item[\"introduction\"], item[\"abstract\"], self.tokenizer)\n",
    "        targets = self.tokenizer(item[\"abstract\"], max_length=self.max_target_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": target_ids,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": item[\"abstract\"],\n",
    "            \"max_input_length\": max_input_length\n",
    "        }\n",
    "\n",
    "# Collate function\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    max_input_lengths = [item[\"max_input_length\"] for item in batch]\n",
    "    max_length = max(max_input_lengths)\n",
    "    input_ids_padded = torch.zeros((len(batch), max_length), dtype=torch.long)\n",
    "    attention_masks_padded = torch.zeros((len(batch), max_length), dtype=torch.long)\n",
    "    for i in range(len(batch)):\n",
    "        length = input_ids[i].size(0)\n",
    "        input_ids_padded[i, :length] = input_ids[i]\n",
    "        attention_masks_padded[i, :length] = attention_masks[i]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    labels_padded = torch.stack(labels)\n",
    "    abstracts = [item[\"abstract\"] for item in batch]\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": attention_masks_padded,\n",
    "        \"labels\": labels_padded,\n",
    "        \"paper_id\": [item[\"paper_id\"] for item in batch],\n",
    "        \"abstract\": abstracts,\n",
    "        \"max_input_length\": max_input_lengths\n",
    "    }\n",
    "\n",
    "# Dataloader\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer)\n",
    "val_dataset = TrainPaperDataset(val_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Training settings\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "num_epochs = 300\n",
    "eval_frequency = 5\n",
    "scaler = GradScaler()\n",
    "\n",
    "warm_up_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min((epoch + 1) / 20, 1.0))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=150, eta_min=1e-6)\n",
    "\n",
    "checkpoint_dir = \"led-arxiv_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "val_loss_window = []\n",
    "window_size = 3\n",
    "patience_counter = 0\n",
    "patience = 100\n",
    "\n",
    "baseline_scores = {\"rouge1\": 0.47, \"rouge2\": 0.12, \"rougeL\": 0.22, \"bertscore_f1\": 0.85}\n",
    "best_rouge1 = 0.0\n",
    "best_rouge2 = 0.0\n",
    "best_rougeL = 0.0\n",
    "best_bertscore_f1 = 0.0\n",
    "\n",
    "metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, label_smoothing=0.1)\n",
    "            loss = loss_fct(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    val_loss_window.append(val_loss)\n",
    "    if len(val_loss_window) > window_size:\n",
    "        val_loss_window.pop(0)\n",
    "    smoothed_val_loss = sum(val_loss_window) / len(val_loss_window)\n",
    "\n",
    "    if smoothed_val_loss < best_val_loss:\n",
    "        best_val_loss = smoothed_val_loss\n",
    "        patience_counter = 0\n",
    "        model.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora\"))\n",
    "        tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora\"))\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # === ROUGE/BERTScore Evaluation every `eval_frequency` ===\n",
    "    if (epoch + 1) % eval_frequency == 0:\n",
    "        model.eval()\n",
    "        predicted_abstracts = []\n",
    "        reference_abstracts = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                reference_abstract = batch[\"abstract\"][0]\n",
    "\n",
    "                generated_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=800,\n",
    "                    min_length=10,\n",
    "                    num_beams=15,\n",
    "                    length_penalty=1.0,\n",
    "                    repetition_penalty=1.1,\n",
    "                    early_stopping=True,\n",
    "                    no_repeat_ngram_size=3\n",
    "                )\n",
    "                generated_abstract = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                predicted_abstracts.append(generated_abstract)\n",
    "                reference_abstracts.append(reference_abstract)\n",
    "\n",
    "        rouge_scores = metric_rouge.compute(predictions=predicted_abstracts, references=reference_abstracts, use_stemmer=True)\n",
    "        bert_scores = metric_bertscore.compute(predictions=predicted_abstracts, references=reference_abstracts, lang=\"en\")\n",
    "\n",
    "        current_rouge1 = rouge_scores['rouge1']\n",
    "        current_rouge2 = rouge_scores['rouge2']\n",
    "        current_rougeL = rouge_scores['rougeL']\n",
    "        current_bertscore_f1 = np.mean(bert_scores['f1'])\n",
    "\n",
    "        print(\"\\n=== Validation Evaluation Results ===\")\n",
    "        print(f\"ROUGE-1: {current_rouge1:.4f} (Baseline: {baseline_scores['rouge1']:.4f})\")\n",
    "        print(f\"ROUGE-2: {current_rouge2:.4f} (Baseline: {baseline_scores['rouge2']:.4f})\")\n",
    "        print(f\"ROUGE-L: {current_rougeL:.4f} (Baseline: {baseline_scores['rougeL']:.4f})\")\n",
    "        print(f\"BERTScore F1: {current_bertscore_f1:.4f} (Baseline: {baseline_scores['bertscore_f1']:.4f})\")\n",
    "\n",
    "        if current_rouge1 > best_rouge1:\n",
    "            best_rouge1 = current_rouge1\n",
    "            best_rouge2 = current_rouge2\n",
    "            best_rougeL = current_rougeL\n",
    "            best_bertscore_f1 = current_bertscore_f1\n",
    "            print(f\"New best ROUGE-1: {best_rouge1:.4f}\")\n",
    "\n",
    "    warm_up_scheduler.step()\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "\n",
    "# Save final model\n",
    "model.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora_final\"))\n",
    "tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora_final\"))\n",
    "print(\"Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import html\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from transformers import LEDForConditionalGeneration, LEDTokenizer, PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from peft import PeftModel\n",
    "\n",
    "# === 設定 ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === 載入模型 ===\n",
    "base_model = \"allenai/led-base-16384\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(base_model)\n",
    "model = LEDForConditionalGeneration.from_pretrained(base_model)\n",
    "model = PeftModel.from_pretrained(model, \"led-arxiv_checkpoints/led-arxiv_lora\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\").to(device).eval()\n",
    "\n",
    "flan_t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "flan_t5_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\").to(device).eval()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# === 工具函數 ===\n",
    "def clean_text(text):\n",
    "    # decode escape characters\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # 替換常見 unicode 標點為 ASCII（em dash、smart quotes）\n",
    "    text = text.replace('\\u2014', '-')      # em dash\n",
    "    text = text.replace('\\u2013', '-')      # en dash\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')  # quotes\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # apostrophes\n",
    "\n",
    "    # 移除 LaTeX 符號，但保留括號內文字\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
    "    text = re.sub(r'\\$([^\\$]*?)\\$', r'\\1', text)  # 保留公式文字內容\n",
    "\n",
    "    # 移除文獻引用，例如 [1], [2, 3]\n",
    "    text = re.sub(r'\\[(\\d+|\\d+,\\s*\\d+)\\]', '', text)\n",
    "\n",
    "    # 移除圖表描述，但保留章節參考\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "\n",
    "    # 避免把 \"-\" 破壞，例如 DP-SGD, self-supervised\n",
    "    # 只清除非語意相關符號\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?()/%=:\\-+<>_\\[\\]\\\"\\'’]', ' ', text)\n",
    "\n",
    "    # 去除多餘空白\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text, top_n=15):\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\", max_features=100)\n",
    "    matrix = tfidf.fit_transform([text])\n",
    "    scores = matrix.toarray()[0]\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    top_indices = scores.argsort()[-top_n:][::-1]\n",
    "    return [feature_names[i] for i in top_indices if len(feature_names[i]) > 2]\n",
    "\n",
    "def ensure_keywords_in_abstract(abstract, intro, top_n=15):\n",
    "    keywords = extract_keywords(intro, top_n)\n",
    "    abstract = clean_text(abstract)\n",
    "    doc = nlp(abstract)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    missing = [kw for kw in keywords if kw.lower() not in abstract.lower()]\n",
    "    if not missing:\n",
    "        return abstract\n",
    "    \n",
    "    for kw in missing[:5]:\n",
    "        for i, sent in enumerate(sentences):\n",
    "            if len(sent.split()) > 5:\n",
    "                sentences[i] = f\"{sent.rstrip('.')} including {kw}.\"\n",
    "                break\n",
    "            elif i == len(sentences) - 1:\n",
    "                sentences[i] += f\" {kw} is considered.\"\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "def enhance_with_pegasus(text):\n",
    "    text = clean_text(text)\n",
    "    inputs = pegasus_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    outputs = pegasus_model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,\n",
    "        min_length=150,\n",
    "        num_beams=8,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    abstract = pegasus_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_text(abstract)\n",
    "\n",
    "def refine_with_flan_t5(intro, abstract):\n",
    "    # 使用 Flan-T5 修正摘要，確保與原文一致\n",
    "    prompt = f\"summarize and refine the following text to make it concise and accurate:\\nIntroduction: {intro[:1000]}\\nGenerated abstract: {abstract}\"\n",
    "    inputs = flan_t5_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=600).to(device)\n",
    "    outputs = flan_t5_model.generate(\n",
    "        **inputs,\n",
    "        max_length=600,  # Flan-T5 傾向生成較精簡的內容\n",
    "        min_length=150,\n",
    "        num_beams=6,    # 增加 beam 數以提升品質\n",
    "        length_penalty=0.8,  # 稍微偏向較短輸出\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    refined_abstract = flan_t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_text(refined_abstract)\n",
    "\n",
    "# === 載入資料 ===\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "    return data\n",
    "\n",
    "def truncate_from_ends_with_importance(introduction, tokenizer, max_length=8192):\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
    "    if len(sentences) <= 1:\n",
    "        encoded = tokenizer(introduction, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "    embeddings = embedder.encode(sentences, convert_to_tensor=True)\n",
    "    intro_embedding = embedder.encode(introduction, convert_to_tensor=True)\n",
    "    similarities = cosine_similarity(embeddings.cpu().numpy(), intro_embedding.cpu().numpy().reshape(1, -1)).flatten()\n",
    "    weights = np.linspace(1.5, 1.0, len(sentences))\n",
    "    scores = similarities * weights\n",
    "\n",
    "    keywords = extract_keywords(introduction, top_n=5)\n",
    "    intro_with_keywords = f\"Keywords: {', '.join(keywords)}. {' '.join(sentences)}\"\n",
    "\n",
    "    sorted_idx = np.argsort(scores)[::-1]\n",
    "    selected = []\n",
    "    cur_len = 0\n",
    "    for i in sorted_idx:\n",
    "        t = tokenizer.encode(sentences[i], add_special_tokens=False)\n",
    "        if cur_len + len(t) <= max_length - len(tokenizer.encode(f\"Keywords: {', '.join(keywords)}. \", add_special_tokens=False)) - 2:\n",
    "            selected.append(sentences[i])\n",
    "            cur_len += len(t)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    truncated = f\"Keywords: {', '.join(keywords)}. {' '.join(selected)}\"\n",
    "    encoded = tokenizer(truncated, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids, attention_mask = truncate_from_ends_with_importance(item[\"introduction\"], self.tokenizer)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"introduction\": item[\"introduction\"]\n",
    "        }\n",
    "\n",
    "# === 開始推理 ===\n",
    "test_data = load_json(\"test.json\")\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "submission = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        input_ids = input_ids.unsqueeze(0) if input_ids.dim() == 1 else input_ids\n",
    "        attention_mask = attention_mask.unsqueeze(0) if attention_mask.dim() == 1 else attention_mask\n",
    "\n",
    "        paper_id_raw = batch[\"paper_id\"][0]\n",
    "        paper_id = re.search(r'\\d+', str(paper_id_raw)).group()\n",
    "        intro = batch[\"introduction\"][0]\n",
    "\n",
    "        # LED 生成\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=10,\n",
    "            max_length=1000,\n",
    "            min_length=200,\n",
    "            length_penalty=1.3,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        abstract = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        abstract = clean_text(abstract)\n",
    "\n",
    "        # PEGASUS 增強\n",
    "        abstract = enhance_with_pegasus(abstract)\n",
    "\n",
    "        # 確保關鍵詞\n",
    "        abstract = ensure_keywords_in_abstract(abstract, intro)\n",
    "\n",
    "        # Flan-T5 修正\n",
    "        abstract = refine_with_flan_t5(intro, abstract)\n",
    "\n",
    "        submission.append({\n",
    "            \"paper_id\": paper_id,\n",
    "            \"abstract\": abstract\n",
    "        })\n",
    "\n",
    "with open(\"submission_led.json\", \"w\") as f:\n",
    "    for item in submission:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(submission)} abstracts to submission_led.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import html\n",
    "import os\n",
    "import numpy as np\n",
    "import spacy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import LEDForConditionalGeneration, LEDTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Clean LaTeX and noise\n",
    "def clean_text(text):\n",
    "    # decode escape characters\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # 替換常見 unicode 標點為 ASCII（em dash、smart quotes）\n",
    "    text = text.replace('\\u2014', '-')      # em dash\n",
    "    text = text.replace('\\u2013', '-')      # en dash\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')  # quotes\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # apostrophes\n",
    "\n",
    "    # 移除 LaTeX 符號，但保留括號內文字\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
    "    text = re.sub(r'\\$([^\\$]*?)\\$', r'\\1', text)  # 保留公式文字內容\n",
    "\n",
    "    # 移除文獻引用，例如 [1], [2, 3]\n",
    "    text = re.sub(r'\\[(\\d+|\\d+,\\s*\\d+)\\]', '', text)\n",
    "\n",
    "    # 移除圖表描述，但保留章節參考\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "\n",
    "    # 避免把 \"-\" 破壞，例如 DP-SGD, self-supervised\n",
    "    # 只清除非語意相關符號\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?()/%=:\\-+<>_\\[\\]\\\"\\'’]', ' ', text)\n",
    "\n",
    "    # 去除多餘空白\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load LED base model and tokenizer\n",
    "model_name = \"allenai/led-base-16384\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(model_name)\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(device)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load dataset\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "train_data = load_json(\"train.json\")\n",
    "test_data = load_json(\"test.json\")\n",
    "val_size = int(0.1 * len(train_data))\n",
    "val_data = train_data[-val_size:]\n",
    "train_data = train_data[:-val_size]\n",
    "\n",
    "# Truncation function\n",
    "def truncate_full_text_only(introduction, abstract=None, tokenizer=tokenizer, max_model_length=16384, attention_window=1024):\n",
    "    tokens = tokenizer.encode(introduction, add_special_tokens=True)\n",
    "    token_length = len(tokens)\n",
    "    min_length = 128\n",
    "\n",
    "    max_input_length = min(max_model_length, max(min_length, token_length))\n",
    "    max_input_length = ((max_input_length + attention_window - 1) // attention_window) * attention_window\n",
    "\n",
    "    tokens = tokenizer(introduction, truncation=True, max_length=max_input_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze(), max_input_length\n",
    "\n",
    "# Dataset definitions\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_target_length=800):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids, attention_mask, max_input_length = truncate_full_text_only(item[\"introduction\"], item[\"abstract\"], self.tokenizer)\n",
    "        targets = self.tokenizer(item[\"abstract\"], max_length=self.max_target_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": target_ids,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": item[\"abstract\"],\n",
    "            \"max_input_length\": max_input_length\n",
    "        }\n",
    "\n",
    "# Collate function\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    max_input_lengths = [item[\"max_input_length\"] for item in batch]\n",
    "    max_length = max(max_input_lengths)\n",
    "    input_ids_padded = torch.zeros((len(batch), max_length), dtype=torch.long)\n",
    "    attention_masks_padded = torch.zeros((len(batch), max_length), dtype=torch.long)\n",
    "    for i in range(len(batch)):\n",
    "        length = input_ids[i].size(0)\n",
    "        input_ids_padded[i, :length] = input_ids[i]\n",
    "        attention_masks_padded[i, :length] = attention_masks[i]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    labels_padded = torch.stack(labels)\n",
    "    abstracts = [item[\"abstract\"] for item in batch]\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": attention_masks_padded,\n",
    "        \"labels\": labels_padded,\n",
    "        \"paper_id\": [item[\"paper_id\"] for item in batch],\n",
    "        \"abstract\": abstracts,\n",
    "        \"max_input_length\": max_input_lengths\n",
    "    }\n",
    "\n",
    "# Dataloader\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer)\n",
    "val_dataset = TrainPaperDataset(val_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=custom_collate_fn)\n",
    "gradient_accumulation_steps = 2  \n",
    "\n",
    "# Training settings\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 200\n",
    "eval_frequency = 10\n",
    "scaler = GradScaler()\n",
    "\n",
    "warm_up_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min((epoch + 1) / 20, 1.0))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=280, eta_min=1e-6)\n",
    "\n",
    "checkpoint_dir = \"led-arxiv_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "val_loss_window = []\n",
    "window_size = 3\n",
    "patience_counter = 0\n",
    "patience = 100\n",
    "\n",
    "baseline_scores = {\"rouge1\": 0.47, \"rouge2\": 0.12, \"rougeL\": 0.22, \"bertscore_f1\": 0.85}\n",
    "best_rouge1 = 0.0\n",
    "best_rouge2 = 0.0\n",
    "best_rougeL = 0.0\n",
    "best_bertscore_f1 = 0.0\n",
    "\n",
    "metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    val_loss_window.append(val_loss)\n",
    "    if len(val_loss_window) > window_size:\n",
    "        val_loss_window.pop(0)\n",
    "    smoothed_val_loss = sum(val_loss_window) / len(val_loss_window)\n",
    "\n",
    "    if smoothed_val_loss < best_val_loss:\n",
    "        best_val_loss = smoothed_val_loss\n",
    "        patience_counter = 0\n",
    "        model.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora\"))\n",
    "        tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora\"))\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # === ROUGE/BERTScore Evaluation every `eval_frequency` ===\n",
    "    if (epoch + 1) % eval_frequency == 0:\n",
    "        model.eval()\n",
    "        predicted_abstracts = []\n",
    "        reference_abstracts = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                reference_abstract = batch[\"abstract\"][0]\n",
    "\n",
    "                generated_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=800,\n",
    "                    min_length=100,\n",
    "                    num_beams=15,\n",
    "                    length_penalty=1.0,\n",
    "                    repetition_penalty=0.9,\n",
    "                    early_stopping=True,\n",
    "                    no_repeat_ngram_size=3\n",
    "                )\n",
    "                generated_abstract = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                predicted_abstracts.append(generated_abstract)\n",
    "                reference_abstracts.append(reference_abstract)\n",
    "\n",
    "        rouge_scores = metric_rouge.compute(predictions=predicted_abstracts, references=reference_abstracts, use_stemmer=True)\n",
    "        bert_scores = metric_bertscore.compute(predictions=predicted_abstracts, references=reference_abstracts, lang=\"en\")\n",
    "\n",
    "        current_rouge1 = rouge_scores['rouge1']\n",
    "        current_rouge2 = rouge_scores['rouge2']\n",
    "        current_rougeL = rouge_scores['rougeL']\n",
    "        current_bertscore_f1 = np.mean(bert_scores['f1'])\n",
    "\n",
    "        print(\"\\n=== Validation Evaluation Results ===\")\n",
    "        print(f\"ROUGE-1: {current_rouge1:.4f} (Baseline: {baseline_scores['rouge1']:.4f})\")\n",
    "        print(f\"ROUGE-2: {current_rouge2:.4f} (Baseline: {baseline_scores['rouge2']:.4f})\")\n",
    "        print(f\"ROUGE-L: {current_rougeL:.4f} (Baseline: {baseline_scores['rougeL']:.4f})\")\n",
    "        print(f\"BERTScore F1: {current_bertscore_f1:.4f} (Baseline: {baseline_scores['bertscore_f1']:.4f})\")\n",
    "\n",
    "        if current_rouge1 > best_rouge1:\n",
    "            best_rouge1 = current_rouge1\n",
    "            best_rouge2 = current_rouge2\n",
    "            best_rougeL = current_rougeL\n",
    "            best_bertscore_f1 = current_bertscore_f1\n",
    "            print(f\"New best ROUGE-1: {best_rouge1:.4f}\")\n",
    "\n",
    "    warm_up_scheduler.step()\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "\n",
    "# Save final model\n",
    "model.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora_final\"))\n",
    "tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora_final\"))\n",
    "print(\"Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import html\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from transformers import LEDForConditionalGeneration, LEDTokenizer, PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from peft import PeftModel\n",
    "\n",
    "# === 設定 ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === 載入模型 ===\n",
    "base_model = \"allenai/led-base-16384\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(base_model)\n",
    "model = LEDForConditionalGeneration.from_pretrained(base_model)\n",
    "model = PeftModel.from_pretrained(model, \"led-arxiv_checkpoints/led-arxiv_lora\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\").to(device).eval()\n",
    "\n",
    "flan_t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "flan_t5_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\").to(device).eval()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# === 工具函數 ===\n",
    "def clean_text(text):\n",
    "    # decode escape characters\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # 替換常見 unicode 標點為 ASCII（em dash、smart quotes）\n",
    "    text = text.replace('\\u2014', '-')      # em dash\n",
    "    text = text.replace('\\u2013', '-')      # en dash\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')  # quotes\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # apostrophes\n",
    "\n",
    "    # 移除 LaTeX 符號，但保留括號內文字\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
    "    text = re.sub(r'\\$([^\\$]*?)\\$', r'\\1', text)  # 保留公式文字內容\n",
    "\n",
    "    # 移除文獻引用，例如 [1], [2, 3]\n",
    "    text = re.sub(r'\\[(\\d+|\\d+,\\s*\\d+)\\]', '', text)\n",
    "\n",
    "    # 移除圖表描述，但保留章節參考\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "\n",
    "    # 避免把 \"-\" 破壞，例如 DP-SGD, self-supervised\n",
    "    # 只清除非語意相關符號\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?()/%=:\\-+<>_\\[\\]\\\"\\'’]', ' ', text)\n",
    "\n",
    "    # 去除多餘空白\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text, top_n=15):\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\", max_features=100)\n",
    "    matrix = tfidf.fit_transform([text])\n",
    "    scores = matrix.toarray()[0]\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    top_indices = scores.argsort()[-top_n:][::-1]\n",
    "    return [feature_names[i] for i in top_indices if len(feature_names[i]) > 2]\n",
    "\n",
    "def ensure_keywords_in_abstract(abstract, intro, top_n=15):\n",
    "    keywords = extract_keywords(intro, top_n)\n",
    "    abstract = clean_text(abstract)\n",
    "    doc = nlp(abstract)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    missing = [kw for kw in keywords if kw.lower() not in abstract.lower()]\n",
    "    if not missing:\n",
    "        return abstract\n",
    "    \n",
    "    for kw in missing[:5]:\n",
    "        for i, sent in enumerate(sentences):\n",
    "            if len(sent.split()) > 5:\n",
    "                sentences[i] = f\"{sent.rstrip('.')} including {kw}.\"\n",
    "                break\n",
    "            elif i == len(sentences) - 1:\n",
    "                sentences[i] += f\" {kw} is considered.\"\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "def enhance_with_pegasus(text):\n",
    "    text = clean_text(text)\n",
    "    inputs = pegasus_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    outputs = pegasus_model.generate(\n",
    "        **inputs,\n",
    "        max_length=700,\n",
    "        min_length=150,\n",
    "        num_beams=8,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    abstract = pegasus_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_text(abstract)\n",
    "\n",
    "def refine_with_flan_t5(intro, abstract):\n",
    "    # 使用 Flan-T5 修正摘要，確保與原文一致\n",
    "    prompt = f\"summarize and refine the following text to make it concise and accurate:\\nIntroduction: {intro[:1000]}\\nGenerated abstract: {abstract}\"\n",
    "    inputs = flan_t5_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=600).to(device)\n",
    "    outputs = flan_t5_model.generate(\n",
    "        **inputs,\n",
    "        max_length=600,  # Flan-T5 傾向生成較精簡的內容\n",
    "        min_length=150,\n",
    "        num_beams=6,    # 增加 beam 數以提升品質\n",
    "        length_penalty=0.8,  # 稍微偏向較短輸出\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    refined_abstract = flan_t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_text(refined_abstract)\n",
    "\n",
    "# === 載入資料 ===\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "    return data\n",
    "\n",
    "def truncate_from_ends_with_importance(introduction, tokenizer, max_length=8192):\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
    "    if len(sentences) <= 1:\n",
    "        encoded = tokenizer(introduction, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "    embeddings = embedder.encode(sentences, convert_to_tensor=True)\n",
    "    intro_embedding = embedder.encode(introduction, convert_to_tensor=True)\n",
    "\n",
    "    weights = np.linspace(1.5, 1.0, len(sentences))\n",
    "    scores = torch.nn.functional.cosine_similarity(embeddings, intro_embedding.unsqueeze(0), dim=1).cpu().numpy() * weights\n",
    "\n",
    "    keywords = extract_keywords(introduction, top_n=5)\n",
    "    sorted_idx = np.argsort(scores)[::-1]\n",
    "    selected = []\n",
    "    cur_len = 0\n",
    "    for i in sorted_idx:\n",
    "        t = tokenizer.encode(sentences[i], add_special_tokens=False)\n",
    "        if cur_len + len(t) <= max_length - 20:  # 預留 keywords 長度\n",
    "            selected.append(sentences[i])\n",
    "            cur_len += len(t)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    truncated = f\"Keywords: {', '.join(keywords)}. {' '.join(selected)}\"\n",
    "    encoded = tokenizer(truncated, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids, attention_mask = truncate_from_ends_with_importance(item[\"introduction\"], self.tokenizer)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"introduction\": item[\"introduction\"]\n",
    "        }\n",
    "\n",
    "# === 開始推理 ===\n",
    "test_data = load_json(\"test.json\")\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "submission = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        input_ids = input_ids.unsqueeze(0) if input_ids.dim() == 1 else input_ids\n",
    "        attention_mask = attention_mask.unsqueeze(0) if attention_mask.dim() == 1 else attention_mask\n",
    "\n",
    "        paper_id_raw = batch[\"paper_id\"][0]\n",
    "        paper_id = re.search(r'\\d+', str(paper_id_raw)).group()\n",
    "        intro = batch[\"introduction\"][0]\n",
    "\n",
    "        # === LED 生成初稿 ===\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=6,\n",
    "            max_length=800,\n",
    "            min_length=150,\n",
    "            length_penalty=1.0,\n",
    "            repetition_penalty=1.0,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        abstract = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        abstract = clean_text(abstract)\n",
    "\n",
    "        # === 條件性觸發 PEGASUS 強化 ===\n",
    "        if len(abstract.split()) < 150 or 'our method' not in abstract.lower():\n",
    "            abstract = enhance_with_pegasus(abstract)\n",
    "\n",
    "        # === 強化關鍵詞 ===\n",
    "        abstract = ensure_keywords_in_abstract(abstract, intro)\n",
    "\n",
    "        # === 根據語意相似度或關鍵詞缺失決定是否 T5 修正 ===\n",
    "        intro_embed = embedder.encode(intro, convert_to_tensor=True)\n",
    "        abs_embed = embedder.encode(abstract, convert_to_tensor=True)\n",
    "        similarity = torch.nn.functional.cosine_similarity(intro_embed, abs_embed, dim=0).item()\n",
    "\n",
    "        missing_keywords = [kw for kw in extract_keywords(intro)[:5] if kw not in abstract.lower()]\n",
    "\n",
    "        if similarity < 0.6 or len(missing_keywords) > 0:\n",
    "            abstract = refine_with_flan_t5(intro, abstract)\n",
    "\n",
    "        submission.append({\n",
    "            \"paper_id\": paper_id,\n",
    "            \"abstract\": abstract\n",
    "        })\n",
    "\n",
    "with open(\"submission_led.json\", \"w\") as f:\n",
    "    for item in submission:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(submission)} abstracts to submission_led.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LontT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "import html\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, LongT5ForConditionalGeneration\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import evaluate\n",
    "\n",
    "# === 基本設定 ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# === 清理文字 ===\n",
    "def clean_text(text):\n",
    "    text = html.unescape(text)\n",
    "    text = text.replace('\\u2014', '-')\n",
    "    text = text.replace('\\u2013', '-')\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
    "    text = re.sub(r'\\$([^\\$]*?)\\$', r'\\1', text)\n",
    "    text = re.sub(r'\\[(\\d+|\\d+,\\s*\\d+)\\]', '', text)\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?()/%=:\\-+<>_\\[\\]\"\\'’]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# === 載入模型與 Tokenizer ===\n",
    "model_name = \"google/long-t5-tglobal-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config).to(device)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# === 載入資料集 ===\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "train_data = load_json(\"train.json\")\n",
    "test_data = load_json(\"test.json\")\n",
    "val_size = int(0.1 * len(train_data))\n",
    "val_data = train_data[-val_size:]\n",
    "train_data = train_data[:-val_size]\n",
    "\n",
    "# === Tokenize function ===\n",
    "def tokenize_inputs(intro, tokenizer, max_length=7000):\n",
    "    tokens = tokenizer(intro, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    return tokens[\"input_ids\"].squeeze(0), tokens[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "# === Dataset 定義 ===\n",
    "class PaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_target_length=712):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids, attention_mask = tokenize_inputs(item[\"introduction\"], self.tokenizer)\n",
    "        target = self.tokenizer(item[\"abstract\"], max_length=self.max_target_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        labels = target[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": item[\"abstract\"]\n",
    "        }\n",
    "\n",
    "# === Collate function ===\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n",
    "    attention_mask = torch.stack([x[\"attention_mask\"] for x in batch])\n",
    "    labels = torch.stack([x[\"labels\"] for x in batch])\n",
    "    abstracts = [x[\"abstract\"] for x in batch]\n",
    "    paper_ids = [x[\"paper_id\"] for x in batch]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"abstract\": abstracts,\n",
    "        \"paper_id\": paper_ids\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(PaperDataset(train_data, tokenizer), batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(PaperDataset(val_data, tokenizer), batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# === 訓練設定 ===\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scaler = GradScaler()\n",
    "num_epochs = 200\n",
    "gradient_accumulation_steps = 10\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "checkpoint_dir = \"longt5-arxiv_lora\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "best_rouge1 = 0.0\n",
    "\n",
    "# === 訓練迴圈 ===\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # 驗證階段\n",
    "    model.eval()\n",
    "    predictions, references, paper_ids = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            reference = batch[\"abstract\"][0]\n",
    "            pid = batch[\"paper_id\"][0]\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=1024,\n",
    "                min_length=200,\n",
    "                num_beams=8, \n",
    "                length_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            predictions.append(pred)\n",
    "            references.append(reference)\n",
    "            paper_ids.append(pid)\n",
    "\n",
    "    rouge = metric_rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    bert = metric_bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    rouge1 = rouge[\"rouge1\"]\n",
    "    print(f\"ROUGE-1: {rouge1:.4f}, BERTScore F1: {np.mean(bert['f1']):.4f}\")\n",
    "\n",
    "    if rouge1 > best_rouge1:\n",
    "        best_rouge1 = rouge1\n",
    "        model.save_pretrained(os.path.join(checkpoint_dir, \"best\"))\n",
    "        tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"best\"))\n",
    "        print(\"Best model saved.\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "# 最後儲存\n",
    "model.save_pretrained(os.path.join(checkpoint_dir, \"final\"))\n",
    "tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"final\"))\n",
    "print(\"Training completed and model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方向2、Large Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英文文本：LLaMA-2 13B + Axolotl + PEFT、Mamba、Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文文本：Qwen2 + QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==2.1.2 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers peft accelerate evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bitsandbytes -f https://huggingface.github.io/bitsandbytes-packages/torch211_cu118.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy nltk rouge-score bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # 用於斷句和 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install absl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "from transformers import default_data_collator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import evaluate\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Clean LaTeX and noise\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', '', text)  # LaTeX commands\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)         # $$ math $$\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)               # $math$\n",
    "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text)  # [1], [1, 2]\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+.*?(?=\\.|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+.*?(?=\\.|$)', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:!?()\\-]', ' ', text)  # remove rare symbols but keep colons etc\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model/tokenizer with QLoRA\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Optimized LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load dataset\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "# Custom Dataset\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=8192):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        abstract = item[\"abstract\"]\n",
    "\n",
    "        # Enhanced CoT-style prompt\n",
    "        input_text = f\"\"\"\n",
    "You are a scientific writing assistant trained to write high-quality research abstracts.\n",
    "Your task is to analyze the given introduction of a computer science or artificial intelligence paper and generate a clear, structured, and academic abstract.\n",
    "\n",
    "Use professional and academic language. Maintain coherence and conciseness. The abstract should be approximately 150–300 words.\n",
    "\n",
    "[Introduction]\n",
    "{introduction}\n",
    "\n",
    "[Abstract]\"\"\"\n",
    "        full_text = input_text + f\" {abstract}\"\n",
    "\n",
    "        tokens = self.tokenizer(full_text, max_length=self.max_length, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        input_ids = tokens[\"input_ids\"].squeeze()\n",
    "        attention_mask = tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "        abstract_start = input_text.count(\"\\n\")  # Rough offset fallback if needed\n",
    "        labels = input_ids.clone()\n",
    "        labels[:len(self.tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].squeeze())] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": abstract,\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_masks = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    abstracts = [item[\"abstract\"] for item in batch]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels,\n",
    "        \"paper_id\": [item[\"paper_id\"] for item in batch],\n",
    "        \"abstract\": abstracts,\n",
    "    }\n",
    "    \n",
    "# 載入資料與切分訓練 / 驗證集\n",
    "train_data = load_json(\"train.json\")   # 假設你的訓練資料檔名為 train.json\n",
    "val_size = int(0.2 * len(train_data))  # 使用 20% 當作驗證集\n",
    "val_data = train_data[-val_size:]\n",
    "train_data = train_data[:-val_size]\n",
    "\n",
    "test_data = load_json(\"test.json\") \n",
    "    \n",
    "\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer)\n",
    "val_dataset = TrainPaperDataset(val_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=default_data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=default_data_collator)\n",
    "\n",
    "# Generation config with stop token handling\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=800,\n",
    "    min_length=150,\n",
    "    num_beams=5,\n",
    "    length_penalty=1.0,\n",
    "    repetition_penalty=1.1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=1e-2)\n",
    "scaler = GradScaler()\n",
    "\n",
    "baseline_scores = {\"rouge1\": 0.47, \"rouge2\": 0.12, \"rougeL\": 0.22, \"bertscore_f1\": 0.85}\n",
    "checkpoint_dir = \"gemma3-12b_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "patience = 30\n",
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "best_rouge1 = 0.0\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    predicted_abstracts = []\n",
    "    reference_abstracts = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "            gen_input = input_ids[0].unsqueeze(0)\n",
    "            generated_ids = model.generate(gen_input, generation_config=generation_config)\n",
    "            decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            generated = decoded.split(\"### Abstract:\")[-1].strip()\n",
    "            predicted_abstracts.append(generated)\n",
    "            reference_abstracts.append(batch[\"abstract\"][0])\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    rouge_scores = rouge.compute(predictions=predicted_abstracts, references=reference_abstracts, use_stemmer=True)\n",
    "    bert_scores = bertscore.compute(predictions=predicted_abstracts, references=reference_abstracts, lang=\"en\")\n",
    "    current_rouge1 = rouge_scores['rouge1']\n",
    "    current_bertscore = np.mean(bert_scores['f1'])\n",
    "\n",
    "    print(f\"ROUGE-1: {current_rouge1:.4f}, BERTScore-F1: {current_bertscore:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss or current_rouge1 > best_rouge1:\n",
    "        best_val_loss = val_loss\n",
    "        best_rouge1 = current_rouge1\n",
    "        patience_counter = 0\n",
    "        model.save_pretrained(os.path.join(checkpoint_dir, \"best_model\"))\n",
    "        print(\"Model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "from transformers import default_data_collator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import evaluate\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Clean LaTeX and noise\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', '', text)  # LaTeX commands\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)         # $$ math $$\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)               # $math$\n",
    "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text)  # [1], [1, 2]\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+.*?(?=\\.|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+.*?(?=\\.|$)', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:!?()\\-]', ' ', text)  # remove rare symbols but keep colons etc\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model/tokenizer with QLoRA\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Optimized LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load dataset\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "# Custom Dataset\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        intro = item[\"introduction\"]\n",
    "        abstract = item[\"abstract\"]\n",
    "\n",
    "        # CoT prompt\n",
    "        prompt_prefix = (\n",
    "            \"You are a scientific writing assistant trained to write high-quality research abstracts.\\n\"\n",
    "            \"Your task is to analyze the given introduction of a computer science or artificial intelligence paper\"\n",
    "            \"and generate a clear, structured, and academic abstract.\\n\\n\"\n",
    "            \"Use professional and academic language. Maintain coherence and conciseness.\"\n",
    "            \"The abstract should be approximately 150–300 words.\\n\\n\"\n",
    "            \"And learn how to start the first sentence of most abstracts.\\n\\n\"\n",
    "            \"[Introduction]\\n\"\n",
    "        )\n",
    "        prompt_suffix = \"\\n\\n[Abstract]\"\n",
    "\n",
    "        # tokenize introduction separately and truncate\n",
    "        intro_tokens = self.tokenizer(prompt_prefix + intro, truncation=True, max_length=1536, return_tensors=\"pt\")\n",
    "        prompt_input_ids = intro_tokens[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = intro_tokens[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # concatenate prompt + abstract for labels\n",
    "        full_text = self.tokenizer.decode(prompt_input_ids, skip_special_tokens=True) + prompt_suffix + \" \" + abstract\n",
    "        full_tokens = self.tokenizer(full_text, truncation=True, max_length=self.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = full_tokens[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = full_tokens[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # 計算 abstract 起始位置，masked labels\n",
    "        label_cutoff = self.tokenizer(full_text.split(\"[Abstract]\")[0], return_tensors=\"pt\")[\"input_ids\"].size(1)\n",
    "        labels = input_ids.clone()\n",
    "        labels[:label_cutoff] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": abstract,\n",
    "    }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_masks = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    abstracts = [item[\"abstract\"] for item in batch]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels,\n",
    "        \"paper_id\": [item[\"paper_id\"] for item in batch],\n",
    "        \"abstract\": abstracts,\n",
    "    }\n",
    "    \n",
    "# 載入資料與切分訓練 / 驗證集\n",
    "train_data = load_json(\"train.json\")   \n",
    "val_size = int(0.1 * len(train_data))  # 使用 10% 當作驗證集\n",
    "val_data = train_data[-val_size:]\n",
    "train_data = train_data[:-val_size]\n",
    "\n",
    "test_data = load_json(\"test.json\") \n",
    "    \n",
    "\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer)\n",
    "val_dataset = TrainPaperDataset(val_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Generation config with stop token handling\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=300,\n",
    "    min_length=150,\n",
    "    num_beams=5,\n",
    "    length_penalty=1.0,\n",
    "    repetition_penalty=1.1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=1e-2)\n",
    "scaler = GradScaler()\n",
    "\n",
    "baseline_scores = {\"rouge1\": 0.47, \"rouge2\": 0.12, \"rougeL\": 0.22, \"bertscore_f1\": 0.85}\n",
    "checkpoint_dir = \"gemma-7b_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "patience = 30\n",
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "best_rouge1 = 0.0\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        if torch.all(labels == -100):\n",
    "            continue\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Starting validation at Epoch {epoch+1}\")\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        predicted_abstracts = []\n",
    "        reference_abstracts = []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(val_loader):\n",
    "                print(f\"[Validation] Processing batch {i+1}/{len(val_loader)}\")  \n",
    "                \n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                with autocast():\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    val_loss += outputs.loss.item()\n",
    "\n",
    "                # 生成摘要\n",
    "                gen_input = input_ids[0].unsqueeze(0)\n",
    "                gen_input = gen_input[:, -1024:]  # 截斷到最後 1024 個 token\n",
    "                generated_ids = model.generate(gen_input, generation_config=generation_config)\n",
    "                decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                # 提取 [Abstract] 後的內容\n",
    "                if \"[Abstract]\" in decoded:\n",
    "                    generated = decoded.split(\"[Abstract]\")[-1].strip()\n",
    "                else:\n",
    "                    # 如果沒有 [Abstract]，嘗試移除提示詞部分\n",
    "                    prompt_end = \"Use professional and academic language. Maintain coherence and conciseness. The abstract should be approximately 150–300 words.\"\n",
    "                    if prompt_end in decoded:\n",
    "                        generated = decoded.split(prompt_end)[-1].strip()\n",
    "                    else:\n",
    "                        generated = decoded  # 如果無法分割，保留全文並記錄警告\n",
    "                        print(f\"Paper {batch['paper_id'][0]}: 無法正確提取摘要，使用完整生成內容\")\n",
    "\n",
    "                # 移除換行符號並規範化空格\n",
    "                generated = generated.replace(\"\\n\", \" \").strip()\n",
    "                generated = re.sub(r'\\s+', ' ', generated)\n",
    "\n",
    "                predicted_abstracts.append(generated)\n",
    "                reference_abstracts.append(batch[\"abstract\"][0])\n",
    "\n",
    "        # 計算評估指標\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        bertscore = evaluate.load(\"bertscore\")\n",
    "        rouge_scores = rouge.compute(predictions=predicted_abstracts, references=reference_abstracts, use_stemmer=True)\n",
    "        bert_scores = bertscore.compute(predictions=predicted_abstracts, references=reference_abstracts, lang=\"en\")\n",
    "        current_rouge1 = rouge_scores['rouge1']\n",
    "        current_rouge2 = rouge_scores['rouge2']\n",
    "        current_rougeL = rouge_scores['rougeL']\n",
    "        current_bertscore = np.mean(bert_scores['f1'])\n",
    "\n",
    "        print(f\"ROUGE-1: {current_rouge1:.4f}, ROUGE-2: {current_rouge2:.4f}, ROUGE-L: {current_rougeL:.4f}, BERTScore-F1: {current_bertscore:.4f}\")\n",
    "\n",
    "        # 檢查是否保存模型或提前停止\n",
    "        if val_loss < best_val_loss or current_rouge1 > best_rouge1:\n",
    "            best_val_loss = val_loss\n",
    "            best_rouge1 = current_rouge1\n",
    "            patience_counter = 0\n",
    "            model.save_pretrained(os.path.join(checkpoint_dir, \"best_model\"))\n",
    "            print(\"Model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ---------- 前處理 ----------\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', '', text)\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)\n",
    "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text)\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+.*?(?=\\.|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+.*?(?=\\.|$)', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:!?()\\-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# ---------- Reranking 分數 ----------\n",
    "def heuristic_score(summary, intro):\n",
    "    words = summary.split()\n",
    "    sents = summary.count('.') + summary.count('?') + summary.count('!')\n",
    "    intro_words = set(intro.lower().split())\n",
    "    summary_words = set(summary.lower().split())\n",
    "    overlap = len(intro_words & summary_words)\n",
    "    return min(len(words), 400) / 400 + min(sents, 8) / 8 + min(overlap, 30) / 30\n",
    "\n",
    "# ---------- 載入模型 ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, \"gemma-7b_checkpoints/best_model\")\n",
    "model.eval()\n",
    "\n",
    "# ---------- Generation 設定 ----------\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=900,\n",
    "    min_length=200,\n",
    "    num_beams=5,\n",
    "    length_penalty=1.0,\n",
    "    repetition_penalty=1.1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# ---------- 載入測試資料 ----------\n",
    "with open(\"test.json\", \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# ---------- 生成摘要 + Self-Reranking ----------\n",
    "results = []\n",
    "for item in tqdm(test_data):\n",
    "    paper_id = item[\"paper_id\"]\n",
    "    intro = clean_text(item[\"introduction\"])  # 清理介紹文本\n",
    "\n",
    "    # 修改提示詞，明確分隔符號\n",
    "    prompt = f\"\"\"You are an AI expert research assistant.\n",
    "Your job is to read the following introduction from a scientific paper in the field of computer science or AI and generate a structured, informative abstract.\n",
    "\n",
    "Focus on the following:\n",
    "1. Research background and motivation.\n",
    "2. Methodology or proposed solution.\n",
    "3. Key results or contributions.\n",
    "\n",
    "Use academic language and maintain clarity. The abstract should be suitable for publication.\n",
    "\n",
    "Introduction: {intro}\n",
    "\n",
    "Generate the abstract below, starting with [ABSTRACT] followed by the content:\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    candidates = []\n",
    "    for _ in range(1):  # 產生多個摘要候選\n",
    "        with torch.no_grad():\n",
    "            gen_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                generation_config=generation_config,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        decoded = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        # 提取 [ABSTRACT] 後的內容，並移除換行符號\n",
    "        try:\n",
    "            summary = decoded.split(\"[ABSTRACT]\")[1].strip().replace(\"\\n\", \" \")\n",
    "        except IndexError:\n",
    "            # 如果模型未生成 [ABSTRACT]，則取最後一段作為摘要\n",
    "            summary = decoded.split(\"Generate the abstract below\")[-1].strip().replace(\"\\n\", \" \")\n",
    "        candidates.append(summary)\n",
    "\n",
    "    # reranking\n",
    "    scores = [heuristic_score(c, intro) for c in candidates]\n",
    "    best_summary = candidates[np.argmax(scores)]\n",
    "\n",
    "    results.append({\"paper_id\": str(paper_id), \"abstract\": best_summary})\n",
    "\n",
    "# ---------- 儲存結果 ----------\n",
    "with open(\"submission_gemma.json\", \"w\") as f:\n",
    "    for item in results:\n",
    "        json.dump(item, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ 推理與重排序完成，已儲存為 submission_gemma.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先計算token數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "# 載入 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-12b-it\", trust_remote_code=True)\n",
    "\n",
    "# 計算 token 數量的函式\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# 讀取資料\n",
    "with open(\"train.json\", \"r\") as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "with open(\"test.json\", \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# 儲存訓練資料的 token 數量\n",
    "train_intro_tokens = []\n",
    "train_abstract_tokens = []\n",
    "\n",
    "# 計算並儲存訓練資料的 token 數量\n",
    "for paper in train_data:\n",
    "    intro = paper[\"introduction\"]\n",
    "    abstr = paper[\"abstract\"]\n",
    "    train_intro_tokens.append(count_tokens(intro))\n",
    "    train_abstract_tokens.append(count_tokens(abstr))\n",
    "\n",
    "# 儲存測試資料的 token 數量\n",
    "test_intro_tokens = []\n",
    "\n",
    "# 計算並儲存測試資料的 token 數量\n",
    "for paper in test_data:\n",
    "    intro = paper[\"introduction\"]\n",
    "    test_intro_tokens.append(count_tokens(intro))\n",
    "\n",
    "# 計算訓練資料的統計數據\n",
    "print(\"訓練資料 (Introduction):\")\n",
    "print(f\"  平均 token 數: {statistics.mean(train_intro_tokens):.2f}\")\n",
    "print(f\"  中位數 token 數: {statistics.median(train_intro_tokens)}\")\n",
    "print(f\"  最小 token 數: {min(train_intro_tokens)}\")\n",
    "print(f\"  最大 token 數: {max(train_intro_tokens)}\")\n",
    "print(f\"  標準差: {statistics.stdev(train_intro_tokens):.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"訓練資料 (Abstract):\")\n",
    "print(f\"  平均 token 數: {statistics.mean(train_abstract_tokens):.2f}\")\n",
    "print(f\"  中位數 token 數: {statistics.median(train_abstract_tokens)}\")\n",
    "print(f\"  最小 token 數: {min(train_abstract_tokens)}\")\n",
    "print(f\"  最大 token 數: {max(train_abstract_tokens)}\")\n",
    "print(f\"  標準差: {statistics.stdev(train_abstract_tokens):.2f}\")\n",
    "print()\n",
    "\n",
    "# 計算測試資料的統計數據\n",
    "print(\"測試資料 (Introduction):\")\n",
    "print(f\"  平均 token 數: {statistics.mean(test_intro_tokens):.2f}\")\n",
    "print(f\"  中位數 token 數: {statistics.median(test_intro_tokens)}\")\n",
    "print(f\"  最小 token 數: {min(test_intro_tokens)}\")\n",
    "print(f\"  最大 token 數: {max(test_intro_tokens)}\")\n",
    "print(f\"  標準差: {statistics.stdev(test_intro_tokens):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch transformers datasets rouge_score scikit-learn numpy tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install einops transformers_stream_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler, BitsAndBytesConfig\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import os\n",
    "os.makedirs(\"qwen_checkpoints/best_model\", exist_ok=True)\n",
    "os.makedirs(\"qwen_checkpoints/final_model\", exist_ok=True)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def extract_important_sentences(text, tokenizer, max_token_length=4096):\n",
    "    # 用 spaCy 分句\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    if len(sentences) <= 1:\n",
    "        return text\n",
    "\n",
    "    # 用 TF-IDF 評估重要性\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    scores = tfidf_matrix.sum(axis=1).A1\n",
    "    ranked_sentences = [sent for _, sent in sorted(zip(scores, sentences), reverse=True)]\n",
    "\n",
    "    selected = []\n",
    "    total_tokens = 0\n",
    "    for sent in ranked_sentences:\n",
    "        tokenized = tokenizer(sent, add_special_tokens=False)[\"input_ids\"]\n",
    "        if total_tokens + len(tokenized) > max_token_length:\n",
    "            break\n",
    "        selected.append(sent)\n",
    "        total_tokens += len(tokenized)\n",
    "\n",
    "    return \" \".join(selected)\n",
    "\n",
    "\n",
    "# 1. 資料處理\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=4096):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        intro = self.data[idx][\"introduction\"]\n",
    "        abstract = self.data[idx][\"abstract\"]\n",
    "\n",
    "        # 如果太長就用 extract_important_sentences\n",
    "        intro = extract_important_sentences(intro, self.tokenizer, max_token_length=self.max_length - 512)\n",
    "\n",
    "        prompt = (\n",
    "            \"You are a Computer Science research assistant. Summarize the following introduction into a clear and concise academic abstract.\\n\\n\"\n",
    "            f\"Introduction: {intro}\\n\\n\"\n",
    "            \"Abstract:\"\n",
    "        )\n",
    "        full_text = f\"{prompt} {abstract}\"\n",
    "\n",
    "        tokenized = self.tokenizer(full_text, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        input_ids = tokenized[\"input_ids\"].squeeze()\n",
    "        attention_mask = tokenized[\"attention_mask\"].squeeze()\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        prompt_len = len(self.tokenizer(prompt, truncation=True, max_length=self.max_length)[\"input_ids\"])\n",
    "        labels[:prompt_len] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "data = []\n",
    "with open(\"train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():  \n",
    "            try:\n",
    "                data.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON line: {e}\")\n",
    "                continue\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "print(f\"Training samples: {len(train_data)}, Validation samples: {len(val_data)}\")\n",
    "\n",
    "# 2. 加載 Qwen 模型與 Tokenizer\n",
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 設置資料集與 DataLoader\n",
    "train_dataset = CustomDataset(train_data, tokenizer, max_length=4096)\n",
    "val_dataset = CustomDataset(val_data, tokenizer, max_length=4096)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "# 3. 訓練設置\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 10\n",
    "gradient_accumulation_steps = 4\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * num_epochs // gradient_accumulation_steps)\n",
    "\n",
    "patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# ROUGE 評估函數\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "def compute_rouge(predictions, references):\n",
    "    scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = scorer.score(ref, pred)\n",
    "        scores[\"rouge1\"].append(score[\"rouge1\"].fmeasure)\n",
    "        scores[\"rouge2\"].append(score[\"rouge2\"].fmeasure)\n",
    "        scores[\"rougeL\"].append(score[\"rougeL\"].fmeasure)\n",
    "    return {k: np.mean(v) for k, v in scores.items()}\n",
    "\n",
    "def compute_bestscore1(rouge_scores):\n",
    "    return rouge_scores[\"rouge1\"]\n",
    "\n",
    "# 4. 訓練與驗證循環\n",
    "best_rouge = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / gradient_accumulation_steps\n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        predictions, references = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=550,\n",
    "                    num_beams=8,\n",
    "                    do_sample=False,\n",
    "                    top_k=30,\n",
    "                    top_p=0.95,\n",
    "                    temperature=0.7,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "                pred_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "                ref_texts = [tokenizer.decode(label, skip_special_tokens=True) for label in batch[\"labels\"]]\n",
    "                pred_texts = [text.split(\"Abstract:\")[-1].strip() if \"Abstract:\" in text else text for text in pred_texts]\n",
    "                predictions.extend(pred_texts)\n",
    "                references.extend(ref_texts)\n",
    "\n",
    "        rouge_scores = compute_rouge(predictions, references)\n",
    "        bestscore1 = compute_bestscore1(rouge_scores)\n",
    "        print(f\"Validation - ROUGE-1: {rouge_scores['rouge1']:.4f}, ROUGE-2: {rouge_scores['rouge2']:.4f}, ROUGE-L: {rouge_scores['rougeL']:.4f}, BestScore-1: {bestscore1:.4f}\")\n",
    "\n",
    "        # Early Stopping 判斷\n",
    "        current_score = rouge_scores[\"rouge1\"]\n",
    "        if current_score > best_rouge:\n",
    "            best_rouge = current_score\n",
    "            early_stopping_counter = 0  # reset counter\n",
    "            print(\"New best ROUGE-1, saving model...\")\n",
    "            model.save_pretrained(\"qwen_checkpoints/best_model\")\n",
    "            tokenizer.save_pretrained(\"qwen_checkpoints/best_model\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.5\n",
    "            print(f\"ROUGE-1 not improved, reducing LR to {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} validations without improvement.\")\n",
    "            break\n",
    "\n",
    "# 5. 保存最終模型\n",
    "model.save_pretrained(\"qwen_checkpoints/final_model\")\n",
    "tokenizer.save_pretrained(\"qwen_checkpoints/final_model\")\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from peft import PeftModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "import unicodedata\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# === 資料清洗 ===\n",
    "allowed_unicode = \"∑∂∇∞θπ𝒟𝒫𝒩αβγδελμσφωℝ𝔽𝓛\"\n",
    "def is_allowed_char(c):\n",
    "    return (\n",
    "        ord(c) < 128 or\n",
    "        c in allowed_unicode or\n",
    "        \"MATHEMATICAL\" in unicodedata.name(c, \"\")\n",
    "    )\n",
    "\n",
    "def clean_intro(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)  # 保留 \\emph{} 內文\n",
    "    text = ''.join(c if is_allowed_char(c) else ' ' for c in text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_important_sentences(text, tokenizer, max_token_length=4096, keep_head_sentences=3):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "    if len(sentences) <= keep_head_sentences:\n",
    "        return text\n",
    "\n",
    "    selected = sentences[:keep_head_sentences]\n",
    "    total_tokens = sum(len(tokenizer(s, add_special_tokens=False)[\"input_ids\"]) for s in selected)\n",
    "\n",
    "    try:\n",
    "        remaining_sentences = sentences[keep_head_sentences:]\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(remaining_sentences)\n",
    "        scores = tfidf_matrix.sum(axis=1).A1\n",
    "        ranked = [s for _, s in sorted(zip(scores, remaining_sentences), reverse=True)]\n",
    "    except Exception as e:\n",
    "        print(f\"[TF-IDF fallback] {e}\")\n",
    "        ranked = remaining_sentences[:5]\n",
    "\n",
    "    for sent in ranked:\n",
    "        token_len = len(tokenizer(sent, add_special_tokens=False)[\"input_ids\"])\n",
    "        if total_tokens + token_len > max_token_length:\n",
    "            break\n",
    "        selected.append(sent)\n",
    "        total_tokens += token_len\n",
    "\n",
    "    return \" \".join(selected)\n",
    "\n",
    "def maybe_extract_important_sentences(text, tokenizer, max_token_threshold=3000, keep_head_sentences=3):\n",
    "    tokens = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(tokens) <= max_token_threshold:\n",
    "        return text\n",
    "    return extract_important_sentences(text, tokenizer, max_token_length=max_token_threshold, keep_head_sentences=keep_head_sentences)\n",
    "\n",
    "def clean_generated_abstract(text):\n",
    "    doc = nlp(text)\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    for sent in doc.sents:\n",
    "        s_strip = sent.text.strip()\n",
    "        if s_strip and s_strip not in seen:\n",
    "            cleaned.append(s_strip)\n",
    "            seen.add(s_strip)\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "# === 設定 ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_input_length = 4096\n",
    "max_output_tokens = 650\n",
    "model_dir = \"qwen_checkpoints/epoch_6\"\n",
    "\n",
    "# === 載入 tokenizer 與模型 ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-7B-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, model_dir)\n",
    "model.eval()\n",
    "\n",
    "# === 載入資料 ===\n",
    "with open(\"test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "# === 推理 ===\n",
    "results = []\n",
    "for item in tqdm(test_data, desc=\"Generating Abstracts\"):\n",
    "    paper_id = item[\"paper_id\"]\n",
    "    intro = clean_intro(item[\"introduction\"])\n",
    "    intro = maybe_extract_important_sentences(intro, tokenizer)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an expert research assistant specialized in artificial intelligence. \"\n",
    "        \"Your task is to read the following paper introduction and write a clear, formal, and concise abstract. \"\n",
    "        \"Focus on the research background, motivation, methods, and key contributions.\\n\\n\"\n",
    "        f\"Introduction:\\n{intro}\\n\\n\"\n",
    "        \"Abstract:\"\n",
    "    )\n",
    "\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\", max_length=4096, truncation=True, padding=True).to(device)\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_output_tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        num_beams=8,\n",
    "        do_sample=False,\n",
    "        temperature=0.7,\n",
    "        top_k=30,\n",
    "        top_p=0.95,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    abstract = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Abstract:\" in abstract:\n",
    "        abstract = abstract.split(\"Abstract:\")[-1].strip()\n",
    "    else:\n",
    "        abstract = abstract.strip().split(\"\\n\")[-1]\n",
    "\n",
    "    abstract = clean_generated_abstract(abstract)\n",
    "    results.append({\"paper_id\": str(paper_id), \"abstract\": abstract})\n",
    "\n",
    "# === 輸出結果 ===\n",
    "with open(\"submission_qwen.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in results:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"摘要已儲存到 submission_qwen.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
