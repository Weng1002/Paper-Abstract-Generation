{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å‰ç½®å‹•ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install evaluate rouge-score bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install peft==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall peft triton -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ•¸æ“šå‰è™•ç†ã€‚åˆ†å‰²è¨“ç·´å’Œé©—è­‰é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# åŠ è¼‰ train.json\n",
    "with open('train.json', 'r') as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "    \n",
    "with open('test.json', 'r') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# è½‰æ›ç‚º Hugging Face Dataset æ ¼å¼\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "\n",
    "# å°‡æ•¸æ“šé›†åˆ†å‰²ç‚ºè¨“ç·´é›†å’Œé©—è­‰é›†ï¼ˆ80% è¨“ç·´ï¼Œ20% é©—è­‰ï¼‰\n",
    "split_dataset = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_split = split_dataset['train']  # è¨“ç·´é›†\n",
    "valid_split = split_dataset['test']   # é©—è­‰é›†\n",
    "\n",
    "print(f\"è¨“ç·´é›†å¤§å°: {len(train_split)}\")\n",
    "print(f\"é©—è­‰é›†å¤§å°: {len(valid_split)}\")\n",
    "print(f\"æ¸¬è©¦é›†å¤§å°: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import PegasusTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# åŠ è¼‰ tokenizer\n",
    "model_name = 'google/pegasus-large'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# åŠ è¼‰æ•¸æ“š\n",
    "with open('train.json', 'r') as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "    \n",
    "with open('test.json', 'r') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# å‡½æ•¸ï¼šè¨ˆç®— token é•·åº¦\n",
    "def get_token_lengths(data, field):\n",
    "    lengths = [len(tokenizer.encode(item[field], add_special_tokens=True)) for item in data]\n",
    "    return lengths\n",
    "\n",
    "# è¨ˆç®— train.json ä¸­ introduction å’Œ abstract çš„é•·åº¦\n",
    "train_intro_lengths = get_token_lengths(train_data, 'introduction')\n",
    "train_abs_lengths = get_token_lengths(train_data, 'abstract')\n",
    "\n",
    "# è¨ˆç®— test.json ä¸­ introduction çš„é•·åº¦\n",
    "test_intro_lengths = get_token_lengths(test_data, 'introduction')\n",
    "\n",
    "# çµ±è¨ˆä¿¡æ¯\n",
    "def print_stats(lengths, name):\n",
    "    print(f\"{name} é•·åº¦çµ±è¨ˆï¼š\")\n",
    "    print(f\"å¹³å‡é•·åº¦: {np.mean(lengths):.1f}\")\n",
    "    print(f\"ä¸­ä½æ•¸: {np.median(lengths):.1f}\")\n",
    "    print(f\"æœ€å¤§é•·åº¦: {max(lengths)}\")\n",
    "    print(f\"æœ€å°é•·åº¦: {min(lengths)}\")\n",
    "    print(f\"90% åˆ†ä½æ•¸: {np.percentile(lengths, 90):.1f}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# è¼¸å‡ºçµæœ\n",
    "print_stats(train_intro_lengths, \"Train Introduction\")\n",
    "print_stats(train_abs_lengths, \"Train Abstract\")\n",
    "print_stats(test_intro_lengths, \"Test Introduction\")\n",
    "\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "model_name = \"google/pegasus-x-large\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# åˆ†æ token é•·åº¦åˆ†ä½ˆ\n",
    "def analyze_token_lengths(data, tokenizer, field=\"introduction\"):\n",
    "    lengths = []\n",
    "    for item in data:\n",
    "        text = item[field]\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        lengths.append(len(tokens))\n",
    "    return lengths\n",
    "\n",
    "# å‡è¨­ train_data å’Œ test_data å·²å®šç¾©\n",
    "# åˆ†æ train å’Œ test æ•¸æ“š\n",
    "train_intro_lengths = analyze_token_lengths(train_data, tokenizer, \"introduction\")\n",
    "train_abs_lengths = analyze_token_lengths(train_data, tokenizer, \"abstract\")\n",
    "test_intro_lengths = analyze_token_lengths(test_data, tokenizer, \"introduction\")\n",
    "\n",
    "# æ‰“å°çµ±è¨ˆä¿¡æ¯\n",
    "import numpy as np\n",
    "\n",
    "print(\"Train Introduction Token Lengths:\")\n",
    "print(f\"Mean: {np.mean(train_intro_lengths):.2f}, Max: {max(train_intro_lengths)}, Min: {min(train_intro_lengths)}\")\n",
    "print(f\"Percentage > 1024: {sum(l > 1024 for l in train_intro_lengths) / len(train_intro_lengths) * 100:.2f}%\")\n",
    "\n",
    "print(\"Train Abstract Token Lengths:\")\n",
    "print(f\"Mean: {np.mean(train_abs_lengths):.2f}, Max: {max(train_abs_lengths)}, Min: {min(train_abs_lengths)}\")\n",
    "print(f\"Percentage > 660: {sum(l > 660 for l in train_abs_lengths) / len(train_abs_lengths) * 100:.2f}%\")\n",
    "\n",
    "print(\"Test Introduction Token Lengths:\")\n",
    "print(f\"Mean: {np.mean(test_intro_lengths):.2f}, Max: {max(test_intro_lengths)}, Min: {min(test_intro_lengths)}\")\n",
    "print(f\"Percentage > 1024: {sum(l > 1024 for l in test_intro_lengths) / len(test_intro_lengths) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–¹å‘1ã€Traditional  Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegasusã€T5 (FLAN-T5)ã€æˆ– BART éƒ½æ˜¯æœ€å¸¸è¦‹çš„ã€Œæ‘˜è¦ä¸‰å·¨é ­ã€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰“ç®—ä½¿ç”¨ PEGASUS-Largeã€PEGASUS-ArXivã€LED ã€LongT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegasus-x-large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy summa nlpaug nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¨­å®šè¶…åƒæ•¸ã€è©•ä¼°æŒ‡æ¨™ã€è¨“ç·´éšæ®µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "# åŠ è¼‰ spacy æ¨¡å‹\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "allowed_unicode = \"âˆ‘âˆ‚âˆ‡âˆÎ¸Ï€ğ’Ÿğ’«ğ’©Î±Î²Î³Î´ÎµÎ»Î¼ÏƒÏ†Ï‰â„ğ”½ğ“›\"\n",
    "def is_allowed_char(c):\n",
    "    return (\n",
    "        ord(c) < 128 or\n",
    "        c in allowed_unicode or\n",
    "        \"MATHEMATICAL\" in unicodedata.name(c, \"\")\n",
    "    )\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)  # ä¿ç•™ \\emph{} å…§æ–‡\n",
    "    text = ''.join(c if is_allowed_char(c) else ' ' for c in text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Load the Pegasus-X model and tokenizer\n",
    "model_name = \"google/pegasus-x-large\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 2. Load the datasets with cleaning\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    # æ¸…ç† introduction å’Œ abstract\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "train_data = load_json(\"train.json\")  \n",
    "test_data = load_json(\"test.json\")    \n",
    "\n",
    "# 3. å¾å…©ç«¯æˆªæ–· + èªç¾©é‡è¦æ€§é¸æ“‡\n",
    "def truncate_from_ends_with_importance(introduction, tokenizer, max_length=1024):\n",
    "    # ä½¿ç”¨ spacy åˆ†å‰²å¥å­\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    # å¦‚æœå¥å­æ•¸å°‘æ–¼ç­‰æ–¼ 1ï¼Œç›´æ¥åˆ†è©ä¸¦æˆªæ–·\n",
    "    if len(sentences) <= 1:\n",
    "        tokens = tokenizer(\n",
    "            introduction,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()    \n",
    "\n",
    "    # è¨ˆç®—æ¯å€‹å¥å­çš„ TF-IDF åˆ†æ•¸\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        sentence_scores = tfidf_matrix.sum(axis=1).A1\n",
    "    except ValueError:\n",
    "        sentence_scores = np.arange(len(sentences), 0, -1)\n",
    "\n",
    "    # ç¢ºå®šå¾å…©ç«¯ä¿ç•™çš„æ¯”ä¾‹ï¼ˆä¾‹å¦‚å‰ 40% å’Œå¾Œ 40%ï¼‰\n",
    "    num_sentences = len(sentences)\n",
    "    num_end_sentences = max(1, int(num_sentences * 0.45))  # è‡³å°‘ä¿ç•™ 1 å¥\n",
    "    start_sentences = sentences[:num_end_sentences]  # é–‹é ­éƒ¨åˆ†\n",
    "    end_sentences = sentences[-num_end_sentences:]   # çµå°¾éƒ¨åˆ†\n",
    "    middle_sentences = sentences[num_end_sentences:-num_end_sentences]  # ä¸­é–“éƒ¨åˆ†\n",
    "    middle_scores = sentence_scores[num_end_sentences:-num_end_sentences]\n",
    "\n",
    "    # è¨ˆç®—é–‹é ­å’Œçµå°¾éƒ¨åˆ†çš„ token æ•¸\n",
    "    start_tokens = tokenizer.encode(\" \".join(start_sentences), add_special_tokens=False)\n",
    "    end_tokens = tokenizer.encode(\" \".join(end_sentences), add_special_tokens=False)\n",
    "    current_token_count = len(start_tokens) + len(end_tokens)\n",
    "\n",
    "    # å¦‚æœé–‹é ­å’Œçµå°¾å·²ç¶“è¶…é max_lengthï¼Œç›´æ¥æˆªæ–·\n",
    "    if current_token_count >= max_length - 2:\n",
    "        combined_text = \" \".join(start_sentences + end_sentences)\n",
    "        tokens = tokenizer(\n",
    "            combined_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "    # å¾ä¸­é–“éƒ¨åˆ†é¸æ“‡é—œéµå¥\n",
    "    selected_middle_sentences = []\n",
    "    if middle_sentences:\n",
    "        sorted_middle_indices = np.argsort(middle_scores)[::-1]\n",
    "        for idx in sorted_middle_indices:\n",
    "            sentence = middle_sentences[idx]\n",
    "            tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            if current_token_count + len(tokens) <= max_length - 2:\n",
    "                selected_middle_sentences.append(sentence)\n",
    "                current_token_count += len(tokens)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    truncated_introduction = \" \".join(start_sentences + selected_middle_sentences + end_sentences)\n",
    "    tokens = tokenizer(\n",
    "        truncated_introduction,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "# è¨“ç·´æ•¸æ“šé›†\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024, max_target_length=660):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        abstract = item[\"abstract\"]\n",
    "\n",
    "        # ä½¿ç”¨å¾å…©ç«¯æˆªæ–· + èªç¾©é‡è¦æ€§é¸æ“‡\n",
    "        input_ids, attention_mask = truncate_from_ends_with_importance(introduction, self.tokenizer, self.max_input_length)\n",
    "\n",
    "        # åˆ†è© abstract (target)\n",
    "        targets = self.tokenizer(\n",
    "            abstract,\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": target_ids,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "\n",
    "# æ¸¬è©¦æ•¸æ“šé›†\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "\n",
    "        # ä½¿ç”¨å¾å…©ç«¯æˆªæ–· + èªç¾©é‡è¦æ€§é¸æ“‡\n",
    "        input_ids, attention_mask = truncate_from_ends_with_importance(introduction, self.tokenizer, self.max_input_length)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"]\n",
    "        }\n",
    "\n",
    "# å‰µå»ºæ•¸æ“šé›†å’Œ DataLoader\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer, max_input_length=1024, max_target_length=660)\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer, max_input_length=1024)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 4. Fine-tune the model with checkpoint saving\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "num_epochs = 300\n",
    "\n",
    "# å‰µå»ºæª¢æŸ¥é»å„²å­˜ç›®éŒ„\n",
    "checkpoint_dir = \"pegasus-x-large_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "best_loss = float('inf')  # ç”¨æ–¼å„²å­˜æœ€ä½³æå¤±\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # å„²å­˜æª¢æŸ¥é»\n",
    "    if epoch % 10 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch + 1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    # å„²å­˜æœ€ä½³æ¨¡å‹ï¼ˆæ ¹æ“šæå¤±ï¼‰\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, best_model_path)\n",
    "        print(f\"Best model saved at {best_model_path} with loss {best_loss:.4f}\")\n",
    "\n",
    "# 5. å¾Œè™•ç†å‡½æ•¸ï¼šæ¸…ç†ç”Ÿæˆçš„æ‘˜è¦\n",
    "def clean_abstract(abstract):\n",
    "    # ç§»é™¤éè‹±æ–‡å­—ç¬¦\n",
    "    abstract = clean_text(abstract)\n",
    "    \n",
    "    # ç§»é™¤é–‹é ­é‡è¤‡çš„ \"In\"\n",
    "    words = abstract.split()\n",
    "    while len(words) > 1 and words[0] == \"In\" and words[1] == \"In\":\n",
    "        words.pop(0)\n",
    "    abstract = \" \".join(words)\n",
    "\n",
    "    # ç§»é™¤é‡è¤‡çš„å¥å­\n",
    "    sentences = abstract.split(\". \")\n",
    "    seen_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence and sentence not in seen_sentences:\n",
    "            seen_sentences.append(sentence)\n",
    "    abstract = \". \".join(seen_sentences)\n",
    "    if abstract and not abstract.endswith(\".\"):\n",
    "        abstract += \".\"\n",
    "\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨ç†éšæ®µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "# åŠ è¼‰ spacy æ¨¡å‹\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# å‰µå»ºæª¢æŸ¥é»å„²å­˜ç›®éŒ„\n",
    "checkpoint_dir = \"pegasus-x-large_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"google/pegasus-x-large\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "allowed_unicode = \"âˆ‘âˆ‚âˆ‡âˆÎ¸Ï€ğ’Ÿğ’«ğ’©Î±Î²Î³Î´ÎµÎ»Î¼ÏƒÏ†Ï‰â„ğ”½ğ“›\"\n",
    "def is_allowed_char(c):\n",
    "    return (\n",
    "        ord(c) < 128 or\n",
    "        c in allowed_unicode or\n",
    "        \"MATHEMATICAL\" in unicodedata.name(c, \"\")\n",
    "    )\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)  # ä¿ç•™ \\emph{} å…§æ–‡\n",
    "    text = ''.join(c if is_allowed_char(c) else ' ' for c in text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    # æ¸…ç† introduction å’Œ abstract\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "def clean_abstract(abstract):\n",
    "    # ç§»é™¤éè‹±æ–‡å­—ç¬¦\n",
    "    abstract = clean_text(abstract)\n",
    "    \n",
    "    # ç§»é™¤é–‹é ­é‡è¤‡çš„ \"In\"\n",
    "    words = abstract.split()\n",
    "    while len(words) > 1 and words[0] == \"In\" and words[1] == \"In\":\n",
    "        words.pop(0)\n",
    "    abstract = \" \".join(words)\n",
    "\n",
    "    # ç§»é™¤é‡è¤‡çš„å¥å­\n",
    "    sentences = abstract.split(\". \")\n",
    "    seen_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence and sentence not in seen_sentences:\n",
    "            seen_sentences.append(sentence)\n",
    "    abstract = \". \".join(seen_sentences)\n",
    "    if abstract and not abstract.endswith(\".\"):\n",
    "        abstract += \".\"\n",
    "\n",
    "    return abstract\n",
    "\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024, max_target_length=660):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        abstract = item[\"abstract\"]\n",
    "\n",
    "        # ä½¿ç”¨å¾å…©ç«¯æˆªæ–· + èªç¾©é‡è¦æ€§é¸æ“‡\n",
    "        input_ids, attention_mask = truncate_from_ends_with_importance(introduction, self.tokenizer, self.max_input_length)\n",
    "\n",
    "        # åˆ†è© abstract (target)\n",
    "        targets = self.tokenizer(\n",
    "            abstract,\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": target_ids,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "\n",
    "# æ¸¬è©¦æ•¸æ“šé›†\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "\n",
    "        # ä½¿ç”¨å¾å…©ç«¯æˆªæ–· + èªç¾©é‡è¦æ€§é¸æ“‡\n",
    "        input_ids, attention_mask = truncate_from_ends_with_importance(introduction, self.tokenizer, self.max_input_length)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"]\n",
    "        }\n",
    "\n",
    "def truncate_from_ends_with_importance(introduction, tokenizer, max_length=1024):\n",
    "    # ä½¿ç”¨ spacy åˆ†å‰²å¥å­\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    # å¦‚æœå¥å­æ•¸å°‘æ–¼ç­‰æ–¼ 1ï¼Œç›´æ¥åˆ†è©ä¸¦æˆªæ–·\n",
    "    if len(sentences) <= 1:\n",
    "        tokens = tokenizer(\n",
    "            introduction,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()    \n",
    "\n",
    "    # è¨ˆç®—æ¯å€‹å¥å­çš„ TF-IDF åˆ†æ•¸\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        sentence_scores = tfidf_matrix.sum(axis=1).A1\n",
    "    except ValueError:\n",
    "        sentence_scores = np.arange(len(sentences), 0, -1)\n",
    "\n",
    "    # ç¢ºå®šå¾å…©ç«¯ä¿ç•™çš„æ¯”ä¾‹ï¼ˆä¾‹å¦‚å‰ 40% å’Œå¾Œ 40%ï¼‰\n",
    "    num_sentences = len(sentences)\n",
    "    num_end_sentences = max(1, int(num_sentences * 0.45))  # è‡³å°‘ä¿ç•™ 1 å¥\n",
    "    start_sentences = sentences[:num_end_sentences]  # é–‹é ­éƒ¨åˆ†\n",
    "    end_sentences = sentences[-num_end_sentences:]   # çµå°¾éƒ¨åˆ†\n",
    "    middle_sentences = sentences[num_end_sentences:-num_end_sentences]  # ä¸­é–“éƒ¨åˆ†\n",
    "    middle_scores = sentence_scores[num_end_sentences:-num_end_sentences]\n",
    "\n",
    "    # è¨ˆç®—é–‹é ­å’Œçµå°¾éƒ¨åˆ†çš„ token æ•¸\n",
    "    start_tokens = tokenizer.encode(\" \".join(start_sentences), add_special_tokens=False)\n",
    "    end_tokens = tokenizer.encode(\" \".join(end_sentences), add_special_tokens=False)\n",
    "    current_token_count = len(start_tokens) + len(end_tokens)\n",
    "\n",
    "    # å¦‚æœé–‹é ­å’Œçµå°¾å·²ç¶“è¶…é max_lengthï¼Œç›´æ¥æˆªæ–·\n",
    "    if current_token_count >= max_length - 2:\n",
    "        combined_text = \" \".join(start_sentences + end_sentences)\n",
    "        tokens = tokenizer(\n",
    "            combined_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "    # å¾ä¸­é–“éƒ¨åˆ†é¸æ“‡é—œéµå¥\n",
    "    selected_middle_sentences = []\n",
    "    if middle_sentences:\n",
    "        sorted_middle_indices = np.argsort(middle_scores)[::-1]\n",
    "        for idx in sorted_middle_indices:\n",
    "            sentence = middle_sentences[idx]\n",
    "            tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            if current_token_count + len(tokens) <= max_length - 2:\n",
    "                selected_middle_sentences.append(sentence)\n",
    "                current_token_count += len(tokens)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    truncated_introduction = \" \".join(start_sentences + selected_middle_sentences + end_sentences)\n",
    "    tokens = tokenizer(\n",
    "        truncated_introduction,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "train_data = load_json(\"train.json\")  # 408 samples\n",
    "test_data = load_json(\"test.json\")    # 103 samples\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer, max_input_length=1024, max_target_length=660)\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer, max_input_length=1024)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 6. æ¨ç†æ™‚åŠ è¼‰è¨“ç·´å¥½çš„æ¨¡å‹\n",
    "# åŠ è¼‰æœ€ä½³æ¨¡å‹ï¼ˆå¯ä»¥æ ¹æ“šéœ€è¦æ”¹ç‚ºç‰¹å®šçš„ epoch æª¢æŸ¥é»ï¼Œä¾‹å¦‚ \"checkpoint_epoch_50.pth\"ï¼‰\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path} (epoch {checkpoint['epoch']}, loss {checkpoint['loss']:.4f})\")\n",
    "else:\n",
    "    print(f\"Checkpoint {checkpoint_path} not found, using the last trained model.\")\n",
    "\n",
    "# è¨­ç½®æ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼\n",
    "model.eval()\n",
    "predictions = []\n",
    "predicted_abstracts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        paper_ids = batch[\"paper_id\"]\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=1024,\n",
    "            min_length=50, \n",
    "            num_beams=8, \n",
    "            length_penalty=1.8,\n",
    "            early_stopping=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "        for i, generated in enumerate(generated_ids):\n",
    "            abstract = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "            abstract = clean_abstract(abstract)\n",
    "            if isinstance(paper_ids[i], torch.Tensor):\n",
    "                paper_id = str(paper_ids[i].item())\n",
    "            elif isinstance(paper_ids[i], np.ndarray):\n",
    "                paper_id = str(paper_ids[i].item())\n",
    "            else:\n",
    "                paper_id = str(paper_ids[i])\n",
    "            abstract = str(abstract) if not isinstance(abstract, str) else abstract\n",
    "            predictions.append({\n",
    "                \"paper_id\": paper_id,\n",
    "                \"abstract\": abstract\n",
    "            })\n",
    "            predicted_abstracts.append(abstract)\n",
    "\n",
    "# 7. è©•ä¼°æ¨¡å‹\n",
    "reference_abstracts = [item[\"abstract\"] for item in train_data[:103]]\n",
    "\n",
    "metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "rouge_scores = metric_rouge.compute(\n",
    "    predictions=predicted_abstracts,\n",
    "    references=reference_abstracts,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "bert_scores = metric_bertscore.compute(\n",
    "    predictions=predicted_abstracts,\n",
    "    references=reference_abstracts,\n",
    "    lang=\"en\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "\n",
    "print(\"\\nBERTScore:\")\n",
    "print(f\"Precision: {np.mean(bert_scores['precision']):.4f}\")\n",
    "print(f\"Recall: {np.mean(bert_scores['recall']):.4f}\")\n",
    "print(f\"F1: {np.mean(bert_scores['f1']):.4f}\")\n",
    "\n",
    "# 8. Save predictions\n",
    "with open(\"submission.json\", \"w\") as f:\n",
    "    for pred in predictions:\n",
    "        f.write(json.dumps(pred) + \"\\n\")\n",
    "\n",
    "print(\"Predictions saved to submission.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegasus-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¾®èª¿è¨“ç·´éšæ®µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge import Rouge\n",
    "import evaluate\n",
    "\n",
    "# åŠ è¼‰ spacy æ¨¡å‹\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# æ¸…ç† LaTeX å’Œäº‚ç¢¼çš„å‡½æ•¸\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7Fâ€™â€˜â€“â€”âˆ‘âˆ‚âˆ‡âˆÎ¸Ï€ğ’Ÿğ’«ğ’©Î±Î²Î³Î´ÎµÎ»Î¼ÏƒÏ†Ï‰â„ğ”½ğ“›()]', ' ', text)  # ä¿ç•™æ‹¬è™Ÿ\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# è¨­ç½®è¨­å‚™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. åŠ è¼‰ Pegasus-ArXiv æ¨¡å‹å’Œ tokenizer\n",
    "model_name = \"google/pegasus-arxiv\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 2. åŠ è¼‰æ•¸æ“šé›†ä¸¦æ¸…ç†æ–‡æœ¬\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "train_data = load_json(\"train.json\")  # 408 å€‹æ¨£æœ¬\n",
    "test_data = load_json(\"test.json\")    # 103 å€‹æ¨£æœ¬\n",
    "\n",
    "# åˆ†å‰²é©—è­‰é›†\n",
    "val_size = int(0.2 * len(train_data))\n",
    "val_data = train_data[-val_size:]\n",
    "train_data = train_data[:-val_size]\n",
    "\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(val_data)}\")\n",
    "\n",
    "# 3. ä½¿ç”¨ sentence-transformers æ”¹é€²æˆªæ–·ç­–ç•¥ï¼Œå¼•å…¥é ˜åŸŸçŸ¥è­˜\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def select_important_sentences(introduction, tokenizer, max_length=1024):\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    if len(sentences) <= 1:\n",
    "        tokens = tokenizer(\n",
    "            introduction,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "    # è¨ˆç®—å¥å­åµŒå…¥\n",
    "    embeddings = embedder.encode(sentences)\n",
    "    doc_embedding = np.mean(embeddings, axis=0)\n",
    "    similarities = cosine_similarity(embeddings, doc_embedding.reshape(1, -1)).flatten()\n",
    "\n",
    "    # å¼•å…¥é ˜åŸŸçŸ¥è­˜ï¼šå®šç¾©å­¸è¡“æ–‡ç« ä¸­å¸¸è¦‹çš„é—œéµè©\n",
    "    academic_keywords = [\n",
    "        \"propose\", \"method\", \"approach\", \"result\", \"finding\", \"conclusion\",\n",
    "        \"demonstrate\", \"show\", \"achieve\", \"contribution\", \"investigate\", \"study\",\n",
    "        \"analysis\", \"evaluate\", \"performance\", \"improve\", \"novel\", \"framework\"\n",
    "    ]\n",
    "\n",
    "    # çµ¦åŒ…å«é—œéµè©çš„å¥å­åŠ æ¬Š\n",
    "    scores = similarities.copy()\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if any(keyword in sentence.lower() for keyword in academic_keywords):\n",
    "            scores[idx] *= 1.5  # æé«˜åŒ…å«é—œéµè©çš„å¥å­çš„åˆ†æ•¸\n",
    "\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "\n",
    "    selected_sentences = []\n",
    "    current_length = 0\n",
    "    for idx in sorted_indices:\n",
    "        sentence = sentences[idx]\n",
    "        tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        if current_length + len(tokens) <= max_length - 2:\n",
    "            selected_sentences.append(sentence)\n",
    "            current_length += len(tokens)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    truncated_text = \" \".join(selected_sentences)\n",
    "    tokens = tokenizer(\n",
    "        truncated_text,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "# 4. å®šç¾©æ•¸æ“šé›†\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024, max_target_length=660):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        abstract = item[\"abstract\"]\n",
    "\n",
    "        input_ids, attention_mask = select_important_sentences(introduction, self.tokenizer, self.max_input_length)\n",
    "\n",
    "        targets = self.tokenizer(\n",
    "            abstract,\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": target_ids,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        input_ids, attention_mask = select_important_sentences(introduction, self.tokenizer, self.max_input_length)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"]\n",
    "        }\n",
    "\n",
    "# å‰µå»ºæ•¸æ“šé›†å’Œ DataLoader\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer, max_input_length=1024, max_target_length=660)\n",
    "val_dataset = TrainPaperDataset(val_data, tokenizer, max_input_length=1024, max_target_length=660)\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer, max_input_length=1024)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 5. å®šç¾© ROUGE å’Œ BERTScore æå¤±è¨ˆç®—å‡½æ•¸\n",
    "rouge = Rouge()\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_rouge_loss(model, input_ids, attention_mask, reference_abstracts):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=660,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_abstracts = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "        scores = rouge.get_scores(generated_abstracts, reference_abstracts, avg=True)\n",
    "        rouge_l = scores['rouge-l']['f']\n",
    "    model.train()\n",
    "    return rouge_l\n",
    "\n",
    "def compute_bertscore_loss(model, input_ids, attention_mask, reference_abstracts):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=660,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_abstracts = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "        scores = bertscore_metric.compute(\n",
    "            predictions=generated_abstracts,\n",
    "            references=reference_abstracts,\n",
    "            lang=\"en\"\n",
    "        )\n",
    "        bertscore_f1 = np.mean(scores['f1'])\n",
    "    model.train()\n",
    "    return bertscore_f1\n",
    "\n",
    "# æ–°å¢ ROUGE-1 è¨ˆç®—å‡½æ•¸\n",
    "def compute_rouge_scores(model, input_ids, attention_mask, reference_abstracts):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=660, num_beams=5)\n",
    "        generated_abstracts = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "        scores = rouge.get_scores(generated_abstracts, reference_abstracts, avg=True)\n",
    "        rouge1 = scores['rouge-1']['f']\n",
    "        rougeL = scores['rouge-l']['f']\n",
    "    model.train()\n",
    "    return rouge1, rougeL\n",
    "\n",
    "# 6. è¨“ç·´æ¨¡å‹ï¼šå„ªåŒ–æ—©åœå’Œå­¸ç¿’ç‡èª¿åº¦ï¼Œä¸¦æ·»åŠ è©•ä¼°\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.05)  # å¢åŠ åˆå§‹å­¸ç¿’ç‡\n",
    "\n",
    "# å®šç¾©æº«æš–å•Ÿå‹•èª¿åº¦å™¨\n",
    "def warm_up_lambda(epoch):\n",
    "    warm_up_epochs = 10  # å¢åŠ æº«æš–å•Ÿå‹•éšæ®µ\n",
    "    if epoch < warm_up_epochs:\n",
    "        return (epoch + 1) / warm_up_epochs  # ç·šæ€§å¢åŠ å­¸ç¿’ç‡\n",
    "    return 1.0\n",
    "\n",
    "def train_segmented_input(model, input_ids, attention_mask, labels, max_segment_length=1024):\n",
    "    total_length = input_ids.shape[1]\n",
    "    if total_length <= max_segment_length:\n",
    "        return model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    \n",
    "    losses = []\n",
    "    for start in range(0, total_length, max_segment_length):\n",
    "        end = min(start + max_segment_length, total_length)\n",
    "        segment_input_ids = input_ids[:, start:end]\n",
    "        segment_attention_mask = attention_mask[:, start:end]\n",
    "        segment_labels = labels[:, start:end] if labels.shape[1] == total_length else labels\n",
    "        output = model(input_ids=segment_input_ids, attention_mask=segment_attention_mask, labels=segment_labels)\n",
    "        losses.append(output.loss)\n",
    "    return torch.mean(torch.stack(losses))\n",
    "\n",
    "warm_up_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warm_up_lambda)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=290, eta_min=1e-6)\n",
    "\n",
    "num_epochs = 50  # å¢åŠ ç¸½è¨“ç·´ epoch æ•¸\n",
    "accumulation_steps = 5\n",
    "\n",
    "checkpoint_dir = \"pegasus-arxiv_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Baseline åˆ†æ•¸ï¼ˆç”¨æ–¼æ¯”è¼ƒï¼‰\n",
    "baseline_scores = {\n",
    "    \"rouge1\": 0.47,\n",
    "    \"rouge2\": 0.12,\n",
    "    \"rougeL\": 0.22,\n",
    "    \"bertscore_f1\": 0.85\n",
    "}\n",
    "\n",
    "eval_frequency = 5  # æ¯ 5 å€‹ epoch é€²è¡Œä¸€æ¬¡è©•ä¼°\n",
    "\n",
    "# å„²å­˜æœ€ä½³ ROUGE-1 åˆ†æ•¸ï¼ˆç”¨æ–¼æ—©åœï¼‰\n",
    "best_rouge1 = 0.0\n",
    "patience = 100\n",
    "patience_counter = 0\n",
    "\n",
    "# å„²å­˜æœ€ä½³è©•ä¼°åˆ†æ•¸ï¼ˆç”¨æ–¼æœ€çµ‚å ±å‘Šï¼‰\n",
    "best_rouge2 = 0.0\n",
    "best_rougeL = 0.0\n",
    "best_bertscore_f1 = 0.0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    total_ce_loss = 0  # è¨˜éŒ„äº¤å‰ç†µæå¤±\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        reference_abstracts = batch[\"abstract\"]\n",
    "\n",
    "        outputs = train_segmented_input(model, input_ids, attention_mask, labels)\n",
    "        ce_loss = outputs.loss  # æå– loss å±¬æ€§\n",
    "        total_ce_loss += ce_loss.item()  # å¾å¼µé‡ä¸­ç²å–æ¨™é‡å€¼\n",
    "\n",
    "        # æ¯ 5 å€‹ batch è¨ˆç®— ROUGE æå¤±ï¼ˆå¢åŠ é »ç‡ï¼‰\n",
    "        if i % 5 == 0:\n",
    "            rouge1, rougeL = compute_rouge_scores(model, input_ids, attention_mask, reference_abstracts)\n",
    "            reward_rouge1 = torch.tensor(rouge1, device=device)\n",
    "            reward_rougeL = torch.tensor(rougeL, device=device)\n",
    "        else:\n",
    "            reward_rouge1 = 0.0\n",
    "            reward_rougeL = 0.0\n",
    "\n",
    "        # æ¯ 10 å€‹ batch è¨ˆç®— BERTScore æå¤±ï¼ˆå¢åŠ é »ç‡ï¼‰\n",
    "        if i % 10 == 0:\n",
    "            bertscore_f1 = compute_bertscore_loss(model, input_ids, attention_mask, reference_abstracts)\n",
    "            reward_bertscore = torch.tensor(bertscore_f1, device=device)\n",
    "        else:\n",
    "            reward_bertscore = 0.0\n",
    "        \n",
    "        loss = ce_loss - 0.3 * reward_rouge1 - 0.1 * reward_rougeL - 0.05 * reward_bertscore\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    if (i + 1) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_ce_loss = total_ce_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_loss:.4f}, CE Loss: {avg_ce_loss:.4f}\")\n",
    "\n",
    "    # é©—è­‰éšæ®µï¼šè¨ˆç®—æå¤±ä¸¦é¸æ“‡æ€§é€²è¡Œè©•ä¼°\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_ce_loss = 0\n",
    "    predicted_abstracts = []\n",
    "    reference_abstracts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            reference_abstract = batch[\"abstract\"]\n",
    "\n",
    "            # è¨ˆç®—é©—è­‰æå¤±\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_ce_loss += outputs.loss.item()\n",
    "\n",
    "            # åƒ…åœ¨éœ€è¦è©•ä¼°æ™‚ç”Ÿæˆæ‘˜è¦\n",
    "            if (epoch + 1) % eval_frequency == 0:\n",
    "                generated_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=660,\n",
    "                    min_length=50,\n",
    "                    num_beams=15,\n",
    "                    length_penalty=1.5,\n",
    "                    repetition_penalty=1.2,\n",
    "                    early_stopping=True,\n",
    "                    no_repeat_ngram_size=3\n",
    "                )\n",
    "                generated_abstract = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                predicted_abstracts.append(generated_abstract)\n",
    "                reference_abstracts.append(reference_abstract[0])\n",
    "\n",
    "    val_ce_loss /= len(val_loader)\n",
    "    val_loss = val_ce_loss\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # æ¯éš” eval_frequency å€‹ epoch é€²è¡Œè©•ä¼°\n",
    "    current_rouge1 = 0.0  # é è¨­å€¼ï¼Œç¢ºä¿æ—©åœé‚è¼¯æ­£å¸¸é‹è¡Œ\n",
    "    if (epoch + 1) % eval_frequency == 0:\n",
    "        # è¨ˆç®— ROUGE å’Œ BERTScore\n",
    "        metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "        metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "        rouge_scores = metric_rouge.compute(\n",
    "            predictions=predicted_abstracts,\n",
    "            references=reference_abstracts,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "\n",
    "        bert_scores = metric_bertscore.compute(\n",
    "            predictions=predicted_abstracts,\n",
    "            references=reference_abstracts,\n",
    "            lang=\"en\"\n",
    "        )\n",
    "\n",
    "        # æå–è©•ä¼°åˆ†æ•¸\n",
    "        current_rouge1 = rouge_scores['rouge1']\n",
    "        current_rouge2 = rouge_scores['rouge2']\n",
    "        current_rougeL = rouge_scores['rougeL']\n",
    "        current_bertscore_f1 = np.mean(bert_scores['f1'])\n",
    "\n",
    "        # æ‰“å°è©•ä¼°çµæœä¸¦èˆ‡ baseline æ¯”è¼ƒ\n",
    "        print(\"\\n=== Validation Evaluation Results ===\")\n",
    "        print(\"ROUGE Scores:\")\n",
    "        print(f\"ROUGE-1: {current_rouge1:.4f} (Baseline: {baseline_scores['rouge1']:.4f}, Diff: {current_rouge1 - baseline_scores['rouge1']:.4f})\")\n",
    "        print(f\"ROUGE-2: {current_rouge2:.4f} (Baseline: {baseline_scores['rouge2']:.4f}, Diff: {current_rouge2 - baseline_scores['rouge2']:.4f})\")\n",
    "        print(f\"ROUGE-L: {current_rougeL:.4f} (Baseline: {baseline_scores['rougeL']:.4f}, Diff: {current_rougeL - baseline_scores['rougeL']:.4f})\")\n",
    "        print(\"\\nBERTScore:\")\n",
    "        print(f\"Precision: {np.mean(bert_scores['precision']):.4f}\")\n",
    "        print(f\"Recall: {np.mean(bert_scores['recall']):.4f}\")\n",
    "        print(f\"F1: {current_bertscore_f1:.4f} (Baseline: {baseline_scores['bertscore_f1']:.4f}, Diff: {current_bertscore_f1 - baseline_scores['bertscore_f1']:.4f})\")\n",
    "\n",
    "    # æº«æš–å•Ÿå‹•èª¿åº¦\n",
    "    warm_up_scheduler.step()\n",
    "    # å‹•æ…‹èª¿æ•´å­¸ç¿’ç‡\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºæ–¼ ROUGE-1ï¼‰\n",
    "    if current_rouge1 > best_rouge1:\n",
    "        best_rouge1 = current_rouge1\n",
    "        patience_counter = 0\n",
    "        best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'val_loss': val_loss,\n",
    "        }, best_model_path)\n",
    "        print(f\"Best model saved at {best_model_path} with ROUGE-1 {best_rouge1:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement in ROUGE-1\")\n",
    "            break\n",
    "\n",
    "    # ä¿å­˜æœ€ä½³è©•ä¼°åˆ†æ•¸ï¼ˆåŸºæ–¼ ROUGE-L å’Œ BERTScore F1ï¼‰\n",
    "    if (epoch + 1) % eval_frequency == 0:\n",
    "        if current_rougeL > best_rougeL or current_bertscore_f1 > best_bertscore_f1:\n",
    "            best_rouge2 = max(best_rouge2, current_rouge2)\n",
    "            best_rougeL = max(best_rougeL, current_rougeL)\n",
    "            best_bertscore_f1 = max(best_bertscore_f1, current_bertscore_f1)\n",
    "            print(f\"New best evaluation scores: ROUGE-L = {best_rougeL:.4f}, BERTScore F1 = {best_bertscore_f1:.4f}\")\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch + 1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# æ‰“å°æœ€çµ‚æœ€ä½³è©•ä¼°åˆ†æ•¸\n",
    "print(\"\\n=== Final Best Evaluation Scores ===\")\n",
    "print(f\"Best ROUGE-1: {best_rouge1:.4f} (Baseline: {baseline_scores['rouge1']:.4f})\")\n",
    "print(f\"Best ROUGE-2: {best_rouge2:.4f} (Baseline: {baseline_scores['rouge2']:.4f})\")\n",
    "print(f\"Best ROUGE-L: {best_rougeL:.4f} (Baseline: {baseline_scores['rougeL']:.4f})\")\n",
    "print(f\"Best BERTScore F1: {best_bertscore_f1:.4f} (Baseline: {baseline_scores['bertscore_f1']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨ç†éšæ®µ(å¾Œè™•ç†å¯åŠ BARTè¼”åŠ©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, BartForConditionalGeneration, BartTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# åŠ è¼‰ spacy æ¨¡å‹\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# æ¸…ç† LaTeX å’Œäº‚ç¢¼çš„å‡½æ•¸\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7Fâ€™â€˜â€“â€”âˆ‘âˆ‚âˆ‡âˆÎ¸Ï€ğ’Ÿğ’«ğ’©Î±Î²Î³Î´ÎµÎ»Î¼ÏƒÏ†Ï‰â„ğ”½ğ“›()]', ' ', text)  # ä¿ç•™æ‹¬è™Ÿ\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# è¨­ç½®è¨­å‚™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. åŠ è¼‰ Pegasus-ArXiv æ¨¡å‹å’Œ tokenizer\n",
    "model_name = \"google/pegasus-arxiv\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 2. åŠ è¼‰æ•¸æ“šé›†ä¸¦æ¸…ç†æ–‡æœ¬\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "test_data = load_json(\"test.json\")  # 103 å€‹æ¨£æœ¬\n",
    "\n",
    "# 3. ä½¿ç”¨ sentence-transformers æ”¹é€²æˆªæ–·ç­–ç•¥ï¼Œå¼•å…¥é ˜åŸŸçŸ¥è­˜\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def select_important_sentences(introduction, tokenizer, max_length=1024):\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    sentences = [\n",
    "        s for s in sentences\n",
    "        if not re.match(r'^\\[\\d+\\]$', s.strip()) and         # éæ¿¾ [12]\n",
    "        not re.search(r'^\\s*(features|methods|results)?\\s*\\d{4}\\s*$', s.strip(), re.IGNORECASE) and  # éæ¿¾ \"features 2021\"\n",
    "        len(s.strip().split()) > 3  # é•·åº¦å¤ªçŸ­é€šå¸¸æ²’è³‡è¨Š\n",
    "    ]\n",
    "        \n",
    "    if len(sentences) <= 1:\n",
    "        tokens = tokenizer(introduction, truncation=True, max_length=max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "    embeddings = embedder.encode(sentences)\n",
    "    doc_embedding = np.mean(embeddings, axis=0)\n",
    "    similarities = cosine_similarity(embeddings, doc_embedding.reshape(1, -1)).flatten()\n",
    "\n",
    "    academic_keywords = [\"propose\", \"method\", \"approach\", \"result\", \"finding\", \"conclusion\",\n",
    "                        \"demonstrate\", \"show\", \"achieve\", \"contribution\", \"investigate\", \"study\",\n",
    "                        \"analysis\", \"evaluate\", \"performance\", \"improve\", \"novel\", \"framework\"]\n",
    "    scores = similarities.copy()\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if any(keyword in sentence.lower() for keyword in academic_keywords):\n",
    "            scores[idx] *= 1.5\n",
    "\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    selected_sentences = []\n",
    "    current_length = 0\n",
    "    for idx in sorted_indices:\n",
    "        sentence = sentences[idx]\n",
    "        tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        if current_length + len(tokens) <= max_length - 2:\n",
    "            selected_sentences.append(sentence)\n",
    "            current_length += len(tokens)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    truncated_text = \" \".join(selected_sentences)\n",
    "    tokens = tokenizer(truncated_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "# 4. å®šç¾©æ¸¬è©¦æ•¸æ“šé›†ï¼ˆç”¨æ–¼æ¨ç†ï¼‰\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        input_ids, attention_mask = select_important_sentences(introduction, self.tokenizer, self.max_input_length)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"introduction\": introduction  # ä¿å­˜åŸå§‹å¼•è¨€ä»¥ä¾›å¾Œè™•ç†ä½¿ç”¨\n",
    "        }\n",
    "\n",
    "# å‰µå»ºæ¸¬è©¦ DataLoader\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer, max_input_length=1024)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 5. å¾Œè™•ç†å‡½æ•¸ï¼šåŸºç¤æ¸…ç†å’Œé—œéµè©ä¿ç•™\n",
    "def clean_abstract(abstract):\n",
    "    abstract = clean_text(abstract)\n",
    "    words = abstract.split()\n",
    "    while len(words) > 1 and words[0] == \"In\" and words[1] == \"In\":\n",
    "        words.pop(0)\n",
    "    abstract = \" \".join(words)\n",
    "    sentences = abstract.split(\". \")\n",
    "    seen_sentences = set()\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence and sentence not in seen_sentences:\n",
    "            seen_sentences.add(sentence)\n",
    "            cleaned_sentences.append(sentence)\n",
    "    abstract = \". \".join(cleaned_sentences)\n",
    "    if abstract and not abstract.endswith(\".\"):\n",
    "        abstract += \".\"\n",
    "    return abstract\n",
    "\n",
    "def extract_keywords(text, top_n=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.toarray()[0]\n",
    "    keyword_indices = scores.argsort()[-top_n:][::-1]\n",
    "    keywords = [feature_names[idx] for idx in keyword_indices]\n",
    "    return keywords\n",
    "\n",
    "def ensure_keywords_in_abstract(abstract, original_text):\n",
    "    keywords = extract_keywords(original_text, top_n=10)\n",
    "    abstract_words = abstract.split()\n",
    "    if len(abstract_words) > 660:\n",
    "        abstract = \" \".join(abstract_words[:660])\n",
    "    abstract_lower = abstract.lower()\n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() not in abstract_lower:\n",
    "            abstract += f\" {keyword}\"\n",
    "    return abstract\n",
    "\n",
    "# 6. ä½¿ç”¨ BART é€²è¡Œå¾Œè™•ç†\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").to(device)\n",
    "\n",
    "def enhance_abstract_with_bart(abstract):\n",
    "    inputs = bart_tokenizer(abstract, return_tensors=\"pt\", max_length=660, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    outputs = bart_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=660,\n",
    "        min_length=100,\n",
    "        num_beams=12,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=1.2,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    enhanced = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_abstract(enhanced)\n",
    "\n",
    "# 7. æ¨ç†éšæ®µï¼šåˆ†æ®µç”Ÿæˆï¼ˆé‡å° test_dataï¼‰\n",
    "checkpoint_dir = \"pegasus-arxiv_checkpoints\"\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path} (epoch {checkpoint['epoch']}, loss {checkpoint['loss']:.4f})\")\n",
    "else:\n",
    "    print(f\"Checkpoint {checkpoint_path} not found, using the last trained model.\")\n",
    "\n",
    "# è¨­ç½®æ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "def generate_segmented_abstract(input_ids, attention_mask, segment_length=1024, overlap=128):\n",
    "    total_length = input_ids.shape[1]\n",
    "    if total_length <= segment_length:\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=660,\n",
    "            min_length=50,\n",
    "            num_beams=6,  \n",
    "            repetition_penalty=1.0,  \n",
    "            length_penalty=1.5,  \n",
    "            early_stopping=False\n",
    "        )\n",
    "        return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    segments = []\n",
    "    start = 0\n",
    "    while start < total_length:\n",
    "        end = min(start + segment_length, total_length)\n",
    "        segment_input_ids = input_ids[:, start:end]\n",
    "        segment_attention_mask = attention_mask[:, start:end]\n",
    "        \n",
    "        if segment_input_ids.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=segment_input_ids,\n",
    "            attention_mask=segment_attention_mask,\n",
    "            max_length=660,\n",
    "            min_length=50,\n",
    "            num_beams=6,\n",
    "            repetition_penalty=1.0,\n",
    "            length_penalty=1.5,\n",
    "            early_stopping=False\n",
    "        )\n",
    "        segments.append(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "\n",
    "        # ç§»å‹•ä¸‹ä¸€æ®µï¼Œå¸¶æœ‰é‡ç–Šå€\n",
    "        start = end - overlap\n",
    "\n",
    "    return \" \".join(segments)\n",
    "\n",
    "\n",
    "test_paper_ids = [item[\"paper_id\"] for item in test_data] \n",
    "prediction_dict = {}  \n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(test_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        paper_ids = batch[\"paper_id\"]\n",
    "        introductions = batch[\"introduction\"]\n",
    "\n",
    "        for i in range(len(paper_ids)):\n",
    "            # åˆ†æ®µç”Ÿæˆ\n",
    "            abstract = generate_segmented_abstract(input_ids[i:i+1], attention_mask[i:i+1], segment_length=1024)\n",
    "            # åŸºç¤æ¸…ç†\n",
    "            abstract = clean_abstract(abstract)\n",
    "            # ä½¿ç”¨ BART å¾Œè™•ç†\n",
    "            abstract = enhance_abstract_with_bart(abstract)\n",
    "            # ç¢ºä¿é—œéµè©ä¿ç•™\n",
    "            abstract = ensure_keywords_in_abstract(abstract, introductions[i])\n",
    "\n",
    "            # ç²å–ç•¶å‰ paper_id\n",
    "            paper_id = paper_ids[i]\n",
    "            if isinstance(paper_id, torch.Tensor):\n",
    "                paper_id = str(paper_id.item())\n",
    "            elif isinstance(paper_id, np.ndarray):\n",
    "                paper_id = str(paper_id.item())\n",
    "            else:\n",
    "                paper_id = str(paper_id)\n",
    "\n",
    "            # å­˜å„²é æ¸¬çµæœ\n",
    "            abstract = str(abstract) if not isinstance(abstract, str) else abstract\n",
    "            prediction_dict[paper_id] = abstract\n",
    "\n",
    "# æŒ‰ç…§ test.json çš„ paper_id é †åºç”Ÿæˆ predictions\n",
    "predictions = []\n",
    "for paper_id in test_paper_ids:\n",
    "    paper_id_str = str(paper_id)\n",
    "    if paper_id_str in prediction_dict:\n",
    "        predictions.append({\n",
    "            \"paper_id\": paper_id_str,\n",
    "            \"abstract\": prediction_dict[paper_id_str]\n",
    "        })\n",
    "    else:\n",
    "        predictions.append({\n",
    "            \"paper_id\": paper_id_str,\n",
    "            \"abstract\": \"\"\n",
    "        })\n",
    "\n",
    "# 9. ä¿å­˜é æ¸¬çµæœ\n",
    "with open(\"submission_arxiv.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for pred in predictions:\n",
    "        f.write(json.dumps(pred, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Predictions saved to submission_arxiv.json, total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, BartForConditionalGeneration, BartTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# åŠ è¼‰ spacy æ¨¡å‹\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# æ¸…ç† LaTeX å’Œäº‚ç¢¼çš„å‡½æ•¸\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7Fâ€™â€˜â€“â€”âˆ‘âˆ‚âˆ‡âˆÎ¸Ï€ğ’Ÿğ’«ğ’©Î±Î²Î³Î´ÎµÎ»Î¼ÏƒÏ†Ï‰â„ğ”½ğ“›()]', ' ', text)  # ä¿ç•™æ‹¬è™Ÿ\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# è¨­ç½®è¨­å‚™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. åŠ è¼‰ Pegasus-ArXiv æ¨¡å‹å’Œ tokenizer\n",
    "model_name = \"google/pegasus-arxiv\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 2. åŠ è¼‰æ•¸æ“šé›†ä¸¦æ¸…ç†æ–‡æœ¬\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "test_data = load_json(\"test.json\")  # 103 å€‹æ¨£æœ¬\n",
    "\n",
    "# 3. ä½¿ç”¨ sentence-transformers æ”¹é€²æˆªæ–·ç­–ç•¥ï¼Œå¼•å…¥é ˜åŸŸçŸ¥è­˜\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def select_important_sentences(introduction, tokenizer, max_length=1024):\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    if len(sentences) <= 1:\n",
    "        tokens = tokenizer(introduction, truncation=True, max_length=max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "    embeddings = embedder.encode(sentences)\n",
    "    doc_embedding = np.mean(embeddings, axis=0)\n",
    "    similarities = cosine_similarity(embeddings, doc_embedding.reshape(1, -1)).flatten()\n",
    "\n",
    "    academic_keywords = [\"propose\", \"method\", \"approach\", \"result\", \"finding\", \"conclusion\",\n",
    "                        \"demonstrate\", \"show\", \"achieve\", \"contribution\", \"investigate\", \"study\",\n",
    "                        \"analysis\", \"evaluate\", \"performance\", \"improve\", \"novel\", \"framework\"]\n",
    "    scores = similarities.copy()\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if any(keyword in sentence.lower() for keyword in academic_keywords):\n",
    "            scores[idx] *= 1.5\n",
    "\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    selected_sentences = []\n",
    "    current_length = 0\n",
    "    for idx in sorted_indices:\n",
    "        sentence = sentences[idx]\n",
    "        tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        if current_length + len(tokens) <= max_length - 2:\n",
    "            selected_sentences.append(sentence)\n",
    "            current_length += len(tokens)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    truncated_text = \" \".join(selected_sentences)\n",
    "    tokens = tokenizer(truncated_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "# 4. å®šç¾©æ¸¬è©¦æ•¸æ“šé›†ï¼ˆç”¨æ–¼æ¨ç†ï¼‰\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        input_ids, attention_mask = select_important_sentences(introduction, self.tokenizer, self.max_input_length)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"introduction\": introduction  # ä¿å­˜åŸå§‹å¼•è¨€ä»¥ä¾›å¾Œè™•ç†ä½¿ç”¨\n",
    "        }\n",
    "\n",
    "# å‰µå»ºæ¸¬è©¦ DataLoader\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer, max_input_length=1024)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 5. å¾Œè™•ç†å‡½æ•¸ï¼šåŸºç¤æ¸…ç†å’Œé—œéµè©ä¿ç•™\n",
    "def clean_abstract(abstract):\n",
    "    abstract = clean_text(abstract)\n",
    "    words = abstract.split()\n",
    "    while len(words) > 1 and words[0] == \"In\" and words[1] == \"In\":\n",
    "        words.pop(0)\n",
    "    abstract = \" \".join(words)\n",
    "    sentences = abstract.split(\". \")\n",
    "    seen_sentences = set()\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence and sentence not in seen_sentences:\n",
    "            seen_sentences.add(sentence)\n",
    "            cleaned_sentences.append(sentence)\n",
    "    abstract = \". \".join(cleaned_sentences)\n",
    "    if abstract and not abstract.endswith(\".\"):\n",
    "        abstract += \".\"\n",
    "    return abstract\n",
    "\n",
    "def extract_keywords(text, top_n=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.toarray()[0]\n",
    "    keyword_indices = scores.argsort()[-top_n:][::-1]\n",
    "    keywords = [feature_names[idx] for idx in keyword_indices]\n",
    "    return keywords\n",
    "\n",
    "def ensure_keywords_in_abstract(abstract, original_text):\n",
    "    keywords = extract_keywords(original_text, top_n=10)\n",
    "    abstract_words = abstract.split()\n",
    "    if len(abstract_words) > 660:\n",
    "        abstract = \" \".join(abstract_words[:660])\n",
    "    abstract_lower = abstract.lower()\n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() not in abstract_lower:\n",
    "            abstract += f\" {keyword}\"\n",
    "    return abstract\n",
    "\n",
    "# 6. ä½¿ç”¨ BART é€²è¡Œå¾Œè™•ç†\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").to(device)\n",
    "\n",
    "def enhance_abstract_with_bart(abstract):\n",
    "    inputs = bart_tokenizer(abstract, return_tensors=\"pt\", max_length=660, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    outputs = bart_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=660,\n",
    "        min_length=100,\n",
    "        num_beams=12,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=1.2,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    enhanced = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_abstract(enhanced)\n",
    "\n",
    "# 7. æ¨ç†éšæ®µï¼šåˆ†æ®µç”Ÿæˆï¼ˆé‡å° test_dataï¼‰\n",
    "checkpoint_dir = \"pegasus-arxiv_checkpoints\"\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path} (epoch {checkpoint['epoch']}, loss {checkpoint['loss']:.4f})\")\n",
    "else:\n",
    "    print(f\"Checkpoint {checkpoint_path} not found, using the last trained model.\")\n",
    "\n",
    "# è¨­ç½®æ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "def generate_segmented_abstract(input_ids, attention_mask, segment_length=1024, overlap=128):\n",
    "    total_length = input_ids.shape[1]\n",
    "    if total_length <= segment_length:\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=660,\n",
    "            min_length=50,\n",
    "            num_beams=6,  \n",
    "            repetition_penalty=1.0,  \n",
    "            length_penalty=1.5,  \n",
    "            early_stopping=False\n",
    "        )\n",
    "        return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    segments = []\n",
    "    start = 0\n",
    "    while start < total_length:\n",
    "        end = min(start + segment_length, total_length)\n",
    "        segment_input_ids = input_ids[:, start:end]\n",
    "        segment_attention_mask = attention_mask[:, start:end]\n",
    "        \n",
    "        if segment_input_ids.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=segment_input_ids,\n",
    "            attention_mask=segment_attention_mask,\n",
    "            max_length=660,\n",
    "            min_length=50,\n",
    "            num_beams=6,\n",
    "            repetition_penalty=1.0,\n",
    "            length_penalty=1.5,\n",
    "            early_stopping=False\n",
    "        )\n",
    "        segments.append(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "\n",
    "        # ç§»å‹•ä¸‹ä¸€æ®µï¼Œå¸¶æœ‰é‡ç–Šå€\n",
    "        start = end - overlap\n",
    "\n",
    "    return \" \".join(segments)\n",
    "\n",
    "\n",
    "test_paper_ids = [item[\"paper_id\"] for item in test_data] \n",
    "prediction_dict = {}  \n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(test_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        paper_ids = batch[\"paper_id\"]\n",
    "        introductions = batch[\"introduction\"]\n",
    "\n",
    "        for i in range(len(paper_ids)):\n",
    "            # åˆ†æ®µç”Ÿæˆ\n",
    "            abstract = generate_segmented_abstract(input_ids[i:i+1], attention_mask[i:i+1], segment_length=1024)\n",
    "            # åŸºç¤æ¸…ç†\n",
    "            abstract = clean_abstract(abstract)\n",
    "            # ä½¿ç”¨ BART å¾Œè™•ç†\n",
    "            abstract = enhance_abstract_with_bart(abstract)\n",
    "            # ç¢ºä¿é—œéµè©ä¿ç•™\n",
    "            abstract = ensure_keywords_in_abstract(abstract, introductions[i])\n",
    "\n",
    "            # ç²å–ç•¶å‰ paper_id\n",
    "            paper_id = paper_ids[i]\n",
    "            if isinstance(paper_id, torch.Tensor):\n",
    "                paper_id = str(paper_id.item())\n",
    "            elif isinstance(paper_id, np.ndarray):\n",
    "                paper_id = str(paper_id.item())\n",
    "            else:\n",
    "                paper_id = str(paper_id)\n",
    "\n",
    "            # å­˜å„²é æ¸¬çµæœ\n",
    "            abstract = str(abstract) if not isinstance(abstract, str) else abstract\n",
    "            prediction_dict[paper_id] = abstract\n",
    "\n",
    "# æŒ‰ç…§ test.json çš„ paper_id é †åºç”Ÿæˆ predictions\n",
    "predictions = []\n",
    "for paper_id in test_paper_ids:\n",
    "    paper_id_str = str(paper_id)\n",
    "    if paper_id_str in prediction_dict:\n",
    "        predictions.append({\n",
    "            \"paper_id\": paper_id_str,\n",
    "            \"abstract\": prediction_dict[paper_id_str]\n",
    "        })\n",
    "    else:\n",
    "        predictions.append({\n",
    "            \"paper_id\": paper_id_str,\n",
    "            \"abstract\": \"\"\n",
    "        })\n",
    "\n",
    "# 9. ä¿å­˜é æ¸¬çµæœ\n",
    "with open(\"submission_arxiv.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for pred in predictions:\n",
    "        f.write(json.dumps(pred, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Predictions saved to submission_arxiv.json, total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¾®èª¿è¨“ç·´éšæ®µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import spacy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import LEDForConditionalGeneration, LEDTokenizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Clean LaTeX and noise\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\\\[\\w]+\\{.*?\\}', '', text)\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(r'\\[\\d+,\\s*\\d+\\]', '', text)\n",
    "    text = re.sub(r'Fig\\.\\s*\\d+.*?(?=\\.\\s|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+.*?(?=\\.\\s|$)', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?()-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load LED base model and tokenizer\n",
    "model_name = \"allenai/led-base-16384\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(model_name)\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(device)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load dataset\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "train_data = load_json(\"train.json\")\n",
    "test_data = load_json(\"test.json\")\n",
    "val_size = int(0.2 * len(train_data))\n",
    "val_data = train_data[-val_size:]\n",
    "train_data = train_data[:-val_size]\n",
    "\n",
    "# Truncation function\n",
    "def truncate_from_ends_with_importance(introduction, abstract=None, tokenizer=tokenizer, max_model_length=16384, attention_window=1024):\n",
    "    tokens = tokenizer.encode(introduction, add_special_tokens=True)\n",
    "    token_length = len(tokens)\n",
    "    min_length = 128\n",
    "    max_input_length = min(max_model_length, max(min_length, token_length))\n",
    "    max_input_length = ((max_input_length + attention_window - 1) // attention_window) * attention_window\n",
    "\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    if len(sentences) <= 1:\n",
    "        tokens = tokenizer(introduction, truncation=True, max_length=max_input_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze(), max_input_length\n",
    "\n",
    "    num_sentences = len(sentences)\n",
    "    proportion = 0.3 if 1024 <= token_length <= 4096 else (0.4 if token_length < 1024 else 0.2)\n",
    "    num_end_sentences = max(1, int(num_sentences * proportion))\n",
    "    start_sentences = sentences[:num_end_sentences]\n",
    "    end_sentences = sentences[-num_end_sentences:]\n",
    "    middle_sentences = sentences[num_end_sentences:-num_end_sentences]\n",
    "\n",
    "    start_tokens = tokenizer.encode(\" \".join(start_sentences), add_special_tokens=False)\n",
    "    end_tokens = tokenizer.encode(\" \".join(end_sentences), add_special_tokens=False)\n",
    "    current_token_count = len(start_tokens) + len(end_tokens)\n",
    "\n",
    "    selected_middle_sentences = []\n",
    "    if middle_sentences and abstract:\n",
    "        abstract_words = set(abstract.lower().split())\n",
    "        sentence_scores = []\n",
    "        for sent in middle_sentences:\n",
    "            sent_words = set(sent.lower().split())\n",
    "            overlap = len(sent_words & abstract_words) / len(sent_words) if sent_words else 0\n",
    "            sentence_scores.append(overlap)\n",
    "        sorted_middle_indices = np.argsort(sentence_scores)[::-1]\n",
    "        for idx in sorted_middle_indices:\n",
    "            sentence = middle_sentences[idx]\n",
    "            tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            if current_token_count + len(tokens) <= max_input_length - 2:\n",
    "                selected_middle_sentences.append(sentence)\n",
    "                current_token_count += len(tokens)\n",
    "            else:\n",
    "                break\n",
    "    elif middle_sentences:\n",
    "        vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "        tfidf_matrix = vectorizer.fit_transform(middle_sentences)\n",
    "        middle_scores = tfidf_matrix.sum(axis=1).A1\n",
    "        sorted_middle_indices = np.argsort(middle_scores)[::-1]\n",
    "        for idx in sorted_middle_indices:\n",
    "            sentence = middle_sentences[idx]\n",
    "            tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            if current_token_count + len(tokens) <= max_input_length - 2:\n",
    "                selected_middle_sentences.append(sentence)\n",
    "                current_token_count += len(tokens)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    truncated_introduction = \" \".join(start_sentences + selected_middle_sentences + end_sentences)\n",
    "    tokens = tokenizer(truncated_introduction, max_length=max_input_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze(), max_input_length\n",
    "\n",
    "# Dataset definitions\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_target_length=800):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids, attention_mask, max_input_length = truncate_from_ends_with_importance(item[\"introduction\"], item[\"abstract\"], self.tokenizer)\n",
    "        targets = self.tokenizer(item[\"abstract\"], max_length=self.max_target_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": target_ids,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": item[\"abstract\"],\n",
    "            \"max_input_length\": max_input_length\n",
    "        }\n",
    "\n",
    "# Collate function\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    max_input_lengths = [item[\"max_input_length\"] for item in batch]\n",
    "    max_length = max(max_input_lengths)\n",
    "    input_ids_padded = torch.zeros((len(batch), max_length), dtype=torch.long)\n",
    "    attention_masks_padded = torch.zeros((len(batch), max_length), dtype=torch.long)\n",
    "    for i in range(len(batch)):\n",
    "        length = input_ids[i].size(0)\n",
    "        input_ids_padded[i, :length] = input_ids[i]\n",
    "        attention_masks_padded[i, :length] = attention_masks[i]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    labels_padded = torch.stack(labels)\n",
    "    abstracts = [item[\"abstract\"] for item in batch]\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": attention_masks_padded,\n",
    "        \"labels\": labels_padded,\n",
    "        \"paper_id\": [item[\"paper_id\"] for item in batch],\n",
    "        \"abstract\": abstracts,\n",
    "        \"max_input_length\": max_input_lengths\n",
    "    }\n",
    "\n",
    "# Dataloader\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer)\n",
    "val_dataset = TrainPaperDataset(val_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Training settings\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "num_epochs = 300\n",
    "eval_frequency = 5\n",
    "scaler = GradScaler()\n",
    "\n",
    "warm_up_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min((epoch + 1) / 20, 1.0))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=150, eta_min=1e-6)\n",
    "\n",
    "checkpoint_dir = \"led-arxiv_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "val_loss_window = []\n",
    "window_size = 3\n",
    "patience_counter = 0\n",
    "patience = 100\n",
    "\n",
    "baseline_scores = {\"rouge1\": 0.47, \"rouge2\": 0.12, \"rougeL\": 0.22, \"bertscore_f1\": 0.85}\n",
    "best_rouge1 = 0.0\n",
    "best_rouge2 = 0.0\n",
    "best_rougeL = 0.0\n",
    "best_bertscore_f1 = 0.0\n",
    "\n",
    "metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, label_smoothing=0.1)\n",
    "            loss = loss_fct(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    val_loss_window.append(val_loss)\n",
    "    if len(val_loss_window) > window_size:\n",
    "        val_loss_window.pop(0)\n",
    "    smoothed_val_loss = sum(val_loss_window) / len(val_loss_window)\n",
    "\n",
    "    if smoothed_val_loss < best_val_loss:\n",
    "        best_val_loss = smoothed_val_loss\n",
    "        patience_counter = 0\n",
    "        model.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora\"))\n",
    "        tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora\"))\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # === ROUGE/BERTScore Evaluation every `eval_frequency` ===\n",
    "    if (epoch + 1) % eval_frequency == 0:\n",
    "        model.eval()\n",
    "        predicted_abstracts = []\n",
    "        reference_abstracts = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                reference_abstract = batch[\"abstract\"][0]\n",
    "\n",
    "                generated_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=800,\n",
    "                    min_length=10,\n",
    "                    num_beams=15,\n",
    "                    length_penalty=1.0,\n",
    "                    repetition_penalty=1.1,\n",
    "                    early_stopping=True,\n",
    "                    no_repeat_ngram_size=3\n",
    "                )\n",
    "                generated_abstract = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                predicted_abstracts.append(generated_abstract)\n",
    "                reference_abstracts.append(reference_abstract)\n",
    "\n",
    "        rouge_scores = metric_rouge.compute(predictions=predicted_abstracts, references=reference_abstracts, use_stemmer=True)\n",
    "        bert_scores = metric_bertscore.compute(predictions=predicted_abstracts, references=reference_abstracts, lang=\"en\")\n",
    "\n",
    "        current_rouge1 = rouge_scores['rouge1']\n",
    "        current_rouge2 = rouge_scores['rouge2']\n",
    "        current_rougeL = rouge_scores['rougeL']\n",
    "        current_bertscore_f1 = np.mean(bert_scores['f1'])\n",
    "\n",
    "        print(\"\\n=== Validation Evaluation Results ===\")\n",
    "        print(f\"ROUGE-1: {current_rouge1:.4f} (Baseline: {baseline_scores['rouge1']:.4f})\")\n",
    "        print(f\"ROUGE-2: {current_rouge2:.4f} (Baseline: {baseline_scores['rouge2']:.4f})\")\n",
    "        print(f\"ROUGE-L: {current_rougeL:.4f} (Baseline: {baseline_scores['rougeL']:.4f})\")\n",
    "        print(f\"BERTScore F1: {current_bertscore_f1:.4f} (Baseline: {baseline_scores['bertscore_f1']:.4f})\")\n",
    "\n",
    "        if current_rouge1 > best_rouge1:\n",
    "            best_rouge1 = current_rouge1\n",
    "            best_rouge2 = current_rouge2\n",
    "            best_rougeL = current_rougeL\n",
    "            best_bertscore_f1 = current_bertscore_f1\n",
    "            print(f\"New best ROUGE-1: {best_rouge1:.4f}\")\n",
    "\n",
    "    warm_up_scheduler.step()\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "\n",
    "# Save final model\n",
    "model.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora_final\"))\n",
    "tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora_final\"))\n",
    "print(\"Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import html\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from transformers import LEDForConditionalGeneration, LEDTokenizer, PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from peft import PeftModel\n",
    "\n",
    "# === è¨­å®š ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === è¼‰å…¥æ¨¡å‹ ===\n",
    "base_model = \"allenai/led-base-16384\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(base_model)\n",
    "model = LEDForConditionalGeneration.from_pretrained(base_model)\n",
    "model = PeftModel.from_pretrained(model, \"led-arxiv_checkpoints/led-arxiv_lora\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\").to(device).eval()\n",
    "\n",
    "flan_t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "flan_t5_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\").to(device).eval()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# === å·¥å…·å‡½æ•¸ ===\n",
    "def clean_text(text):\n",
    "    # decode escape characters\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # æ›¿æ›å¸¸è¦‹ unicode æ¨™é»ç‚º ASCIIï¼ˆem dashã€smart quotesï¼‰\n",
    "    text = text.replace('\\u2014', '-')      # em dash\n",
    "    text = text.replace('\\u2013', '-')      # en dash\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')  # quotes\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # apostrophes\n",
    "\n",
    "    # ç§»é™¤ LaTeX ç¬¦è™Ÿï¼Œä½†ä¿ç•™æ‹¬è™Ÿå…§æ–‡å­—\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
    "    text = re.sub(r'\\$([^\\$]*?)\\$', r'\\1', text)  # ä¿ç•™å…¬å¼æ–‡å­—å…§å®¹\n",
    "\n",
    "    # ç§»é™¤æ–‡ç»å¼•ç”¨ï¼Œä¾‹å¦‚ [1], [2, 3]\n",
    "    text = re.sub(r'\\[(\\d+|\\d+,\\s*\\d+)\\]', '', text)\n",
    "\n",
    "    # ç§»é™¤åœ–è¡¨æè¿°ï¼Œä½†ä¿ç•™ç« ç¯€åƒè€ƒ\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "\n",
    "    # é¿å…æŠŠ \"-\" ç ´å£ï¼Œä¾‹å¦‚ DP-SGD, self-supervised\n",
    "    # åªæ¸…é™¤éèªæ„ç›¸é—œç¬¦è™Ÿ\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?()/%=:\\-+<>_\\[\\]\\\"\\'â€™]', ' ', text)\n",
    "\n",
    "    # å»é™¤å¤šé¤˜ç©ºç™½\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text, top_n=15):\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\", max_features=100)\n",
    "    matrix = tfidf.fit_transform([text])\n",
    "    scores = matrix.toarray()[0]\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    top_indices = scores.argsort()[-top_n:][::-1]\n",
    "    return [feature_names[i] for i in top_indices if len(feature_names[i]) > 2]\n",
    "\n",
    "def ensure_keywords_in_abstract(abstract, intro, top_n=15):\n",
    "    keywords = extract_keywords(intro, top_n)\n",
    "    abstract = clean_text(abstract)\n",
    "    doc = nlp(abstract)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    missing = [kw for kw in keywords if kw.lower() not in abstract.lower()]\n",
    "    if not missing:\n",
    "        return abstract\n",
    "    \n",
    "    for kw in missing[:5]:\n",
    "        for i, sent in enumerate(sentences):\n",
    "            if len(sent.split()) > 5:\n",
    "                sentences[i] = f\"{sent.rstrip('.')} including {kw}.\"\n",
    "                break\n",
    "            elif i == len(sentences) - 1:\n",
    "                sentences[i] += f\" {kw} is considered.\"\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "def enhance_with_pegasus(text):\n",
    "    text = clean_text(text)\n",
    "    inputs = pegasus_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    outputs = pegasus_model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,\n",
    "        min_length=150,\n",
    "        num_beams=8,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    abstract = pegasus_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_text(abstract)\n",
    "\n",
    "def refine_with_flan_t5(intro, abstract):\n",
    "    # ä½¿ç”¨ Flan-T5 ä¿®æ­£æ‘˜è¦ï¼Œç¢ºä¿èˆ‡åŸæ–‡ä¸€è‡´\n",
    "    prompt = f\"summarize and refine the following text to make it concise and accurate:\\nIntroduction: {intro[:1000]}\\nGenerated abstract: {abstract}\"\n",
    "    inputs = flan_t5_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=600).to(device)\n",
    "    outputs = flan_t5_model.generate(\n",
    "        **inputs,\n",
    "        max_length=600,  # Flan-T5 å‚¾å‘ç”Ÿæˆè¼ƒç²¾ç°¡çš„å…§å®¹\n",
    "        min_length=150,\n",
    "        num_beams=6,    # å¢åŠ  beam æ•¸ä»¥æå‡å“è³ª\n",
    "        length_penalty=0.8,  # ç¨å¾®åå‘è¼ƒçŸ­è¼¸å‡º\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    refined_abstract = flan_t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_text(refined_abstract)\n",
    "\n",
    "# === è¼‰å…¥è³‡æ–™ ===\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "    return data\n",
    "\n",
    "def truncate_from_ends_with_importance(introduction, tokenizer, max_length=8192):\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
    "    if len(sentences) <= 1:\n",
    "        encoded = tokenizer(introduction, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "    embeddings = embedder.encode(sentences, convert_to_tensor=True)\n",
    "    intro_embedding = embedder.encode(introduction, convert_to_tensor=True)\n",
    "    similarities = cosine_similarity(embeddings.cpu().numpy(), intro_embedding.cpu().numpy().reshape(1, -1)).flatten()\n",
    "    weights = np.linspace(1.5, 1.0, len(sentences))\n",
    "    scores = similarities * weights\n",
    "\n",
    "    keywords = extract_keywords(introduction, top_n=5)\n",
    "    intro_with_keywords = f\"Keywords: {', '.join(keywords)}. {' '.join(sentences)}\"\n",
    "\n",
    "    sorted_idx = np.argsort(scores)[::-1]\n",
    "    selected = []\n",
    "    cur_len = 0\n",
    "    for i in sorted_idx:\n",
    "        t = tokenizer.encode(sentences[i], add_special_tokens=False)\n",
    "        if cur_len + len(t) <= max_length - len(tokenizer.encode(f\"Keywords: {', '.join(keywords)}. \", add_special_tokens=False)) - 2:\n",
    "            selected.append(sentences[i])\n",
    "            cur_len += len(t)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    truncated = f\"Keywords: {', '.join(keywords)}. {' '.join(selected)}\"\n",
    "    encoded = tokenizer(truncated, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids, attention_mask = truncate_from_ends_with_importance(item[\"introduction\"], self.tokenizer)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"introduction\": item[\"introduction\"]\n",
    "        }\n",
    "\n",
    "# === é–‹å§‹æ¨ç† ===\n",
    "test_data = load_json(\"test.json\")\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "submission = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        input_ids = input_ids.unsqueeze(0) if input_ids.dim() == 1 else input_ids\n",
    "        attention_mask = attention_mask.unsqueeze(0) if attention_mask.dim() == 1 else attention_mask\n",
    "\n",
    "        paper_id_raw = batch[\"paper_id\"][0]\n",
    "        paper_id = re.search(r'\\d+', str(paper_id_raw)).group()\n",
    "        intro = batch[\"introduction\"][0]\n",
    "\n",
    "        # LED ç”Ÿæˆ\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=10,\n",
    "            max_length=1000,\n",
    "            min_length=200,\n",
    "            length_penalty=1.3,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        abstract = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        abstract = clean_text(abstract)\n",
    "\n",
    "        # PEGASUS å¢å¼·\n",
    "        abstract = enhance_with_pegasus(abstract)\n",
    "\n",
    "        # ç¢ºä¿é—œéµè©\n",
    "        abstract = ensure_keywords_in_abstract(abstract, intro)\n",
    "\n",
    "        # Flan-T5 ä¿®æ­£\n",
    "        abstract = refine_with_flan_t5(intro, abstract)\n",
    "\n",
    "        submission.append({\n",
    "            \"paper_id\": paper_id,\n",
    "            \"abstract\": abstract\n",
    "        })\n",
    "\n",
    "with open(\"submission_led.json\", \"w\") as f:\n",
    "    for item in submission:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(submission)} abstracts to submission_led.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¬¬äºŒç‰ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import html\n",
    "import os\n",
    "import numpy as np\n",
    "import spacy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import LEDForConditionalGeneration, LEDTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Clean LaTeX and noise\n",
    "def clean_text(text):\n",
    "    # decode escape characters\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # æ›¿æ›å¸¸è¦‹ unicode æ¨™é»ç‚º ASCIIï¼ˆem dashã€smart quotesï¼‰\n",
    "    text = text.replace('\\u2014', '-')      # em dash\n",
    "    text = text.replace('\\u2013', '-')      # en dash\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')  # quotes\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # apostrophes\n",
    "\n",
    "    # ç§»é™¤ LaTeX ç¬¦è™Ÿï¼Œä½†ä¿ç•™æ‹¬è™Ÿå…§æ–‡å­—\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
    "    text = re.sub(r'\\$([^\\$]*?)\\$', r'\\1', text)  # ä¿ç•™å…¬å¼æ–‡å­—å…§å®¹\n",
    "\n",
    "    # ç§»é™¤æ–‡ç»å¼•ç”¨ï¼Œä¾‹å¦‚ [1], [2, 3]\n",
    "    text = re.sub(r'\\[(\\d+|\\d+,\\s*\\d+)\\]', '', text)\n",
    "\n",
    "    # ç§»é™¤åœ–è¡¨æè¿°ï¼Œä½†ä¿ç•™ç« ç¯€åƒè€ƒ\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "\n",
    "    # é¿å…æŠŠ \"-\" ç ´å£ï¼Œä¾‹å¦‚ DP-SGD, self-supervised\n",
    "    # åªæ¸…é™¤éèªæ„ç›¸é—œç¬¦è™Ÿ\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?()/%=:\\-+<>_\\[\\]\\\"\\'â€™]', ' ', text)\n",
    "\n",
    "    # å»é™¤å¤šé¤˜ç©ºç™½\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load LED base model and tokenizer\n",
    "model_name = \"allenai/led-base-16384\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(model_name)\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(device)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load dataset\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "train_data = load_json(\"train.json\")\n",
    "test_data = load_json(\"test.json\")\n",
    "val_size = int(0.1 * len(train_data))\n",
    "val_data = train_data[-val_size:]\n",
    "train_data = train_data[:-val_size]\n",
    "\n",
    "# Truncation function\n",
    "def truncate_full_text_only(introduction, abstract=None, tokenizer=tokenizer, max_model_length=16384, attention_window=1024):\n",
    "    tokens = tokenizer.encode(introduction, add_special_tokens=True)\n",
    "    token_length = len(tokens)\n",
    "    min_length = 128\n",
    "\n",
    "    max_input_length = min(max_model_length, max(min_length, token_length))\n",
    "    max_input_length = ((max_input_length + attention_window - 1) // attention_window) * attention_window\n",
    "\n",
    "    tokens = tokenizer(introduction, truncation=True, max_length=max_input_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"].squeeze(), tokens[\"attention_mask\"].squeeze(), max_input_length\n",
    "\n",
    "# Dataset definitions\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_target_length=800):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids, attention_mask, max_input_length = truncate_full_text_only(item[\"introduction\"], item[\"abstract\"], self.tokenizer)\n",
    "        targets = self.tokenizer(item[\"abstract\"], max_length=self.max_target_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": target_ids,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": item[\"abstract\"],\n",
    "            \"max_input_length\": max_input_length\n",
    "        }\n",
    "\n",
    "# Collate function\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    max_input_lengths = [item[\"max_input_length\"] for item in batch]\n",
    "    max_length = max(max_input_lengths)\n",
    "    input_ids_padded = torch.zeros((len(batch), max_length), dtype=torch.long)\n",
    "    attention_masks_padded = torch.zeros((len(batch), max_length), dtype=torch.long)\n",
    "    for i in range(len(batch)):\n",
    "        length = input_ids[i].size(0)\n",
    "        input_ids_padded[i, :length] = input_ids[i]\n",
    "        attention_masks_padded[i, :length] = attention_masks[i]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    labels_padded = torch.stack(labels)\n",
    "    abstracts = [item[\"abstract\"] for item in batch]\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": attention_masks_padded,\n",
    "        \"labels\": labels_padded,\n",
    "        \"paper_id\": [item[\"paper_id\"] for item in batch],\n",
    "        \"abstract\": abstracts,\n",
    "        \"max_input_length\": max_input_lengths\n",
    "    }\n",
    "\n",
    "# Dataloader\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer)\n",
    "val_dataset = TrainPaperDataset(val_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=custom_collate_fn)\n",
    "gradient_accumulation_steps = 2  \n",
    "\n",
    "# Training settings\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 200\n",
    "eval_frequency = 10\n",
    "scaler = GradScaler()\n",
    "\n",
    "warm_up_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: min((epoch + 1) / 20, 1.0))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=280, eta_min=1e-6)\n",
    "\n",
    "checkpoint_dir = \"led-arxiv_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "val_loss_window = []\n",
    "window_size = 3\n",
    "patience_counter = 0\n",
    "patience = 100\n",
    "\n",
    "baseline_scores = {\"rouge1\": 0.47, \"rouge2\": 0.12, \"rougeL\": 0.22, \"bertscore_f1\": 0.85}\n",
    "best_rouge1 = 0.0\n",
    "best_rouge2 = 0.0\n",
    "best_rougeL = 0.0\n",
    "best_bertscore_f1 = 0.0\n",
    "\n",
    "metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    val_loss_window.append(val_loss)\n",
    "    if len(val_loss_window) > window_size:\n",
    "        val_loss_window.pop(0)\n",
    "    smoothed_val_loss = sum(val_loss_window) / len(val_loss_window)\n",
    "\n",
    "    if smoothed_val_loss < best_val_loss:\n",
    "        best_val_loss = smoothed_val_loss\n",
    "        patience_counter = 0\n",
    "        model.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora\"))\n",
    "        tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora\"))\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # === ROUGE/BERTScore Evaluation every `eval_frequency` ===\n",
    "    if (epoch + 1) % eval_frequency == 0:\n",
    "        model.eval()\n",
    "        predicted_abstracts = []\n",
    "        reference_abstracts = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                reference_abstract = batch[\"abstract\"][0]\n",
    "\n",
    "                generated_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=800,\n",
    "                    min_length=100,\n",
    "                    num_beams=15,\n",
    "                    length_penalty=1.0,\n",
    "                    repetition_penalty=0.9,\n",
    "                    early_stopping=True,\n",
    "                    no_repeat_ngram_size=3\n",
    "                )\n",
    "                generated_abstract = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                predicted_abstracts.append(generated_abstract)\n",
    "                reference_abstracts.append(reference_abstract)\n",
    "\n",
    "        rouge_scores = metric_rouge.compute(predictions=predicted_abstracts, references=reference_abstracts, use_stemmer=True)\n",
    "        bert_scores = metric_bertscore.compute(predictions=predicted_abstracts, references=reference_abstracts, lang=\"en\")\n",
    "\n",
    "        current_rouge1 = rouge_scores['rouge1']\n",
    "        current_rouge2 = rouge_scores['rouge2']\n",
    "        current_rougeL = rouge_scores['rougeL']\n",
    "        current_bertscore_f1 = np.mean(bert_scores['f1'])\n",
    "\n",
    "        print(\"\\n=== Validation Evaluation Results ===\")\n",
    "        print(f\"ROUGE-1: {current_rouge1:.4f} (Baseline: {baseline_scores['rouge1']:.4f})\")\n",
    "        print(f\"ROUGE-2: {current_rouge2:.4f} (Baseline: {baseline_scores['rouge2']:.4f})\")\n",
    "        print(f\"ROUGE-L: {current_rougeL:.4f} (Baseline: {baseline_scores['rougeL']:.4f})\")\n",
    "        print(f\"BERTScore F1: {current_bertscore_f1:.4f} (Baseline: {baseline_scores['bertscore_f1']:.4f})\")\n",
    "\n",
    "        if current_rouge1 > best_rouge1:\n",
    "            best_rouge1 = current_rouge1\n",
    "            best_rouge2 = current_rouge2\n",
    "            best_rougeL = current_rougeL\n",
    "            best_bertscore_f1 = current_bertscore_f1\n",
    "            print(f\"New best ROUGE-1: {best_rouge1:.4f}\")\n",
    "\n",
    "    warm_up_scheduler.step()\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "\n",
    "# Save final model\n",
    "model.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora_final\"))\n",
    "tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"led-arxiv_lora_final\"))\n",
    "print(\"Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import html\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from transformers import LEDForConditionalGeneration, LEDTokenizer, PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from peft import PeftModel\n",
    "\n",
    "# === è¨­å®š ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === è¼‰å…¥æ¨¡å‹ ===\n",
    "base_model = \"allenai/led-base-16384\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(base_model)\n",
    "model = LEDForConditionalGeneration.from_pretrained(base_model)\n",
    "model = PeftModel.from_pretrained(model, \"led-arxiv_checkpoints/led-arxiv_lora\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\").to(device).eval()\n",
    "\n",
    "flan_t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "flan_t5_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\").to(device).eval()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# === å·¥å…·å‡½æ•¸ ===\n",
    "def clean_text(text):\n",
    "    # decode escape characters\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # æ›¿æ›å¸¸è¦‹ unicode æ¨™é»ç‚º ASCIIï¼ˆem dashã€smart quotesï¼‰\n",
    "    text = text.replace('\\u2014', '-')      # em dash\n",
    "    text = text.replace('\\u2013', '-')      # en dash\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')  # quotes\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # apostrophes\n",
    "\n",
    "    # ç§»é™¤ LaTeX ç¬¦è™Ÿï¼Œä½†ä¿ç•™æ‹¬è™Ÿå…§æ–‡å­—\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
    "    text = re.sub(r'\\$([^\\$]*?)\\$', r'\\1', text)  # ä¿ç•™å…¬å¼æ–‡å­—å…§å®¹\n",
    "\n",
    "    # ç§»é™¤æ–‡ç»å¼•ç”¨ï¼Œä¾‹å¦‚ [1], [2, 3]\n",
    "    text = re.sub(r'\\[(\\d+|\\d+,\\s*\\d+)\\]', '', text)\n",
    "\n",
    "    # ç§»é™¤åœ–è¡¨æè¿°ï¼Œä½†ä¿ç•™ç« ç¯€åƒè€ƒ\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "\n",
    "    # é¿å…æŠŠ \"-\" ç ´å£ï¼Œä¾‹å¦‚ DP-SGD, self-supervised\n",
    "    # åªæ¸…é™¤éèªæ„ç›¸é—œç¬¦è™Ÿ\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?()/%=:\\-+<>_\\[\\]\\\"\\'â€™]', ' ', text)\n",
    "\n",
    "    # å»é™¤å¤šé¤˜ç©ºç™½\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text, top_n=15):\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\", max_features=100)\n",
    "    matrix = tfidf.fit_transform([text])\n",
    "    scores = matrix.toarray()[0]\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    top_indices = scores.argsort()[-top_n:][::-1]\n",
    "    return [feature_names[i] for i in top_indices if len(feature_names[i]) > 2]\n",
    "\n",
    "def ensure_keywords_in_abstract(abstract, intro, top_n=15):\n",
    "    keywords = extract_keywords(intro, top_n)\n",
    "    abstract = clean_text(abstract)\n",
    "    doc = nlp(abstract)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    missing = [kw for kw in keywords if kw.lower() not in abstract.lower()]\n",
    "    if not missing:\n",
    "        return abstract\n",
    "    \n",
    "    for kw in missing[:5]:\n",
    "        for i, sent in enumerate(sentences):\n",
    "            if len(sent.split()) > 5:\n",
    "                sentences[i] = f\"{sent.rstrip('.')} including {kw}.\"\n",
    "                break\n",
    "            elif i == len(sentences) - 1:\n",
    "                sentences[i] += f\" {kw} is considered.\"\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "def enhance_with_pegasus(text):\n",
    "    text = clean_text(text)\n",
    "    inputs = pegasus_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    outputs = pegasus_model.generate(\n",
    "        **inputs,\n",
    "        max_length=700,\n",
    "        min_length=150,\n",
    "        num_beams=8,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    abstract = pegasus_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_text(abstract)\n",
    "\n",
    "def refine_with_flan_t5(intro, abstract):\n",
    "    # ä½¿ç”¨ Flan-T5 ä¿®æ­£æ‘˜è¦ï¼Œç¢ºä¿èˆ‡åŸæ–‡ä¸€è‡´\n",
    "    prompt = f\"summarize and refine the following text to make it concise and accurate:\\nIntroduction: {intro[:1000]}\\nGenerated abstract: {abstract}\"\n",
    "    inputs = flan_t5_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=600).to(device)\n",
    "    outputs = flan_t5_model.generate(\n",
    "        **inputs,\n",
    "        max_length=600,  # Flan-T5 å‚¾å‘ç”Ÿæˆè¼ƒç²¾ç°¡çš„å…§å®¹\n",
    "        min_length=150,\n",
    "        num_beams=6,    # å¢åŠ  beam æ•¸ä»¥æå‡å“è³ª\n",
    "        length_penalty=0.8,  # ç¨å¾®åå‘è¼ƒçŸ­è¼¸å‡º\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    refined_abstract = flan_t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_text(refined_abstract)\n",
    "\n",
    "# === è¼‰å…¥è³‡æ–™ ===\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "    return data\n",
    "\n",
    "def truncate_from_ends_with_importance(introduction, tokenizer, max_length=8192):\n",
    "    doc = nlp(introduction)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
    "    if len(sentences) <= 1:\n",
    "        encoded = tokenizer(introduction, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "    embeddings = embedder.encode(sentences, convert_to_tensor=True)\n",
    "    intro_embedding = embedder.encode(introduction, convert_to_tensor=True)\n",
    "\n",
    "    weights = np.linspace(1.5, 1.0, len(sentences))\n",
    "    scores = torch.nn.functional.cosine_similarity(embeddings, intro_embedding.unsqueeze(0), dim=1).cpu().numpy() * weights\n",
    "\n",
    "    keywords = extract_keywords(introduction, top_n=5)\n",
    "    sorted_idx = np.argsort(scores)[::-1]\n",
    "    selected = []\n",
    "    cur_len = 0\n",
    "    for i in sorted_idx:\n",
    "        t = tokenizer.encode(sentences[i], add_special_tokens=False)\n",
    "        if cur_len + len(t) <= max_length - 20:  # é ç•™ keywords é•·åº¦\n",
    "            selected.append(sentences[i])\n",
    "            cur_len += len(t)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    truncated = f\"Keywords: {', '.join(keywords)}. {' '.join(selected)}\"\n",
    "    encoded = tokenizer(truncated, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "\n",
    "class TestPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids, attention_mask = truncate_from_ends_with_importance(item[\"introduction\"], self.tokenizer)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"introduction\": item[\"introduction\"]\n",
    "        }\n",
    "\n",
    "# === é–‹å§‹æ¨ç† ===\n",
    "test_data = load_json(\"test.json\")\n",
    "test_dataset = TestPaperDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "submission = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        input_ids = input_ids.unsqueeze(0) if input_ids.dim() == 1 else input_ids\n",
    "        attention_mask = attention_mask.unsqueeze(0) if attention_mask.dim() == 1 else attention_mask\n",
    "\n",
    "        paper_id_raw = batch[\"paper_id\"][0]\n",
    "        paper_id = re.search(r'\\d+', str(paper_id_raw)).group()\n",
    "        intro = batch[\"introduction\"][0]\n",
    "\n",
    "        # === LED ç”Ÿæˆåˆç¨¿ ===\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=6,\n",
    "            max_length=800,\n",
    "            min_length=150,\n",
    "            length_penalty=1.0,\n",
    "            repetition_penalty=1.0,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        abstract = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        abstract = clean_text(abstract)\n",
    "\n",
    "        # === æ¢ä»¶æ€§è§¸ç™¼ PEGASUS å¼·åŒ– ===\n",
    "        if len(abstract.split()) < 150 or 'our method' not in abstract.lower():\n",
    "            abstract = enhance_with_pegasus(abstract)\n",
    "\n",
    "        # === å¼·åŒ–é—œéµè© ===\n",
    "        abstract = ensure_keywords_in_abstract(abstract, intro)\n",
    "\n",
    "        # === æ ¹æ“šèªæ„ç›¸ä¼¼åº¦æˆ–é—œéµè©ç¼ºå¤±æ±ºå®šæ˜¯å¦ T5 ä¿®æ­£ ===\n",
    "        intro_embed = embedder.encode(intro, convert_to_tensor=True)\n",
    "        abs_embed = embedder.encode(abstract, convert_to_tensor=True)\n",
    "        similarity = torch.nn.functional.cosine_similarity(intro_embed, abs_embed, dim=0).item()\n",
    "\n",
    "        missing_keywords = [kw for kw in extract_keywords(intro)[:5] if kw not in abstract.lower()]\n",
    "\n",
    "        if similarity < 0.6 or len(missing_keywords) > 0:\n",
    "            abstract = refine_with_flan_t5(intro, abstract)\n",
    "\n",
    "        submission.append({\n",
    "            \"paper_id\": paper_id,\n",
    "            \"abstract\": abstract\n",
    "        })\n",
    "\n",
    "with open(\"submission_led.json\", \"w\") as f:\n",
    "    for item in submission:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(submission)} abstracts to submission_led.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LontT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "import html\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, LongT5ForConditionalGeneration\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import evaluate\n",
    "\n",
    "# === åŸºæœ¬è¨­å®š ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# === æ¸…ç†æ–‡å­— ===\n",
    "def clean_text(text):\n",
    "    text = html.unescape(text)\n",
    "    text = text.replace('\\u2014', '-')\n",
    "    text = text.replace('\\u2013', '-')\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
    "    text = re.sub(r'\\$([^\\$]*?)\\$', r'\\1', text)\n",
    "    text = re.sub(r'\\[(\\d+|\\d+,\\s*\\d+)\\]', '', text)\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+[a-zA-Z]?(.*?)?(\\.|\\s|$)', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?()/%=:\\-+<>_\\[\\]\"\\'â€™]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# === è¼‰å…¥æ¨¡å‹èˆ‡ Tokenizer ===\n",
    "model_name = \"google/long-t5-tglobal-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config).to(device)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# === è¼‰å…¥è³‡æ–™é›† ===\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "train_data = load_json(\"train.json\")\n",
    "test_data = load_json(\"test.json\")\n",
    "val_size = int(0.1 * len(train_data))\n",
    "val_data = train_data[-val_size:]\n",
    "train_data = train_data[:-val_size]\n",
    "\n",
    "# === Tokenize function ===\n",
    "def tokenize_inputs(intro, tokenizer, max_length=7000):\n",
    "    tokens = tokenizer(intro, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    return tokens[\"input_ids\"].squeeze(0), tokens[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "# === Dataset å®šç¾© ===\n",
    "class PaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_target_length=712):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids, attention_mask = tokenize_inputs(item[\"introduction\"], self.tokenizer)\n",
    "        target = self.tokenizer(item[\"abstract\"], max_length=self.max_target_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        labels = target[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": item[\"abstract\"]\n",
    "        }\n",
    "\n",
    "# === Collate function ===\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n",
    "    attention_mask = torch.stack([x[\"attention_mask\"] for x in batch])\n",
    "    labels = torch.stack([x[\"labels\"] for x in batch])\n",
    "    abstracts = [x[\"abstract\"] for x in batch]\n",
    "    paper_ids = [x[\"paper_id\"] for x in batch]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"abstract\": abstracts,\n",
    "        \"paper_id\": paper_ids\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(PaperDataset(train_data, tokenizer), batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(PaperDataset(val_data, tokenizer), batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# === è¨“ç·´è¨­å®š ===\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scaler = GradScaler()\n",
    "num_epochs = 200\n",
    "gradient_accumulation_steps = 10\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "checkpoint_dir = \"longt5-arxiv_lora\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "best_rouge1 = 0.0\n",
    "\n",
    "# === è¨“ç·´è¿´åœˆ ===\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # é©—è­‰éšæ®µ\n",
    "    model.eval()\n",
    "    predictions, references, paper_ids = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            reference = batch[\"abstract\"][0]\n",
    "            pid = batch[\"paper_id\"][0]\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=1024,\n",
    "                min_length=200,\n",
    "                num_beams=8, \n",
    "                length_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            predictions.append(pred)\n",
    "            references.append(reference)\n",
    "            paper_ids.append(pid)\n",
    "\n",
    "    rouge = metric_rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    bert = metric_bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    rouge1 = rouge[\"rouge1\"]\n",
    "    print(f\"ROUGE-1: {rouge1:.4f}, BERTScore F1: {np.mean(bert['f1']):.4f}\")\n",
    "\n",
    "    if rouge1 > best_rouge1:\n",
    "        best_rouge1 = rouge1\n",
    "        model.save_pretrained(os.path.join(checkpoint_dir, \"best\"))\n",
    "        tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"best\"))\n",
    "        print(\"Best model saved.\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "# æœ€å¾Œå„²å­˜\n",
    "model.save_pretrained(os.path.join(checkpoint_dir, \"final\"))\n",
    "tokenizer.save_pretrained(os.path.join(checkpoint_dir, \"final\"))\n",
    "print(\"Training completed and model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–¹å‘2ã€Large Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‹±æ–‡æ–‡æœ¬ï¼šLLaMA-2 13B + Axolotl + PEFTã€Mambaã€Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸­æ–‡æ–‡æœ¬ï¼šQwen2 + QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==2.1.2 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers peft accelerate evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bitsandbytes -f https://huggingface.github.io/bitsandbytes-packages/torch211_cu118.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy nltk rouge-score bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # ç”¨æ–¼æ–·å¥å’Œ tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install absl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "from transformers import default_data_collator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import evaluate\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Clean LaTeX and noise\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', '', text)  # LaTeX commands\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)         # $$ math $$\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)               # $math$\n",
    "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text)  # [1], [1, 2]\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+.*?(?=\\.|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+.*?(?=\\.|$)', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:!?()\\-]', ' ', text)  # remove rare symbols but keep colons etc\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model/tokenizer with QLoRA\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Optimized LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load dataset\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "# Custom Dataset\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=8192):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        introduction = item[\"introduction\"]\n",
    "        abstract = item[\"abstract\"]\n",
    "\n",
    "        # Enhanced CoT-style prompt\n",
    "        input_text = f\"\"\"\n",
    "You are a scientific writing assistant trained to write high-quality research abstracts.\n",
    "Your task is to analyze the given introduction of a computer science or artificial intelligence paper and generate a clear, structured, and academic abstract.\n",
    "\n",
    "Use professional and academic language. Maintain coherence and conciseness. The abstract should be approximately 150â€“300 words.\n",
    "\n",
    "[Introduction]\n",
    "{introduction}\n",
    "\n",
    "[Abstract]\"\"\"\n",
    "        full_text = input_text + f\" {abstract}\"\n",
    "\n",
    "        tokens = self.tokenizer(full_text, max_length=self.max_length, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        input_ids = tokens[\"input_ids\"].squeeze()\n",
    "        attention_mask = tokens[\"attention_mask\"].squeeze()\n",
    "\n",
    "        abstract_start = input_text.count(\"\\n\")  # Rough offset fallback if needed\n",
    "        labels = input_ids.clone()\n",
    "        labels[:len(self.tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].squeeze())] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": abstract,\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_masks = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    abstracts = [item[\"abstract\"] for item in batch]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels,\n",
    "        \"paper_id\": [item[\"paper_id\"] for item in batch],\n",
    "        \"abstract\": abstracts,\n",
    "    }\n",
    "    \n",
    "# è¼‰å…¥è³‡æ–™èˆ‡åˆ‡åˆ†è¨“ç·´ / é©—è­‰é›†\n",
    "train_data = load_json(\"train.json\")   # å‡è¨­ä½ çš„è¨“ç·´è³‡æ–™æª”åç‚º train.json\n",
    "val_size = int(0.2 * len(train_data))  # ä½¿ç”¨ 20% ç•¶ä½œé©—è­‰é›†\n",
    "val_data = train_data[-val_size:]\n",
    "train_data = train_data[:-val_size]\n",
    "\n",
    "test_data = load_json(\"test.json\") \n",
    "    \n",
    "\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer)\n",
    "val_dataset = TrainPaperDataset(val_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=default_data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=default_data_collator)\n",
    "\n",
    "# Generation config with stop token handling\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=800,\n",
    "    min_length=150,\n",
    "    num_beams=5,\n",
    "    length_penalty=1.0,\n",
    "    repetition_penalty=1.1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=1e-2)\n",
    "scaler = GradScaler()\n",
    "\n",
    "baseline_scores = {\"rouge1\": 0.47, \"rouge2\": 0.12, \"rougeL\": 0.22, \"bertscore_f1\": 0.85}\n",
    "checkpoint_dir = \"gemma3-12b_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "patience = 30\n",
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "best_rouge1 = 0.0\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    predicted_abstracts = []\n",
    "    reference_abstracts = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "            gen_input = input_ids[0].unsqueeze(0)\n",
    "            generated_ids = model.generate(gen_input, generation_config=generation_config)\n",
    "            decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            generated = decoded.split(\"### Abstract:\")[-1].strip()\n",
    "            predicted_abstracts.append(generated)\n",
    "            reference_abstracts.append(batch[\"abstract\"][0])\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    rouge_scores = rouge.compute(predictions=predicted_abstracts, references=reference_abstracts, use_stemmer=True)\n",
    "    bert_scores = bertscore.compute(predictions=predicted_abstracts, references=reference_abstracts, lang=\"en\")\n",
    "    current_rouge1 = rouge_scores['rouge1']\n",
    "    current_bertscore = np.mean(bert_scores['f1'])\n",
    "\n",
    "    print(f\"ROUGE-1: {current_rouge1:.4f}, BERTScore-F1: {current_bertscore:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss or current_rouge1 > best_rouge1:\n",
    "        best_val_loss = val_loss\n",
    "        best_rouge1 = current_rouge1\n",
    "        patience_counter = 0\n",
    "        model.save_pretrained(os.path.join(checkpoint_dir, \"best_model\"))\n",
    "        print(\"Model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ”¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "from transformers import default_data_collator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import evaluate\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Clean LaTeX and noise\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', '', text)  # LaTeX commands\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)         # $$ math $$\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)               # $math$\n",
    "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text)  # [1], [1, 2]\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+.*?(?=\\.|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+.*?(?=\\.|$)', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:!?()\\-]', ' ', text)  # remove rare symbols but keep colons etc\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model/tokenizer with QLoRA\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Optimized LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load dataset\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    for item in data:\n",
    "        item[\"introduction\"] = clean_text(item[\"introduction\"])\n",
    "        if \"abstract\" in item:\n",
    "            item[\"abstract\"] = clean_text(item[\"abstract\"])\n",
    "    return data\n",
    "\n",
    "# Custom Dataset\n",
    "class TrainPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        intro = item[\"introduction\"]\n",
    "        abstract = item[\"abstract\"]\n",
    "\n",
    "        # CoT prompt\n",
    "        prompt_prefix = (\n",
    "            \"You are a scientific writing assistant trained to write high-quality research abstracts.\\n\"\n",
    "            \"Your task is to analyze the given introduction of a computer science or artificial intelligence paper\"\n",
    "            \"and generate a clear, structured, and academic abstract.\\n\\n\"\n",
    "            \"Use professional and academic language. Maintain coherence and conciseness.\"\n",
    "            \"The abstract should be approximately 150â€“300 words.\\n\\n\"\n",
    "            \"And learn how to start the first sentence of most abstracts.\\n\\n\"\n",
    "            \"[Introduction]\\n\"\n",
    "        )\n",
    "        prompt_suffix = \"\\n\\n[Abstract]\"\n",
    "\n",
    "        # tokenize introduction separately and truncate\n",
    "        intro_tokens = self.tokenizer(prompt_prefix + intro, truncation=True, max_length=1536, return_tensors=\"pt\")\n",
    "        prompt_input_ids = intro_tokens[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = intro_tokens[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # concatenate prompt + abstract for labels\n",
    "        full_text = self.tokenizer.decode(prompt_input_ids, skip_special_tokens=True) + prompt_suffix + \" \" + abstract\n",
    "        full_tokens = self.tokenizer(full_text, truncation=True, max_length=self.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = full_tokens[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = full_tokens[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # è¨ˆç®— abstract èµ·å§‹ä½ç½®ï¼Œmasked labels\n",
    "        label_cutoff = self.tokenizer(full_text.split(\"[Abstract]\")[0], return_tensors=\"pt\")[\"input_ids\"].size(1)\n",
    "        labels = input_ids.clone()\n",
    "        labels[:label_cutoff] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"paper_id\": item[\"paper_id\"],\n",
    "            \"abstract\": abstract,\n",
    "    }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_masks = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    abstracts = [item[\"abstract\"] for item in batch]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels,\n",
    "        \"paper_id\": [item[\"paper_id\"] for item in batch],\n",
    "        \"abstract\": abstracts,\n",
    "    }\n",
    "    \n",
    "# è¼‰å…¥è³‡æ–™èˆ‡åˆ‡åˆ†è¨“ç·´ / é©—è­‰é›†\n",
    "train_data = load_json(\"train.json\")   \n",
    "val_size = int(0.1 * len(train_data))  # ä½¿ç”¨ 10% ç•¶ä½œé©—è­‰é›†\n",
    "val_data = train_data[-val_size:]\n",
    "train_data = train_data[:-val_size]\n",
    "\n",
    "test_data = load_json(\"test.json\") \n",
    "    \n",
    "\n",
    "train_dataset = TrainPaperDataset(train_data, tokenizer)\n",
    "val_dataset = TrainPaperDataset(val_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Generation config with stop token handling\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=300,\n",
    "    min_length=150,\n",
    "    num_beams=5,\n",
    "    length_penalty=1.0,\n",
    "    repetition_penalty=1.1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=1e-2)\n",
    "scaler = GradScaler()\n",
    "\n",
    "baseline_scores = {\"rouge1\": 0.47, \"rouge2\": 0.12, \"rougeL\": 0.22, \"bertscore_f1\": 0.85}\n",
    "checkpoint_dir = \"gemma-7b_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "patience = 30\n",
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "best_rouge1 = 0.0\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        if torch.all(labels == -100):\n",
    "            continue\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Starting validation at Epoch {epoch+1}\")\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        predicted_abstracts = []\n",
    "        reference_abstracts = []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(val_loader):\n",
    "                print(f\"[Validation] Processing batch {i+1}/{len(val_loader)}\")  \n",
    "                \n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                with autocast():\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    val_loss += outputs.loss.item()\n",
    "\n",
    "                # ç”Ÿæˆæ‘˜è¦\n",
    "                gen_input = input_ids[0].unsqueeze(0)\n",
    "                gen_input = gen_input[:, -1024:]  # æˆªæ–·åˆ°æœ€å¾Œ 1024 å€‹ token\n",
    "                generated_ids = model.generate(gen_input, generation_config=generation_config)\n",
    "                decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                # æå– [Abstract] å¾Œçš„å…§å®¹\n",
    "                if \"[Abstract]\" in decoded:\n",
    "                    generated = decoded.split(\"[Abstract]\")[-1].strip()\n",
    "                else:\n",
    "                    # å¦‚æœæ²’æœ‰ [Abstract]ï¼Œå˜—è©¦ç§»é™¤æç¤ºè©éƒ¨åˆ†\n",
    "                    prompt_end = \"Use professional and academic language. Maintain coherence and conciseness. The abstract should be approximately 150â€“300 words.\"\n",
    "                    if prompt_end in decoded:\n",
    "                        generated = decoded.split(prompt_end)[-1].strip()\n",
    "                    else:\n",
    "                        generated = decoded  # å¦‚æœç„¡æ³•åˆ†å‰²ï¼Œä¿ç•™å…¨æ–‡ä¸¦è¨˜éŒ„è­¦å‘Š\n",
    "                        print(f\"Paper {batch['paper_id'][0]}: ç„¡æ³•æ­£ç¢ºæå–æ‘˜è¦ï¼Œä½¿ç”¨å®Œæ•´ç”Ÿæˆå…§å®¹\")\n",
    "\n",
    "                # ç§»é™¤æ›è¡Œç¬¦è™Ÿä¸¦è¦ç¯„åŒ–ç©ºæ ¼\n",
    "                generated = generated.replace(\"\\n\", \" \").strip()\n",
    "                generated = re.sub(r'\\s+', ' ', generated)\n",
    "\n",
    "                predicted_abstracts.append(generated)\n",
    "                reference_abstracts.append(batch[\"abstract\"][0])\n",
    "\n",
    "        # è¨ˆç®—è©•ä¼°æŒ‡æ¨™\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        bertscore = evaluate.load(\"bertscore\")\n",
    "        rouge_scores = rouge.compute(predictions=predicted_abstracts, references=reference_abstracts, use_stemmer=True)\n",
    "        bert_scores = bertscore.compute(predictions=predicted_abstracts, references=reference_abstracts, lang=\"en\")\n",
    "        current_rouge1 = rouge_scores['rouge1']\n",
    "        current_rouge2 = rouge_scores['rouge2']\n",
    "        current_rougeL = rouge_scores['rougeL']\n",
    "        current_bertscore = np.mean(bert_scores['f1'])\n",
    "\n",
    "        print(f\"ROUGE-1: {current_rouge1:.4f}, ROUGE-2: {current_rouge2:.4f}, ROUGE-L: {current_rougeL:.4f}, BERTScore-F1: {current_bertscore:.4f}\")\n",
    "\n",
    "        # æª¢æŸ¥æ˜¯å¦ä¿å­˜æ¨¡å‹æˆ–æå‰åœæ­¢\n",
    "        if val_loss < best_val_loss or current_rouge1 > best_rouge1:\n",
    "            best_val_loss = val_loss\n",
    "            best_rouge1 = current_rouge1\n",
    "            patience_counter = 0\n",
    "            model.save_pretrained(os.path.join(checkpoint_dir, \"best_model\"))\n",
    "            print(\"Model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ---------- å‰è™•ç† ----------\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', '', text)\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)\n",
    "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text)\n",
    "    text = re.sub(r'(Fig\\.|Figure)\\s*\\d+.*?(?=\\.|$)', '', text)\n",
    "    text = re.sub(r'Table\\s*\\d+.*?(?=\\.|$)', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:!?()\\-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# ---------- Reranking åˆ†æ•¸ ----------\n",
    "def heuristic_score(summary, intro):\n",
    "    words = summary.split()\n",
    "    sents = summary.count('.') + summary.count('?') + summary.count('!')\n",
    "    intro_words = set(intro.lower().split())\n",
    "    summary_words = set(summary.lower().split())\n",
    "    overlap = len(intro_words & summary_words)\n",
    "    return min(len(words), 400) / 400 + min(sents, 8) / 8 + min(overlap, 30) / 30\n",
    "\n",
    "# ---------- è¼‰å…¥æ¨¡å‹ ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, \"gemma-7b_checkpoints/best_model\")\n",
    "model.eval()\n",
    "\n",
    "# ---------- Generation è¨­å®š ----------\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=900,\n",
    "    min_length=200,\n",
    "    num_beams=5,\n",
    "    length_penalty=1.0,\n",
    "    repetition_penalty=1.1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# ---------- è¼‰å…¥æ¸¬è©¦è³‡æ–™ ----------\n",
    "with open(\"test.json\", \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# ---------- ç”Ÿæˆæ‘˜è¦ + Self-Reranking ----------\n",
    "results = []\n",
    "for item in tqdm(test_data):\n",
    "    paper_id = item[\"paper_id\"]\n",
    "    intro = clean_text(item[\"introduction\"])  # æ¸…ç†ä»‹ç´¹æ–‡æœ¬\n",
    "\n",
    "    # ä¿®æ”¹æç¤ºè©ï¼Œæ˜ç¢ºåˆ†éš”ç¬¦è™Ÿ\n",
    "    prompt = f\"\"\"You are an AI expert research assistant.\n",
    "Your job is to read the following introduction from a scientific paper in the field of computer science or AI and generate a structured, informative abstract.\n",
    "\n",
    "Focus on the following:\n",
    "1. Research background and motivation.\n",
    "2. Methodology or proposed solution.\n",
    "3. Key results or contributions.\n",
    "\n",
    "Use academic language and maintain clarity. The abstract should be suitable for publication.\n",
    "\n",
    "Introduction: {intro}\n",
    "\n",
    "Generate the abstract below, starting with [ABSTRACT] followed by the content:\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    candidates = []\n",
    "    for _ in range(1):  # ç”¢ç”Ÿå¤šå€‹æ‘˜è¦å€™é¸\n",
    "        with torch.no_grad():\n",
    "            gen_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                generation_config=generation_config,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        decoded = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        # æå– [ABSTRACT] å¾Œçš„å…§å®¹ï¼Œä¸¦ç§»é™¤æ›è¡Œç¬¦è™Ÿ\n",
    "        try:\n",
    "            summary = decoded.split(\"[ABSTRACT]\")[1].strip().replace(\"\\n\", \" \")\n",
    "        except IndexError:\n",
    "            # å¦‚æœæ¨¡å‹æœªç”Ÿæˆ [ABSTRACT]ï¼Œå‰‡å–æœ€å¾Œä¸€æ®µä½œç‚ºæ‘˜è¦\n",
    "            summary = decoded.split(\"Generate the abstract below\")[-1].strip().replace(\"\\n\", \" \")\n",
    "        candidates.append(summary)\n",
    "\n",
    "    # reranking\n",
    "    scores = [heuristic_score(c, intro) for c in candidates]\n",
    "    best_summary = candidates[np.argmax(scores)]\n",
    "\n",
    "    results.append({\"paper_id\": str(paper_id), \"abstract\": best_summary})\n",
    "\n",
    "# ---------- å„²å­˜çµæœ ----------\n",
    "with open(\"submission_gemma.json\", \"w\") as f:\n",
    "    for item in results:\n",
    "        json.dump(item, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"âœ… æ¨ç†èˆ‡é‡æ’åºå®Œæˆï¼Œå·²å„²å­˜ç‚º submission_gemma.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…ˆè¨ˆç®—tokenæ•¸é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "# è¼‰å…¥ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-12b-it\", trust_remote_code=True)\n",
    "\n",
    "# è¨ˆç®— token æ•¸é‡çš„å‡½å¼\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# è®€å–è³‡æ–™\n",
    "with open(\"train.json\", \"r\") as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "with open(\"test.json\", \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# å„²å­˜è¨“ç·´è³‡æ–™çš„ token æ•¸é‡\n",
    "train_intro_tokens = []\n",
    "train_abstract_tokens = []\n",
    "\n",
    "# è¨ˆç®—ä¸¦å„²å­˜è¨“ç·´è³‡æ–™çš„ token æ•¸é‡\n",
    "for paper in train_data:\n",
    "    intro = paper[\"introduction\"]\n",
    "    abstr = paper[\"abstract\"]\n",
    "    train_intro_tokens.append(count_tokens(intro))\n",
    "    train_abstract_tokens.append(count_tokens(abstr))\n",
    "\n",
    "# å„²å­˜æ¸¬è©¦è³‡æ–™çš„ token æ•¸é‡\n",
    "test_intro_tokens = []\n",
    "\n",
    "# è¨ˆç®—ä¸¦å„²å­˜æ¸¬è©¦è³‡æ–™çš„ token æ•¸é‡\n",
    "for paper in test_data:\n",
    "    intro = paper[\"introduction\"]\n",
    "    test_intro_tokens.append(count_tokens(intro))\n",
    "\n",
    "# è¨ˆç®—è¨“ç·´è³‡æ–™çš„çµ±è¨ˆæ•¸æ“š\n",
    "print(\"è¨“ç·´è³‡æ–™ (Introduction):\")\n",
    "print(f\"  å¹³å‡ token æ•¸: {statistics.mean(train_intro_tokens):.2f}\")\n",
    "print(f\"  ä¸­ä½æ•¸ token æ•¸: {statistics.median(train_intro_tokens)}\")\n",
    "print(f\"  æœ€å° token æ•¸: {min(train_intro_tokens)}\")\n",
    "print(f\"  æœ€å¤§ token æ•¸: {max(train_intro_tokens)}\")\n",
    "print(f\"  æ¨™æº–å·®: {statistics.stdev(train_intro_tokens):.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"è¨“ç·´è³‡æ–™ (Abstract):\")\n",
    "print(f\"  å¹³å‡ token æ•¸: {statistics.mean(train_abstract_tokens):.2f}\")\n",
    "print(f\"  ä¸­ä½æ•¸ token æ•¸: {statistics.median(train_abstract_tokens)}\")\n",
    "print(f\"  æœ€å° token æ•¸: {min(train_abstract_tokens)}\")\n",
    "print(f\"  æœ€å¤§ token æ•¸: {max(train_abstract_tokens)}\")\n",
    "print(f\"  æ¨™æº–å·®: {statistics.stdev(train_abstract_tokens):.2f}\")\n",
    "print()\n",
    "\n",
    "# è¨ˆç®—æ¸¬è©¦è³‡æ–™çš„çµ±è¨ˆæ•¸æ“š\n",
    "print(\"æ¸¬è©¦è³‡æ–™ (Introduction):\")\n",
    "print(f\"  å¹³å‡ token æ•¸: {statistics.mean(test_intro_tokens):.2f}\")\n",
    "print(f\"  ä¸­ä½æ•¸ token æ•¸: {statistics.median(test_intro_tokens)}\")\n",
    "print(f\"  æœ€å° token æ•¸: {min(test_intro_tokens)}\")\n",
    "print(f\"  æœ€å¤§ token æ•¸: {max(test_intro_tokens)}\")\n",
    "print(f\"  æ¨™æº–å·®: {statistics.stdev(test_intro_tokens):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch transformers datasets rouge_score scikit-learn numpy tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install einops transformers_stream_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler, BitsAndBytesConfig\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import os\n",
    "os.makedirs(\"qwen_checkpoints/best_model\", exist_ok=True)\n",
    "os.makedirs(\"qwen_checkpoints/final_model\", exist_ok=True)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def extract_important_sentences(text, tokenizer, max_token_length=4096):\n",
    "    # ç”¨ spaCy åˆ†å¥\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    if len(sentences) <= 1:\n",
    "        return text\n",
    "\n",
    "    # ç”¨ TF-IDF è©•ä¼°é‡è¦æ€§\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    scores = tfidf_matrix.sum(axis=1).A1\n",
    "    ranked_sentences = [sent for _, sent in sorted(zip(scores, sentences), reverse=True)]\n",
    "\n",
    "    selected = []\n",
    "    total_tokens = 0\n",
    "    for sent in ranked_sentences:\n",
    "        tokenized = tokenizer(sent, add_special_tokens=False)[\"input_ids\"]\n",
    "        if total_tokens + len(tokenized) > max_token_length:\n",
    "            break\n",
    "        selected.append(sent)\n",
    "        total_tokens += len(tokenized)\n",
    "\n",
    "    return \" \".join(selected)\n",
    "\n",
    "\n",
    "# 1. è³‡æ–™è™•ç†\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=4096):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        intro = self.data[idx][\"introduction\"]\n",
    "        abstract = self.data[idx][\"abstract\"]\n",
    "\n",
    "        # å¦‚æœå¤ªé•·å°±ç”¨ extract_important_sentences\n",
    "        intro = extract_important_sentences(intro, self.tokenizer, max_token_length=self.max_length - 512)\n",
    "\n",
    "        prompt = (\n",
    "            \"You are a Computer Science research assistant. Summarize the following introduction into a clear and concise academic abstract.\\n\\n\"\n",
    "            f\"Introduction: {intro}\\n\\n\"\n",
    "            \"Abstract:\"\n",
    "        )\n",
    "        full_text = f\"{prompt} {abstract}\"\n",
    "\n",
    "        tokenized = self.tokenizer(full_text, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        input_ids = tokenized[\"input_ids\"].squeeze()\n",
    "        attention_mask = tokenized[\"attention_mask\"].squeeze()\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        prompt_len = len(self.tokenizer(prompt, truncation=True, max_length=self.max_length)[\"input_ids\"])\n",
    "        labels[:prompt_len] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "data = []\n",
    "with open(\"train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():  \n",
    "            try:\n",
    "                data.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON line: {e}\")\n",
    "                continue\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "print(f\"Training samples: {len(train_data)}, Validation samples: {len(val_data)}\")\n",
    "\n",
    "# 2. åŠ è¼‰ Qwen æ¨¡å‹èˆ‡ Tokenizer\n",
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# è¨­ç½®è³‡æ–™é›†èˆ‡ DataLoader\n",
    "train_dataset = CustomDataset(train_data, tokenizer, max_length=4096)\n",
    "val_dataset = CustomDataset(val_data, tokenizer, max_length=4096)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "# 3. è¨“ç·´è¨­ç½®\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 10\n",
    "gradient_accumulation_steps = 4\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * num_epochs // gradient_accumulation_steps)\n",
    "\n",
    "patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# ROUGE è©•ä¼°å‡½æ•¸\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "def compute_rouge(predictions, references):\n",
    "    scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = scorer.score(ref, pred)\n",
    "        scores[\"rouge1\"].append(score[\"rouge1\"].fmeasure)\n",
    "        scores[\"rouge2\"].append(score[\"rouge2\"].fmeasure)\n",
    "        scores[\"rougeL\"].append(score[\"rougeL\"].fmeasure)\n",
    "    return {k: np.mean(v) for k, v in scores.items()}\n",
    "\n",
    "def compute_bestscore1(rouge_scores):\n",
    "    return rouge_scores[\"rouge1\"]\n",
    "\n",
    "# 4. è¨“ç·´èˆ‡é©—è­‰å¾ªç’°\n",
    "best_rouge = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / gradient_accumulation_steps\n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        predictions, references = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=550,\n",
    "                    num_beams=8,\n",
    "                    do_sample=False,\n",
    "                    top_k=30,\n",
    "                    top_p=0.95,\n",
    "                    temperature=0.7,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "                pred_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "                ref_texts = [tokenizer.decode(label, skip_special_tokens=True) for label in batch[\"labels\"]]\n",
    "                pred_texts = [text.split(\"Abstract:\")[-1].strip() if \"Abstract:\" in text else text for text in pred_texts]\n",
    "                predictions.extend(pred_texts)\n",
    "                references.extend(ref_texts)\n",
    "\n",
    "        rouge_scores = compute_rouge(predictions, references)\n",
    "        bestscore1 = compute_bestscore1(rouge_scores)\n",
    "        print(f\"Validation - ROUGE-1: {rouge_scores['rouge1']:.4f}, ROUGE-2: {rouge_scores['rouge2']:.4f}, ROUGE-L: {rouge_scores['rougeL']:.4f}, BestScore-1: {bestscore1:.4f}\")\n",
    "\n",
    "        # Early Stopping åˆ¤æ–·\n",
    "        current_score = rouge_scores[\"rouge1\"]\n",
    "        if current_score > best_rouge:\n",
    "            best_rouge = current_score\n",
    "            early_stopping_counter = 0  # reset counter\n",
    "            print(\"New best ROUGE-1, saving model...\")\n",
    "            model.save_pretrained(\"qwen_checkpoints/best_model\")\n",
    "            tokenizer.save_pretrained(\"qwen_checkpoints/best_model\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.5\n",
    "            print(f\"ROUGE-1 not improved, reducing LR to {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} validations without improvement.\")\n",
    "            break\n",
    "\n",
    "# 5. ä¿å­˜æœ€çµ‚æ¨¡å‹\n",
    "model.save_pretrained(\"qwen_checkpoints/final_model\")\n",
    "tokenizer.save_pretrained(\"qwen_checkpoints/final_model\")\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from peft import PeftModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "import unicodedata\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# === è³‡æ–™æ¸…æ´— ===\n",
    "allowed_unicode = \"âˆ‘âˆ‚âˆ‡âˆÎ¸Ï€ğ’Ÿğ’«ğ’©Î±Î²Î³Î´ÎµÎ»Î¼ÏƒÏ†Ï‰â„ğ”½ğ“›\"\n",
    "def is_allowed_char(c):\n",
    "    return (\n",
    "        ord(c) < 128 or\n",
    "        c in allowed_unicode or\n",
    "        \"MATHEMATICAL\" in unicodedata.name(c, \"\")\n",
    "    )\n",
    "\n",
    "def clean_intro(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)  # ä¿ç•™ \\emph{} å…§æ–‡\n",
    "    text = ''.join(c if is_allowed_char(c) else ' ' for c in text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_important_sentences(text, tokenizer, max_token_length=4096, keep_head_sentences=3):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "    if len(sentences) <= keep_head_sentences:\n",
    "        return text\n",
    "\n",
    "    selected = sentences[:keep_head_sentences]\n",
    "    total_tokens = sum(len(tokenizer(s, add_special_tokens=False)[\"input_ids\"]) for s in selected)\n",
    "\n",
    "    try:\n",
    "        remaining_sentences = sentences[keep_head_sentences:]\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(remaining_sentences)\n",
    "        scores = tfidf_matrix.sum(axis=1).A1\n",
    "        ranked = [s for _, s in sorted(zip(scores, remaining_sentences), reverse=True)]\n",
    "    except Exception as e:\n",
    "        print(f\"[TF-IDF fallback] {e}\")\n",
    "        ranked = remaining_sentences[:5]\n",
    "\n",
    "    for sent in ranked:\n",
    "        token_len = len(tokenizer(sent, add_special_tokens=False)[\"input_ids\"])\n",
    "        if total_tokens + token_len > max_token_length:\n",
    "            break\n",
    "        selected.append(sent)\n",
    "        total_tokens += token_len\n",
    "\n",
    "    return \" \".join(selected)\n",
    "\n",
    "def maybe_extract_important_sentences(text, tokenizer, max_token_threshold=3000, keep_head_sentences=3):\n",
    "    tokens = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(tokens) <= max_token_threshold:\n",
    "        return text\n",
    "    return extract_important_sentences(text, tokenizer, max_token_length=max_token_threshold, keep_head_sentences=keep_head_sentences)\n",
    "\n",
    "def clean_generated_abstract(text):\n",
    "    doc = nlp(text)\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    for sent in doc.sents:\n",
    "        s_strip = sent.text.strip()\n",
    "        if s_strip and s_strip not in seen:\n",
    "            cleaned.append(s_strip)\n",
    "            seen.add(s_strip)\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "# === è¨­å®š ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_input_length = 4096\n",
    "max_output_tokens = 650\n",
    "model_dir = \"qwen_checkpoints/epoch_6\"\n",
    "\n",
    "# === è¼‰å…¥ tokenizer èˆ‡æ¨¡å‹ ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-7B-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, model_dir)\n",
    "model.eval()\n",
    "\n",
    "# === è¼‰å…¥è³‡æ–™ ===\n",
    "with open(\"test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "# === æ¨ç† ===\n",
    "results = []\n",
    "for item in tqdm(test_data, desc=\"Generating Abstracts\"):\n",
    "    paper_id = item[\"paper_id\"]\n",
    "    intro = clean_intro(item[\"introduction\"])\n",
    "    intro = maybe_extract_important_sentences(intro, tokenizer)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an expert research assistant specialized in artificial intelligence. \"\n",
    "        \"Your task is to read the following paper introduction and write a clear, formal, and concise abstract. \"\n",
    "        \"Focus on the research background, motivation, methods, and key contributions.\\n\\n\"\n",
    "        f\"Introduction:\\n{intro}\\n\\n\"\n",
    "        \"Abstract:\"\n",
    "    )\n",
    "\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\", max_length=4096, truncation=True, padding=True).to(device)\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_output_tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        num_beams=8,\n",
    "        do_sample=False,\n",
    "        temperature=0.7,\n",
    "        top_k=30,\n",
    "        top_p=0.95,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    abstract = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Abstract:\" in abstract:\n",
    "        abstract = abstract.split(\"Abstract:\")[-1].strip()\n",
    "    else:\n",
    "        abstract = abstract.strip().split(\"\\n\")[-1]\n",
    "\n",
    "    abstract = clean_generated_abstract(abstract)\n",
    "    results.append({\"paper_id\": str(paper_id), \"abstract\": abstract})\n",
    "\n",
    "# === è¼¸å‡ºçµæœ ===\n",
    "with open(\"submission_qwen.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in results:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"æ‘˜è¦å·²å„²å­˜åˆ° submission_qwen.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
